---

marp: true
# header: 'Header content'
footer: '跨库SER系统的设计与实现'
theme: gaia
class: lead


---

# 跨库语音情感识别系统的设计与实现


![bg contain left:45% 98% 在这里插入图片描述](https://img-blog.csdnimg.cn/be61e759517a4f038c3eb69928e097aa.png)

计科1902 徐超信
指导老师：蒋海华

---

<!-- paginate: true -->

## 内容目录

-  

---

## 背景&意义

- 语音情感识别是从语音中识别说话人情感状态的重要研究方向，不同语言和文化的情感表达方式和特征可能存在差异性和相似性，因此需要进行跨库语音情感识别。这项研究可以反映模型的泛化和迁移能力，同时也有助于揭示不同语言和文化之间的情感特征和规律。
- 跨库语音情感识别的研究对于提高语音情感识别的实用性和普适性，以及促进多语言和多文化的人机交互具有重要意义。

---


## 开发路径

<!-- _theme:uncover -->

![bg contain right:80% 95% 在这里插入图片描述](https://img-blog.csdnimg.cn/b0fca5b946ea4e20acc513c1ed304c50.png)

### 研究内容

- 本文设计并实现一个可跨语料库的语音情感识别系统，方便用户通过客户端进行语音情感识别操作。目前的语音情感识别系统大多是基于特定语料库训练和测试的，这导致了系统的泛化能力较差，难以适应不同语言、不同场景、不同说话人的语音情感识别任务。为了解决这一问题，本文提出了一种跨语料库语音情感识别系统，旨在寻找一种能够进行跨库识别的识别模型。
- 本文基于已有的多个情感语料库，提取有效的语音情感特征，建立分类模型，然后对新的语音进行情感分类，同时将跨库模型与传统的语音情感识别模型在跨库语音情感识别的任务中对性能作比较，最后完成语音情感识别系统的开发。

---

1. 基于多种语音情感特征的提取。本文构建了一个包含时域特征，频域特征，谱特征的语音情感特征表示，能够有效地捕捉语音中的情感信息。
2. 特征优选的研究。本文通过实验表明，采用合适的数据预处理的方法有利于提高模型的泛化能力。本文采用StandardScaler和PCA等方法处理基本的语音情感特征，去除数据的量纲带来的不利影响，减少冗余特征，降低计算复杂度，提高模型性能。

---



1. 分类模型的性能评估和选择。本文选用多种常用的机器学习算法进行跨库语音情感识别实验，采用带随机化的K-Fold等交叉验证法对模型做出初步的性能评估，再根据识别模型在测试集上的预测准确率作为主要评价指标，结合超参数空间搜索等方法计算最优超参数，并对各种算法的性能做出比较，从中选择最佳的识别算法和模型。
2. 软件设计和开发。基于多个公开的跨语料库语音情感识别数据集上，本文基于Python编程语言设计并实现了用于跨库语音情感识别任务的相对完善的软件系统，应用了较为丰富的可视化元素，提供了丰富的操作性和灵活性，方便用户定制化训练识别模型以及对跨库语音情感识别模型性能进行对比。

---

### 语料库的选用



- EMO-DB : 该数据集是由 10 名演员(分别从5个男性和5个女性说话人的表演语音中获得)模拟 7 种情绪产生的 10 个德语语句, 7 种情绪分别是:中性、愤怒、恐惧、喜悦、悲伤、厌恶和厌倦, 数据库共536 个样本, 该数据库已经成为许多研究的基础。
- SAVEE: 该数据集是一个使用英国英语的多模态情感数据集。 它总共包含了 480 条语音以及 7 种不同的情感: 中性、快乐、悲伤、愤怒、惊讶、恐惧和厌恶。 这些话语由 4 个专业的男性演员产生。 为了保持情感表演的良好质量, 本数据集的所有录音均由 10 位不同的评价者在音频、视觉和视听条件下进行验证这些录音中的脚本选自常规 TIMIT 语料库。

---

- RAVDESS: 该数据集是情感语音和歌曲的多模态语料。 该数据集是性别均衡的, 由 24 名专门演员组成, 他们以中性的北美（英语）发音产生语音和歌曲样本。 对于情感性言语, 它由平静、欢乐、悲伤、愤怒、恐惧、惊讶、厌恶构成。 对于情感性的歌曲, 它由平静、欢乐、悲伤、愤怒、恐惧、惊讶、厌恶和恐惧组成。 每个表情都是在情感强度的两个层次上产生的, 带有一个附加的中性表情。 最后收集的 7 356 份录音在情感有效性、强度和真实性方面分别被评为 10 次。 对于这些收视率, 雇佣了来自北美的 247 名未经培训的研究对象。

---

- 上述数据库既可以完成同语言不同库的跨库识别(SAVEE和RAVDESA)实验,也可以完成跨语言跨库的识别实验（EMO-DB和SAVEE或EMO-DB和RAVDESS)。

---

### 语音情感特征的选用

- 特征提取是所有模式识别系统的重要部分。在跨库语音情感识别中，提取域不变性的情感特征是非常关键的，域不变性特征(domain invariant )的提取将直接影响到跨库语音的情感识别效果。

- MFCC（Mel频率倒谱系数）是一种常用的语音特征提取方法，它通过将语音信号转换为频域特征，再将其转换为倒谱系数，以捕捉语音信号中的重要信息。

- MelSpectrogram是一种基于梅尔刻度的声谱图，它将声音信号转换为频率和时间的二维图像。与传统的声谱图相比，MelSpectrogram可以更好地模拟人类听觉系统对声音的感知。

- Chromagram特征的提取方法是一种基于短时傅里叶变换（STFT）的特征提取方法，用于将语音信号转换为色谱图（chromagram）。色谱图是一种表示音乐和语音信号的频率分布的特殊形式，它描述了信号中各个音高的强度及其在时间上的变化。


---

##  机器学习算法在本系统中的运用

跨库语音情感识别任务旨在识别来自不同语音库或说话人的语音情感。为了解决这个问题，系统采用了多种机器学习算法。

### KNN

K近邻算法（K-Nearest Neighbors，KNN）是一种基于实例的分类算法，可以用于跨库语音情感识别中。将KNN算法运用于跨库语音情感识别中，会有以下特点：

非参数化：KNN算法是一种非参数化的算法，不需要对数据分布做出任何先验假设，因此可以灵活处理不同数据分布的情况。该特点对于跨库语音情感识别任务来说尤为重要，因为不同语音库之间的分布可能存在差异。

高效性：KNN算法的预测速度相对较快，因为它只需要计算测试样本与训练样本之间的距离，并选择最近的K个样本来进行分类。此外，KNN算法可以使用kd-tree或ball-tree等数据结构来加速搜索。

简单性：KNN算法的实现相对简单，只需要选择一个合适的距离度量和K值即可。此外，KNN算法也比较容易理解和解释，可以帮助分析数据的特征和分布。

对噪声敏感：KNN算法对噪声比较敏感，因为它的预测结果取决于最近邻的样本。如果训练数据中包含噪声，那么KNN算法的分类结果可能会受到影响。

### SVM

支持向量机（SVM）：SVM是一种一组监督学习方法，也是一种二分类算法。SVM可用于分类、回归和异常值检测。

该算法具有较好的泛化能力和分类效果，因此在跨库语音情感识别中得到了广泛应用[17][18]。SVM通过选择一个最优的超平面来将不同情感的语音样本分开。在构建超平面时，SVM 算法会选择一个最佳的截距，使得映射后的数据距离每个类别的中心更近。超平面的法线方向表示类别之间的最大边距，该方向的向量表示两个类别之间的距离。

SVM 算法有两个主要的特点：稀疏性和稳健性。稀疏性指的是 SVM 算法可以使用核函数将数据映射到高维空间中，这样可以减少数据的维度，从而减少了数据的存储空间和计算时间。稳健性指的是 SVM 算法在处理异常值和噪声数据时表现出较好的鲁棒性。SVM的优点包括：

1. 在高维空间中表现良好。
2. 在特征数量大于样本数量的情况下仍然可用。
3. 使用训练点的子集（称为支持向量）来进行决策，因此内存使用效率高。
4. 灵活性强：可以为决策函数指定不同的核函数。除了提供常见的核函数外，还可以使用自定义核函数。

但SVM也有其缺点：

1. 如果特征数量远大于样本数量，则在选择核函数和正则化项时需要避免过拟合。
2. SVM不能直接提供概率估计，需要使用复杂的交叉验证来计算

### MLP

多层感知器（MLP）是一种具有多个隐层的前馈神经网络，它可以用于解决各种分类和回归问题。在回归任务中，MLP算法的基本思想是，通过多层非线性变换将输入数据映射到一个高维空间中，然后通过输出层将高维空间中的结果映射回原始空间中的标签。具体而言，MLP算法会在每个隐层中使用多个神经元来学习非线性特征，最终输出一个连续的预测值。

### Ensemble Learning

集成学习（Ensemble Learning）：集成学习是一种将多个分类器进行集成，以达到更好的分类性能的机器学习方法。在跨库语音情感识别中，集成学习算法可以用于提高情感识别的准确率和鲁棒性[19]。以下介绍本文采用的集成学习算法：

#### RandomForest

RandomForest是一种基于决策树的集成学习算法，可以用于跨库语音情感识别中[20]。随机森林通过随机选择特征和样本，构建多个决策树，然后将它们的预测结果进行平均或投票来获得最终的分类结果。在跨库情感识别中，随机森林可以通过对多个语音库进行随机选择，从而提高分类器的鲁棒性和泛化能力。

#### Bagging

Bagging（bootstrap aggregating）是一种基于自助采样的集成学习算法，可以用于跨库语音情感识别中。它通过对训练集进行有放回的随机抽样，构建多个数据子集，然后使用每个数据子集训练出一个基本分类器，最后将这些基本分类器的结果进行投票或求平均来得到最终的分类结果。这种方法可以降低模型的方差，提高模型的泛化能力。在跨库情感识别中，Bagging还可以通过对多个语音库进行自助采样，从而提高分类器的鲁棒性和泛化能力。



## 跨库语义情感识别系统的构建

### 超参数的选择和优化策略

本文主要采用GridSearch对机器学习算法作为超参数选择策略[26]，借助sklearn提供的超参数搜索框架进行设计和实现。网格搜索是一种机器学习中的超参数调优技术，其目的是找到模型超参数的最优值。超参数是在训练过程中不会被学习的参数，但在训练前需要设置，可以对模型性能产生重要影响。它基于一个预定义的超参数网格（grid），对每个超参数组合进行评估和比较，从而选择最佳的超参数组合。Grid Search 将每个超参数的取值范围划分成一组离散的值，然后对所有可能的超参数组合进行遍历，对每个组合训练一个模型，并使用交叉验证等方法评估模型性能。最后，系统选择具有最佳性能的超参数组合作为最终模型的超参数。如图 3.2所示图像是sklearn官网提供的某个SVM分类模型通过GridSearch交叉验证搜索最优核函数得到的AUC曲线例图。

![image-20230510225114089](D:\repos\CCSER\SER\assets\image-20230510225114089.png)

对SVM模型执行GridSearchCV的AUC折线图

Grid Search 是一种简单而有效的调参方法，但它需要遍历所有可能的超参数组合，因此计算成本较高。为了减少计算成本，可以使用随机搜索（Random Search）[27]等其它调参方法。

<!-- class: default  -->

<!-- class: lead  -->

---

![image-20230112193055545](D:\repos\blogs\graduationDesign\assets\image-20230112193055545.png)

识别的流程
![image-20230510221301394](D:\repos\CCSER\SER\assets\image-20230510221301394.png)

## 模型评估与优化

在k-fold CV中，训练集被分成k个小集合，模型在k-1个集合上进行训练，并在剩余的集合上进行验证。这个过程重复k次，每个集合都曾经作为验证集。通过这种方法，我们可以避免浪费数据，并且在样本数量较小的问题中具有明显的优势。最终的性能度量是每次循环中计算的值的平均值。

常见的交叉验证方法，包括k-fold、stratified k-fold、shuffled split和stratified shuffled split。本文借助sklearn框架将这几种选择交叉验证器集成到了系统中，以例图中的流程框架进行，放便用户对模型以不同的方式进行评估：

![image-20230510225441591](D:\repos\CCSER\SER\assets\image-20230510225441591.png)

### 语音情感识别系统的开发：

- 本系统将在windows上开发，采用python语言和PyQT技术进行图形界面的开发。深度学习框架使用pytorch。
- 系统功能：可视化地展示模型的训练过程和语音情感的识别过程。拟实现一个抑郁症患者情绪跟踪管理系统，比如日常性对病人发送问题，根据患者的回答进行情感分析，辅助医生分析患者病情走势

---

各部分间的关系
![ 在这里插入图片描述](https://img-blog.csdnimg.cn/10f77de059604bd8be125e1dd3c39d55.png)

## 系统开发



### 系统架构

![image-20230510225608660](D:\repos\CCSER\SER\assets\image-20230510225608660.png)

### 系统功能

包括自定义识别模型、训练与识别结果分析、语音文件浏览与情感识别系统、语音文件可视化以及批量识别结果的展示与保存等



## 实验结果

共记录了7组不同的实验结果，并对它们做了性能分析和对比分析。实验结果表明，传统的机器学习算法在执行语音情感识别任务上具有一定的效果，特别是在同库识别上，对典型情感分类表现出良好的性能。然而，在执行跨库语音情感识别任务时，其表现不尽如人意。当情感类别较为简单时，采用SVM分类器进行实验可以获得较高准确率。但是，当数据量较大时，训练SVM分类模型的时间也会大大增加。随着参与实验的情感种类的增加，集成学习方法较SVM方法会有更高的识别率。

## 总结

- 





