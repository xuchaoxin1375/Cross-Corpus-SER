{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# from typing_extensions import deprecated\n",
    "import warnings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import config.MetaPath as mp\n",
    "from config.MetaPath import test_emodb_csv\n",
    "from config.algoparams import ava_ML_algorithms\n",
    "import random\n",
    "from time import time\n",
    "from config.algoparams import ava_cv_modes\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from deprecated import deprecated\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    fbeta_score,\n",
    "    make_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ShuffleSplit,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "from audio.extractor import AudioExtractor, load_data_from_meta\n",
    "import config.EF as ef\n",
    "from config.EF import AHNPS, e_config_def, f_config_def, validate_emotions\n",
    "from config.MetaPath import (\n",
    "    emodb,\n",
    "    meta_paths_of_db,\n",
    "    ravdess,\n",
    "    savee,\n",
    "    validate_partition,\n",
    "    project_dir,\n",
    ")\n",
    "import config.MetaPath as meta\n",
    "from audio.core import best_estimators, extract_feature_of_audio\n",
    "from config.algoparams import random_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "class EmotionRecognizer:\n",
    "    \"\"\"A class for training, testing ,predicting,anaylzing emotions based on\n",
    "    speech's features that are extracted and fed into `sklearn` model\n",
    "\n",
    "    examples\n",
    "    -\n",
    "    rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=[ravdess],train_dbs=[ravdess], verbose=1)\n",
    "\n",
    "    rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=emodb,train_dbs=emodb, verbose=1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        classification_task=True,\n",
    "        dbs=None,\n",
    "        e_config=None,\n",
    "        f_config=None,\n",
    "        train_dbs=None,\n",
    "        test_dbs=None,\n",
    "        balance=False,\n",
    "        shuffle=True,\n",
    "        override_csv=True,\n",
    "        cross=False,  # è¡¨ç¤ºè·¨åº“(è¿™å›è®©å…¶è¯»å–train_db_econfig.csvä½œä¸ºæµ‹è¯•é›†,æ ·ä¾‹æ›´ä¸°å¯Œ,æ›´åˆç†,å½“ç„¶è¿˜å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›,è¯»å–all_db_econfg.csv,ä½†æ˜¯è¿™é‡Œæš‚ä¸æ‰§è¡Œ)\n",
    "        verbose=0,\n",
    "        **feature_transforms,\n",
    "        # **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            model (sklearn model): the model used to detect emotions. If `model` is None, then self.determine_best_model()\n",
    "                will be automatically called\n",
    "                è¿™ä¸ªå‚æ•°å…¶å®å°±æ˜¯sklearnä¸­çš„Estimatorå¯¹è±¡,ä¾‹å¦‚SVC()ç¤ºä¾‹åŒ–å‡ºæ¥çš„å¯¹è±¡\n",
    "            emotions (list): list of emotions to be used. Note that these emotions must be available in\n",
    "                RAVDESS & EMODB Datasets, available nine emotions are the following:\n",
    "                    'neutral', 'calm', 'happy', 'sad', 'angry', 'fear', 'disgust', 'ps' ( pleasant surprised ), 'boredom'.\n",
    "                é»˜è®¤è¯†åˆ«ä¸‰ç§æƒ…æ„Ÿ\n",
    "                Default is [\"sad\", \"neutral\", \"happy\"].\n",
    "            æƒ…æ„Ÿæ•°æ®åº“çš„ä½¿ç”¨å¼€å…³:\n",
    "            ravdess (bool): whether to use RAVDESS Speech datasets, default is True\n",
    "            emodb (bool): whether to use EMO-DB Speech dataset, default is True,\n",
    "            custom_db (bool): whether to use custom Speech dataset that is located in `data/train-custom`\n",
    "                and `data/test-custom`, default is True\n",
    "            è¾“å‡ºæ–‡ä»¶åçš„æŒ‡å®š(åº”è¯¥æŒ‡å®šä¸ºcsvæ–‡ä»¶,å³å‚æ•°å¸¦æœ‰æ‰©å±•åç¼€csv)\n",
    "            ravdess_name (str): the name of the output CSV file for RAVDESS dataset, default is \"ravdess.csv\"\n",
    "            emodb_name (str): the name of the output CSV file for EMO-DB dataset, default is \"emodb.csv\"\n",
    "            custom_db_name (str): the name of the output CSV file for the custom dataset, default is \"custom.csv\"\n",
    "            æŒ‡å®šéœ€è¦æå–çš„æƒ…æ„Ÿç‰¹å¾,é»˜è®¤ä¸‰ç§:mfcc,chroma,mel\n",
    "            features (list): list of speech features to use, default is [\"mfcc\", \"chroma\", \"mel\"]\n",
    "                (i.e MFCC, Chroma and MEL spectrogram )\n",
    "            æŒ‡å®šè¦ä½¿ç”¨çš„åˆ†ç±»æ¨¡å‹è¿˜æ˜¯å›å½’æ¨¡å‹,é»˜è®¤ä½¿ç”¨åˆ†ç±»æ¨¡å‹\n",
    "            classification (bool): whether to use classification or regression, default is True\n",
    "            balance (bool): whether to balance the dataset ( both training and testing ), default is True\n",
    "            verbose (bool/int): whether to print messages on certain tasks, default is 1\n",
    "        Note that when `ravdess`, `emodb` and `custom_db` are set to `False`, `ravdess` will be set to True\n",
    "        automatically.\n",
    "        \"\"\"\n",
    "        # emotions\n",
    "        self.e_config = e_config if e_config else e_config_def\n",
    "        # make sure that there are only available emotions\n",
    "        validate_emotions(self.e_config)\n",
    "        self.f_config = f_config if f_config else f_config_def\n",
    "        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼(å¾…ä¼˜åŒ–)\n",
    "        # @deprecated(version='1.0', reason='è¯·ä½¿ç”¨ new_function() ä»£æ›¿')\n",
    "        # self._f_config_dict: dict[str, bool] = get_f_config_dict(self.f_config)\n",
    "        self.train_dbs = train_dbs\n",
    "        self.test_dbs = test_dbs\n",
    "\n",
    "        # print(self.train_meta_files, self.test_meta_files)\n",
    "        self.feature_transforms = feature_transforms\n",
    "\n",
    "        # å¯ä»¥ä½¿ç”¨python é»˜è®¤å‚æ•°æ¥æ”¹é€ å†™æ³•\n",
    "        # é»˜è®¤æ‰§è¡Œåˆ†ç±»ä»»åŠ¡\n",
    "        self.classification_task = classification_task\n",
    "        self.verbose = verbose\n",
    "        # boolean attributes\n",
    "        self.override_csv = override_csv\n",
    "        self.shuffle = shuffle\n",
    "        self.balance = balance\n",
    "        self.cross = cross\n",
    "        # éæ„é€ å™¨åˆå§‹åŒ–å˜é‡\n",
    "        self.data_loaded = False\n",
    "        self.model_trained = False\n",
    "        self.ae = None\n",
    "        self.dbs = dbs if dbs else [ravdess]\n",
    "        # é‰´äºæ•°æ®é›†(ç‰¹å¾å’Œæ ‡ç­¾)åœ¨è¯„ä¼°æ–¹æ³•æ—¶å°†åå¤ç”¨åˆ°,å› æ­¤è¿™é‡Œå°†è®¾ç½®ç›¸åº”çš„å±æ€§æ¥ä¿å­˜å®ƒä»¬\n",
    "        # å¦ä¸€æ–¹é¢,å¦‚æœæ¨¡ä»¿sklearnä¸­çš„ç¼–å†™é£æ ¼,å…¶å®æ˜¯å°†æ•°æ®å’Œæ¨¡å‹è®¡ç®—åˆ†å¸ƒåœ¨ä¸åŒçš„æ¨¡å—(ç±»)ä¸­,æ¯”å¦‚\n",
    "        # sklearn.datasetsè´Ÿè´£æ•°æ®é›†ç”Ÿæˆ\n",
    "        # sklearn.model_selectionè´Ÿè´£åˆ’åˆ†æ•°æ®é›†å’Œè®­ç»ƒé›†\n",
    "        # sklearn.algorithms* è´Ÿè´£åˆ›å»ºæ¨¡å‹\n",
    "        # sklearn.metrics è´Ÿè´£è¯„ä¼°æ¨¡å‹\n",
    "        # è®¾ç½®ç›¸åº”çš„å±æ€§çš„æ–¹ä¾¿ä¹‹å¤„åœ¨äºæ–¹æ³•çš„è°ƒç”¨å¯ä»¥å°‘ä¼ å‚\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "        self.y_pred = []\n",
    "        self.train_audio_paths = []\n",
    "        self.test_audio_paths = []\n",
    "        # å¼€å§‹å¡«å……æ•°æ®(æœ€å…ˆå¼€å§‹çš„æ­¥éª¤,æ”¾åœ¨initä¸­éšç€åˆå§‹åŒ–å®ä¾‹çš„æ—¶å€™æ‰§è¡Œ)\n",
    "        # self.load_data()\n",
    "        # å±æ€§çš„å…ˆåä½ç½®ä¼šå½±å“ç¨‹åºçš„è¿è¡Œ\n",
    "        # print(\"@{model}\")\n",
    "        # print(model,\"\\ncomparing and choosing the best model...\")\n",
    "        # !RandomForestClassifierå®ä¾‹ä¸èƒ½ç›´æ¥ç”¨bool()æ¥åˆ¤æ–­,ä¼šæç¤ºestimators_ä¸å­˜åœ¨\n",
    "        self.model = model\n",
    "        # if self.model is None:\n",
    "        # ä¾èµ–äºboolean attributes\n",
    "\n",
    "        self.train_meta_files = meta_paths_of_db(\n",
    "            db=self.train_dbs,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=\"train\",\n",
    "        )\n",
    "        # åˆ¤æ–­è·¨åº“ä»»åŠ¡\n",
    "        test_meta_partition = \"test\"\n",
    "        if self.cross:\n",
    "            test_meta_partition = \"train\"\n",
    "        self.test_meta_files = meta_paths_of_db(\n",
    "            db=self.test_dbs,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=test_meta_partition,\n",
    "        )\n",
    "\n",
    "    # def prepare():\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        å¯¼å…¥æŒ‡å®šçš„è¯­æ–™åº“æ•°æ®,å¹¶æå–ç‰¹å¾\n",
    "        Loads and extracts features from the audio files for the db's specified\n",
    "        - æ³¨æ„,ç”±äºbalanceæ“ä½œå¯¹äºæ•°æ®é›†åˆ’åˆ†æœ‰ä¸€å®šè¦æ±‚,ä¸æ˜¯ä»»ä½•æ•°æ®é›†éƒ½å¯ä»¥æ‰§è¡Œbalanceæ“ä½œ(ä¾‹å¦‚ä¸‰åˆ†ç±»ä¸­,test setä¸­åªç¼ºå¤±äº†æŸä¸€ä¸ªç±»åˆ«çš„æ ·æœ¬,è¿™ä¸­æƒ…å†µä¸‹æ‰§è¡Œbalance,å°†å¯¼è‡´æµ‹è¯•é›†æ ·æœ¬æ•°é‡ä¸ºç©º)\n",
    "        \"\"\"\n",
    "        # åˆ¤æ–­æ˜¯å¦å·²ç»å¯¼å…¥è¿‡æ•°æ®.å¦‚æœå·²ç»å¯¼å…¥,åˆ™è·³è¿‡,å¦åˆ™æ‰§è¡Œå¯¼å…¥\n",
    "        if not self.data_loaded:\n",
    "            # è°ƒç”¨extractorä¸­çš„æ•°æ®å¯¼å…¥å‡½æ•°\n",
    "            data = load_data_from_meta(\n",
    "                train_meta_files=self.train_meta_files,\n",
    "                test_meta_files=self.test_meta_files,\n",
    "                f_config=self.f_config,\n",
    "                e_config=self.e_config,\n",
    "                classification_task=self.classification_task,\n",
    "                balance=self.balance,\n",
    "                shuffle=self.shuffle,\n",
    "                feature_transforms=self.feature_transforms,\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            # è®¾ç½®å®ä¾‹çš„å„ä¸ªå±æ€§\n",
    "            # äº‹å®ä¸Š,ä¹Ÿå¯ä»¥ç›´æ¥ç”¨load_data_from_metaè¿”å›çš„ç»“æœä¸­çš„aeå¯¹è±¡,èµ‹å€¼ERå¯¹è±¡(self.ae=data[\"ae\"])\n",
    "            self.ae = data[\"ae\"]\n",
    "            self.X_train = data[\"X_train\"]\n",
    "            self.X_test = data[\"X_test\"]\n",
    "            self.y_train = data[\"y_train\"]\n",
    "            self.y_test = data[\"y_test\"]\n",
    "            self.train_audio_paths = data[\"train_audio_paths\"]\n",
    "            self.test_audio_paths = data[\"test_audio_paths\"]\n",
    "\n",
    "            self.balanced_success(data)\n",
    "            if self.verbose:\n",
    "                print(\"[I] Data loaded\\n\")\n",
    "                print(f\"{self.ae=}\")\n",
    "                print(f\"{self.ae.pca=}ğŸˆ\")\n",
    "            self.data_loaded = True\n",
    "            # print(id(self))\n",
    "            if self.verbose > 1:\n",
    "                print(vars(self))\n",
    "\n",
    "    def balanced_success(self, res):\n",
    "        self.balance = res[\"balance\"]\n",
    "\n",
    "    def train(self, choosing=False, verbose=1):\n",
    "        \"\"\"\n",
    "        è½½å…¥æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹(sklearn.estimator.fit)\n",
    "        Train the model, if data isn't loaded, it will be loaded automatically\n",
    "\n",
    "        X_train=None, y_train=None\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            # if data isn't loaded yet, load it then\n",
    "            self.load_data()\n",
    "        if self.verbose > 1:\n",
    "            print(\"@{self.model}:\")\n",
    "            print(self.model)\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        if not self.model_trained or choosing:\n",
    "            X_train = self.X_train\n",
    "            y_train = self.y_train\n",
    "            # estimatorè®­ç»ƒ(fit)æ¨¡å‹\n",
    "            model.fit(X=X_train, y=y_train)\n",
    "            self.model_trained = True\n",
    "        if self.verbose > 1:\n",
    "            if choosing == True:\n",
    "                print(\n",
    "                    f\"[I] Model trained with{choosing=},choosing the best model,override the trained model..\"\n",
    "                )\n",
    "\n",
    "    def predict(self, audio_path):\n",
    "        \"\"\"\n",
    "        é¢„æµ‹å•ä¸ªéŸ³é¢‘çš„æƒ…æ„Ÿ\n",
    "        ç”±äºæ˜¯å•ä¸ªéŸ³é¢‘çš„æƒ…æ„Ÿé¢„æµ‹,å› æ­¤ä¸éœ€è¦è€ƒè™‘shuffleå’Œbalanceè¿™äº›æ“ä½œ,åªéœ€è¦æå–è¯­éŸ³ç‰¹å¾,ç„¶åè¿›è¡Œè°ƒç”¨æ¨¡å‹é¢„æµ‹å³å¯\n",
    "        given an `audio_path`, this method extracts the features\n",
    "        and predicts the emotion\n",
    "\n",
    "        ä»¥ä¸‹è¯­å¥ä¸å†é€‚åˆå…·æœ‰pcaé™ç»´æ“ä½œä¸‹çš„æƒ…å½¢\n",
    "        feature_audio = extract_feature_of_audio(audio_path, self.f_config)\n",
    "        print(feature1.shape)\n",
    "        print(feature1,\"@{feature1}\",feature1.shape)\n",
    "        feature2=feature1.T\n",
    "        print(feature2,\"@{feature2}\",feature2.shape)\n",
    "        print(feature3,\"@{feature3}\",feature3.shape)\n",
    "        \"\"\"\n",
    "        feature_audio = self.extract_feature_single_audio(audio_path)\n",
    "\n",
    "        feature = feature_audio.reshape(1, -1)\n",
    "        model = self.model if self.model else self.best_model()\n",
    "        res = model.predict(feature)\n",
    "        # reså¯èƒ½æ˜¯ä¸ªåˆ—è¡¨\n",
    "        # print(res, \"@{res}\")\n",
    "        return res[0]\n",
    "\n",
    "    def extract_feature_single_audio(self, audio_path):\n",
    "        \"\"\"extract a single audio file feature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        audio_path : path\n",
    "            audio path\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            audio feature\n",
    "        \"\"\"\n",
    "        ae: AudioExtractor = self.ae\n",
    "        pca = ae.pca\n",
    "        print(pca, \"@{pca} in 'predict' method\")\n",
    "        # if pca:\n",
    "        #     feature_audio=pca.transform(feature_audio)\n",
    "        #     print(feature_audio.shape, \"@{feature_audio.shape}\")\n",
    "        feature_audio = ae.extract_features(partition=\"test\", audio_paths=[audio_path])\n",
    "        return feature_audio\n",
    "        # return self.model.predict(feature2)[0]\n",
    "\n",
    "    def peek_test_set(self, n=5):\n",
    "        res = [\n",
    "            self.test_audio_paths[:n],\n",
    "            self.X_test[:n],\n",
    "            self.y_test[:n],\n",
    "            self.y_pred[:n],\n",
    "        ]\n",
    "        return res\n",
    "\n",
    "    def predict_proba(self, audio_path):\n",
    "        \"\"\"\n",
    "        Predicts the probability of each emotion.\n",
    "        \"\"\"\n",
    "        if self.classification_task:\n",
    "            # feature = extract_feature_of_audio(audio_path, self.f_config).reshape(1, -1)\n",
    "            feature = self.extract_feature_single_audio(audio_path)\n",
    "            proba = self.model.predict_proba(feature)[0]\n",
    "            result = {}\n",
    "            for emotion, prob in zip(self.model.classes_, proba):\n",
    "                result[emotion] = prob\n",
    "            return result\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Probability prediction doesn't make sense for regression\"\n",
    "            )\n",
    "\n",
    "    def show_second(self):\n",
    "        peeker = er.peek_test_set(2)\n",
    "        feature = peeker[1][1]\n",
    "        audio_path = peeker[0][1]\n",
    "        feature_pred = self.predict(audio_path)\n",
    "        print(feature[:5], feature_pred[:5])\n",
    "\n",
    "    def grid_search(self, params, n_jobs=2, verbose=3):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨ç½‘æ ¼åŒ–æœç´¢çš„æ–¹å¼æœç´¢æœ€ä¼˜è¶…å‚æ•°\n",
    "        Performs GridSearchCV on `params` passed on the `self.model`\n",
    "        And returns the tuple: (best_estimator, best_params, best_score).\n",
    "        \"\"\"\n",
    "        score = accuracy_score if self.classification_task else mean_absolute_error\n",
    "        grid = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=params,\n",
    "            scoring=make_scorer(score),\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "            cv=3,\n",
    "        )\n",
    "        # è°ƒç”¨fitå¼€å§‹ä¼ å…¥æ•°æ®é›†å¹¶æœç´¢\n",
    "        X_train, y_train = self.X_train, self.y_train\n",
    "        if X_train is None or y_train is None:\n",
    "            raise ValueError(\"X_train and y_train are None\")\n",
    "        # fitè¿‡ç¨‹æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹\n",
    "        grid_result = grid.fit(X_train, y_train)\n",
    "        return (\n",
    "            grid_result.best_estimator_,\n",
    "            grid_result.best_params_,\n",
    "            grid_result.best_score_,\n",
    "        )\n",
    "\n",
    "    def best_model(self):\n",
    "        \"\"\"\n",
    "        ä»å¸¸è§çš„æ¨¡å‹ä¸­è®¡ç®—å‡ºæœ€å¥½çš„Estimator(model)\n",
    "        è®¡ç®—æœ€ä¼˜modelæ—¶,ä¹Ÿå¯ä»¥è€ƒè™‘åˆ›å»ºæ–°çš„ERå®ä¾‹æ¥åšè®¡ç®—æœ€ä¼˜modelçš„ç”¨é€”,ä½†ä¼šå¢åŠ å¼€é”€\n",
    "\n",
    "        Loads best estimators and determine which is best for test data,\n",
    "        and then set it to `self.model`.\n",
    "        # ä½¿ç”¨MSEæ¥è¯„ä»·å›å½’æ¨¡å‹,ä½¿ç”¨accuracyæ¥è¯„ä»·åˆ†ç±»æ¨¡å‹\n",
    "        In case of regression, the metric used is MSE(å‡æ–¹è¯¯å·®) and accuracy for classification.\n",
    "\n",
    "        Note that the execution of this method may take several minutes due\n",
    "        to training all estimators (stored in `grid` folder) for determining the best possible one.\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            self.load_data()\n",
    "\n",
    "        # loads estimators\n",
    "        estimators = best_estimators()\n",
    "\n",
    "        result = []\n",
    "\n",
    "        if self.verbose:\n",
    "            # æ§åˆ¶æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "            # é€šè¿‡tqdmå°è£…estimatorè¿™ä¸ªå¯è¿­ä»£å¯¹è±¡,å°±å¯ä»¥åœ¨éå†estimatoræ—¶,æ§åˆ¶è¿›åº¦æ¡çš„æ˜¾ç¤º\n",
    "            estimators = tqdm(estimators)\n",
    "\n",
    "        for epc in estimators:\n",
    "            estimator, params_, cv_score_ = epc\n",
    "            ecn = estimator.__class__.__name__\n",
    "\n",
    "            if self.verbose:\n",
    "                # å¦‚æœå¯ç”¨verboseé€‰é¡¹,é‚£ä¹ˆestimatorsä¼šè¢«tqdmåŒ…è£…\n",
    "                # æ­¤æ—¶å¯ä»¥é€šè¿‡set_descriptionæ–¹æ³•æ¥ä¿®æ”¹è¿›åº¦æ¡çš„æè¿°ä¿¡æ¯\n",
    "                # æ¯”å¦‚,estimators.set_description(f\"Evaluating {estimator.__class__.__name__}\")\n",
    "                estimators.set_description(f\"Evaluating <{ecn}>\")\n",
    "            # ä¸ºä¾‹é¿å…ç›¸äº’å¹²æ‰°,æ¯æµ‹è¯•æ¨¡å‹å°±åˆ›å»ºä¸€ä¸ªERå¯¹è±¡(er)\n",
    "            er = EmotionRecognizer(\n",
    "                model=estimator,\n",
    "                emotions=self.e_config,\n",
    "                classification_task=self.classification_task,\n",
    "                f_config=self.f_config,\n",
    "                balance=self.balance,\n",
    "                override_csv=False,\n",
    "                verbose=0,\n",
    "            )\n",
    "            # data already loaded\n",
    "            er.X_train = self.X_train\n",
    "            er.X_test = self.X_test\n",
    "            er.y_train = self.y_train\n",
    "            er.y_test = self.y_test\n",
    "            er.data_loaded = True\n",
    "            # train the model\n",
    "            er.train(verbose=0)\n",
    "            # get test accuracy\n",
    "            accuracy = er.test_score()\n",
    "            # append to result\n",
    "            result.append((er.model, accuracy))\n",
    "\n",
    "            # æ–¹æ³•2:(å°å¿ƒä½¿ç”¨)\n",
    "            # ä½¿ç”¨æœ¬å¯¹è±¡selfè€Œä¸æ˜¯åœ¨åˆ›å»ºä¸€ä¸ªERå¯¹è±¡\n",
    "            # self.model = estimator\n",
    "            # er = self\n",
    "            # ä»¥ä¸‹çš„è®¡ç®—æ˜¯ç”¨æ¥é€‰å‡ºmodelçš„,è€Œä¸æ˜¯ç›´æ¥ä½œä¸ºselfå¯¹è±¡çš„å±æ€§,è¿™é‡Œå°†selfèµ‹å€¼ç»™er,ä»¥ç¤ºåŒºåˆ«\n",
    "            # train(fit) the model\n",
    "            # å¦‚æœè®¾ç½®verbose=1,åˆ™ä¼šé€ä¸ªæ‰“å°å½“å‰è®¡ç®—çš„æ¨¡å‹(è¿›åº¦ä¸æ˜¯åŒä¸€æ¡)\n",
    "            # er.train(choosing=True, verbose=0)\n",
    "            # train(fit) the model\n",
    "            # self.train(verbose=1)\n",
    "            # accuracy = er.test_score(choosing=True)\n",
    "            # append to result\n",
    "            # result.append((estimator, accuracy))\n",
    "\n",
    "            print(f\"\\n[I] {ecn} with {accuracy} test accuracy\")\n",
    "\n",
    "        # sort the result\n",
    "        # regression: best is the lower, not the higher\n",
    "        # classification: best is higher, not the lower\n",
    "        result = sorted(\n",
    "            result, key=lambda item: item[1], reverse=self.classification_task\n",
    "        )\n",
    "        best_estimator = result[0][0]\n",
    "        accuracy = result[0][1]\n",
    "\n",
    "        self.model = best_estimator\n",
    "        self.model_trained = True\n",
    "        if self.verbose:\n",
    "            if self.classification_task:\n",
    "                print(\n",
    "                    f\"[ğŸˆ] Best model : {self.model.__class__.__name__} with {accuracy * 100:.3f}% test accuracy\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[I] Best model : {self.model.__class__.__name__} with {accuracy:.5f} mean absolute error\"\n",
    "                )\n",
    "        return best_estimator\n",
    "\n",
    "    def test_score(self, choosing=False, verbose=0, report=False):\n",
    "        \"\"\"\n",
    "        Calculates score on testing data\n",
    "        Please call the `train` method before call this method.\n",
    "\n",
    "        just like sklearn convention:call estimator.call `fit` at first,then call `predict` or `score` method\n",
    "\n",
    "        if `self.classification` is True, the metric used is accuracy,\n",
    "        Mean-Squared-Error is used otherwise (regression)\n",
    "\n",
    "        1.è°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
    "\n",
    "        2.å¦‚æœmodelæ˜¯None,é‚£ä¹ˆè°ƒç”¨best_modelè·å–æœ€ä¼˜æ¨¡å‹(è¿™ä¸ªè¿‡ç¨‹ä¼šéå†ä¸€ä¸ªå¯ç”¨æ¨¡å‹åˆ—è¡¨,æ˜¯é€šè¿‡è°ƒç”¨ERå®ä¾‹çš„test_score()æ–¹æ³•æ¥è®¡ç®—,ä¸è¿‡è¿™é‡Œä¸ä¼šé‡åˆ°Noneçš„æƒ…å†µ,å› æ­¤é—´æ¥é€’å½’è°ƒç”¨ä¸è¶…è¿‡2å±‚)\n",
    "        \"\"\"\n",
    "        X_test = self.X_test\n",
    "        y_test = self.y_test\n",
    "\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        self.validate_empty_array(X_test=X_test, y_test=y_test)\n",
    "\n",
    "        # é¢„æµ‹è®¡ç®—\n",
    "        if verbose > 1:\n",
    "            print(X_test.shape, y_test.shape, \"ğŸˆ\")\n",
    "        # æ ¹æ®å½“å‰æ¨¡å‹è¿›è¡Œé¢„æµ‹(ç›´æ¥è°ƒç”¨estimatorçš„predictæ–¹æ³•)\n",
    "        y_pred = model.predict(X_test)  # type: ignore\n",
    "        # å¦‚æœå¤„äºbest_modelçš„è¿‡ç¨‹ä¸­è°ƒç”¨æœ¬æ–¹æ³•(choosing=True),åˆ™ä¸Šè¿°é¢„æµ‹å†…å®¹ä½œä¸ºä¸´æ—¶ç»“æœä¸å†™å…¥å¯¹è±¡å±æ€§ä¿å­˜;å¦åˆ™ä½œä¸ºæœ€ç»ˆç»“æœå†™å…¥å±æ€§ä¿å­˜\n",
    "        # é»˜è®¤choosing=False,ä¹Ÿå°±æ˜¯å°†ç»“æœä¿å­˜åˆ°å¯¹è±¡å±æ€§ä¸­\n",
    "        if choosing == False:\n",
    "            self.y_pred = np.array(y_pred)\n",
    "\n",
    "        if self.classification_task:\n",
    "            res = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "            # ç»“æœå’Œ:er.model.score(er.X_test,er.y_test)ä¸€æ ·,ä½†æ˜¯è¿™ç§åšæ³•å›ç‹¬ç«‹å°†X_testé¢„æµ‹ä¸€é,è€Œä¸ä¿å­˜é¢„æµ‹ç»“æœ,åªç»™å‡ºå¾—åˆ†\n",
    "        else:\n",
    "            res = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "        if report:\n",
    "            self.check_report(y_test, y_pred)\n",
    "        return res\n",
    "\n",
    "    def check_report(self):\n",
    "        \"\"\"è¾“å‡ºæ¨¡å‹å½“å‰ç»“æœçš„å¤šä¸ªæŒ‡æ ‡æŠ¥å‘Š\n",
    "        å¯¹äºåˆ†ç±»ä»»åŠ¡,åŒ…æ‹¬precision(æŸ¥å‡†ç‡),recall(å›å¬æˆ–æŸ¥å…¨ç‡),f1-scoreä»¥åŠå„ç±»åˆ«çš„æ ·æœ¬æ•°é‡\n",
    "        å¯¹äºå¤šåˆ†ç±»,è¿˜æœ‰ä¸€äº›ç»¼åˆçš„æŒ‡æ ‡(macro,weighted),æ¯ä¸ªå•å…ƒæ ¼ç»“åˆä¸¤ä¸ªç»´åº¦çš„è¡¨å¤´è¿›è¡Œç†è§£å’Œé˜…è¯»\n",
    "\n",
    "        ç”±äºè·¨åº“å®éªŒæ¯”è¾ƒå›°éš¾,æœ‰çš„æ ·æœ¬ç±»åˆ«æ— æ³•è¢«æ­£ç¡®åˆ†ç±»(æ‰€æœ‰è¯¥ç±»åˆ«éƒ½è¢«é”™è¯¯åˆ†ç±»),æ­¤æ—¶classification_reportæ–¹æ³•ä¼šæå‡ºè­¦å‘Š,é™¤éä½¿ç”¨zero_divisionå‚æ•°æ›¿æ¢æ‰é»˜è®¤çš„warn.\n",
    "        \"\"\"\n",
    "        y_test = self.y_test\n",
    "        y_pred = self.y_pred\n",
    "\n",
    "        report = classification_report(y_true=y_test, y_pred=y_pred, zero_division=0)\n",
    "        # print(report, self.model.__class__.__name__)\n",
    "        return report\n",
    "\n",
    "    def model_cv_score(\n",
    "        self,\n",
    "        choosing=False,\n",
    "        verbose=1,\n",
    "        mean_only=True,\n",
    "        n_splits=5,\n",
    "        test_size=0.2,\n",
    "        cv_mode=\"sss\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨äº¤å‰éªŒè¯çš„æ–¹å¼æ¥è¯„ä¼°æ¨¡å‹\n",
    "        Calculates score on testing data\n",
    "        \"\"\"\n",
    "        X_train = self.X_train\n",
    "        y_train = self.y_train\n",
    "        # è°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        self.validate_empty_array(X_train, y_train)\n",
    "\n",
    "        # é¢„æµ‹è®¡ç®—\n",
    "        if verbose > 1:\n",
    "            print(X_train.shape, y_train.shape, \"ğŸˆ\")\n",
    "            print(f\"{n_splits=}\")\n",
    "        n_splits = int(n_splits)\n",
    "\n",
    "        y_pred = model.predict(X_train)  # type: ignore\n",
    "        if choosing == False:\n",
    "            self.y_pred = np.array(y_pred)\n",
    "        # äº¤å‰éªŒè¯çš„æ–¹å¼è¯„ä¼°æ¨¡å‹çš„å¾—åˆ†\n",
    "        cv_mode_dict = dict(\n",
    "            sss=StratifiedShuffleSplit(\n",
    "                n_splits=n_splits, test_size=test_size, random_state=0\n",
    "            ),\n",
    "            ss=ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=0),\n",
    "            kfold=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "            skfold=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "        )\n",
    "        cv_mode_selected = cv_mode_dict[cv_mode]\n",
    "        if verbose > 1:\n",
    "            print(f\"{cv_mode=}ğŸˆ\")\n",
    "        res = [0]\n",
    "        if self.classification_task:\n",
    "            # res = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "            # å°†äº¤å‰éªŒè¯å™¨cvä¼ é€’ç»™cross_val_scoreå‡½æ•°æ‰§è¡Œè¯„ä¼°æ“ä½œ\n",
    "            # è€Œéè‡ªå·±ä½¿ç”¨æ¥å®ŒæˆkæŠ˜äº¤å‰éªŒè¯\n",
    "            # æ‰€æœ‰å¯¹è±¡éƒ½æœ‰__class__å±æ€§ä»¥åŠ__name__äºŒçº§å±æ€§\n",
    "            if model.__class__.__name__ in ava_ML_algorithms:\n",
    "                res = cross_val_score(model, X_train, y_train, cv=cv_mode_selected)\n",
    "                if mean_only:\n",
    "                    res = res.mean()\n",
    "\n",
    "        else:\n",
    "            # ä½¿ç”¨å›å½’å™¨çš„æƒ…å†µ\n",
    "            res = mean_squared_error(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "        if self.verbose > 2:\n",
    "            report = classification_report(\n",
    "                y_true=y_train, y_pred=y_pred\n",
    "            )  # è®­ç»ƒé›†ä¸Š,å‡ ä¹æ€»æ˜¯æ˜¯æ»¡åˆ†\n",
    "            print(report, self.model.__class__.__name__)\n",
    "        return res\n",
    "\n",
    "    def validate_empty_array(self, X_test=[], y_test=[]):\n",
    "        if len(X_test) == 0:\n",
    "            raise ValueError(\"X is empty\")\n",
    "        if len(y_test) == 0:\n",
    "            raise ValueError(\"y is empty\")\n",
    "\n",
    "    def meta_paths_of_db(self, db, partition=\"test\"):\n",
    "        res = meta_paths_of_db(\n",
    "            db=db,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=partition,\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    def update_test_set(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def update_test_set_by_meta(self, test_meta):\n",
    "        \"\"\"\n",
    "        è¿™ä¸ªå‡½æ•°è®¾è®¡ç”¨æ¥åšè·¨åº“è¯†åˆ«è¯•éªŒ\n",
    "        ä»…ä»…æ›¿æ¢æµ‹è¯•é›†ä¸ºä¸åŒåº“,æœ¬èº«æ²¡æœ‰é’ˆå¯¹è·¨åº“è¿›è¡Œä¼˜åŒ–\n",
    "\n",
    "\n",
    "        Load test data from given test metadata file paths and update instance's test set attributes.\n",
    "\n",
    "        Args:\n",
    "            test_meta (list of str): List of file paths of test metadata files.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        examples:\n",
    "        >>> rec = EmotionRecognizer(model=my_model,**meta_dict, verbose=1)\n",
    "        >>> rec.train()\n",
    "        >>> rec.update_test_set_by_meta(test_emodb_csv)\n",
    "        >>> rec.test_meta_files\n",
    "        >>> 'meta_files\\\\test_emodb_HNS.csv'\n",
    "        >>> rec.X_test.shape\n",
    "        >>> (43,180)\n",
    "\n",
    "        >>> rec.test_score()\n",
    "        >>> 0.4651\n",
    "\n",
    "        \"\"\"\n",
    "        # rec.update_test_set_by_meta(test_emodb_csv)\n",
    "        self.test_meta_files = test_meta\n",
    "        print(test_meta, \"@{test_meta}\")\n",
    "        test_data = load_data_from_meta(\n",
    "            test_meta_files=test_meta, e_config=self.e_config, f_config=self.f_config\n",
    "        )\n",
    "\n",
    "        X_test = test_data[\"X_test\"]\n",
    "        y_test = test_data[\"y_test\"]\n",
    "        # è®¾ç½®å®ä¾‹çš„å„ä¸ªå±æ€§\n",
    "        self.test_audio_paths = test_data[\"test_audio_paths\"]\n",
    "        self.update_test_set(X_test, y_test)\n",
    "\n",
    "    def train_score(self, X_train=None, y_train=None):\n",
    "        \"\"\"\n",
    "        Calculates accuracy score on training data\n",
    "        if `self.classification` is True, the metric used is accuracy,\n",
    "        Mean-Squared-Error is used otherwise (regression)\n",
    "        \"\"\"\n",
    "        if X_train is None or y_train is None:\n",
    "            X_train = self.X_train\n",
    "            y_train = self.y_train\n",
    "        y_pred = self.model.predict(X_train)\n",
    "        if self.classification_task:\n",
    "            return accuracy_score(y_true=y_train, y_pred=y_pred)\n",
    "        else:\n",
    "            return mean_squared_error(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    def train_fbeta_score(self, beta):\n",
    "        y_pred = self.model.predict(self.X_train)\n",
    "        y_train = self.y_train\n",
    "        if y_train is None:\n",
    "            raise ValueError(\"y_train is None\")\n",
    "\n",
    "        return fbeta_score(y_true=y_train, y_pred=y_pred, beta=beta, average=\"micro\")\n",
    "\n",
    "    def test_fbeta_score(self, beta):\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        if y_test is None:\n",
    "            raise ValueError(\"y_test is None\")\n",
    "        return fbeta_score(y_true=y_test, y_pred=y_pred, beta=beta, average=\"micro\")\n",
    "\n",
    "    def confusion_matrix(self, percentage=True, labeled=True):\n",
    "        \"\"\"\n",
    "        Computes confusion matrix to evaluate the test accuracy of the classification\n",
    "        and returns it as numpy matrix or pandas dataframe (depends on params).\n",
    "        params:\n",
    "            percentage (bool): whether to use percentage instead of number of samples, default is True.\n",
    "            labeled (bool): whether to label the columns and indexes in the dataframe.\n",
    "        \"\"\"\n",
    "        if not self.classification_task:\n",
    "            raise NotImplementedError(\n",
    "                \"Confusion matrix works only when it is a classification problem\"\n",
    "            )\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        if y_test is None:\n",
    "            raise ValueError(\"y_test is None\")\n",
    "        matrix = confusion_matrix(\n",
    "            y_true=y_test, y_pred=y_pred, labels=self.e_config\n",
    "        ).astype(np.float32)\n",
    "        if percentage:\n",
    "            for i in range(len(matrix)):\n",
    "                matrix[i] = matrix[i] / np.sum(matrix[i])\n",
    "            # make it percentage\n",
    "            matrix *= 100\n",
    "        if labeled:\n",
    "            matrix_df = pd.DataFrame(\n",
    "                matrix,\n",
    "                index=[f\"true_{e}\" for e in self.e_config],\n",
    "                columns=[f\"predicted_{e}\" for e in self.e_config],\n",
    "            )\n",
    "        return matrix_df\n",
    "\n",
    "    def draw_confusion_matrix(self):\n",
    "        \"\"\"Calculates the confusion matrix and shows it\"\"\"\n",
    "        matrix = self.confusion_matrix(percentage=False, labeled=False)\n",
    "        # TODO: add labels, title, legends, etc.\n",
    "        pl.imshow(matrix, cmap=\"binary\")\n",
    "        pl.show()\n",
    "\n",
    "    def count_samples_in_partition(self, emotion, partition):\n",
    "        \"\"\"\n",
    "        Get the number of data samples of the `emotion` class in a particular `partition` ('test' or 'train').\n",
    "\n",
    "        :param emotion: The emotion class to count.\n",
    "        :param partition: The partition to count samples in ('test' or 'train').\n",
    "        :return: The number of data samples of the `emotion` class in the `partition`.\n",
    "        :raises ValueError: If `y_test` or `y_train` is `None`.\n",
    "        \"\"\"\n",
    "        partition = validate_partition(partition, Noneable=False)\n",
    "        if partition == \"test\":\n",
    "            if self.y_test is None:\n",
    "                raise ValueError(\"y_test is None\")\n",
    "            count = sum(1 for y in self.y_test if y == emotion)\n",
    "        else:\n",
    "            if self.y_train is None:\n",
    "                raise ValueError(\"y_train is None\")\n",
    "            count = sum(1 for y in self.y_train if y == emotion)\n",
    "        return count\n",
    "\n",
    "    def count_samples_by_class(self):\n",
    "        \"\"\"\n",
    "        Returns a dataframe that contains the number of training\n",
    "        and testing samples for all emotions.\n",
    "        Note that if data isn't loaded yet, it'll be loaded\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            self.load_data()\n",
    "        train_samples = []\n",
    "        test_samples = []\n",
    "        total = []\n",
    "        for emotion in self.e_config:\n",
    "            n_train = self.count_samples_in_partition(emotion, \"train\")\n",
    "            n_test = self.count_samples_in_partition(emotion, \"test\")\n",
    "            train_samples.append(n_train)\n",
    "            test_samples.append(n_test)\n",
    "            total.append(n_train + n_test)\n",
    "\n",
    "        # get total\n",
    "        total.append(sum(train_samples) + sum(test_samples))\n",
    "        train_samples.append(sum(train_samples))\n",
    "        test_samples.append(sum(test_samples))\n",
    "        return pd.DataFrame(\n",
    "            data={\"train\": train_samples, \"test\": test_samples, \"total\": total},\n",
    "            index=self.e_config + [\"total\"],\n",
    "        )\n",
    "\n",
    "    def get_random_emotion_index(self, emotion, partition=\"train\"):\n",
    "        \"\"\"\n",
    "        Returns a random index of a `partition` sample with the given `emotion`.\n",
    "\n",
    "        Args:\n",
    "            emotion (str): The name of the emotion to look for.\n",
    "            partition (str): The partition to sample from. Only \"train\" or \"test\" are accepted.\n",
    "\n",
    "        Returns:\n",
    "            int: The index of a random sample with the given `emotion` in the specified `partition`.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If `partition` is not \"train\" or \"test\".\n",
    "        \"\"\"\n",
    "        partition = validate_partition(partition, Noneable=False)\n",
    "        indices = []\n",
    "        if partition == \"train\":\n",
    "            indices = [i for i, y in enumerate(self.y_train) if y == emotion]\n",
    "        elif partition == \"test\":\n",
    "            indices = [i for i, y in enumerate(self.y_test) if y == emotion]\n",
    "\n",
    "        return random.choice(indices)\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# def get_stacking_clf():\n",
    "#     random_state = 42\n",
    "#     # å®šä¹‰åˆçº§å­¦ä¹ å™¨\n",
    "#     estimators_basic_mix = estrs_basic_mix( )\n",
    "#     estimators_dt = estrs_esdt(random_state)\n",
    "#     estimators_basic_dt_mix = estrs_basic_dt_mix( )\n",
    "#     estimators_basic_dt_mpl_mix = estrs_basic_dt_mpl_mix(\n",
    "#         random_state, estimators_basic_dt_mix\n",
    "#     )\n",
    "#     estimators_basic_dmb_mix = estimators_basic_dt_mix + [(\"gnb\", GaussianNB())]\n",
    "#     # ç®€å•å †å \n",
    "#     stack_dt_linear = StackingClassifier(\n",
    "#         estimators=estimators_dt, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     stack_mix_linear = StackingClassifier(\n",
    "#         estimators=estimators_basic_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     # æ”¶æ•›å›°éš¾(æ•ˆæœä¸å¦‚mix_linear)\n",
    "#     stack_mix_svm = StackingClassifier(\n",
    "#         estimators=estimators_basic_mix,\n",
    "#         final_estimator=SVC(),\n",
    "#     )\n",
    "#     stack_basic_dt_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dt_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     # æ•°æ®é‡å¤ªå°\n",
    "#     stack_basic_dt_mlp_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dt_mpl_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     stack_dmb_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dmb_mix,\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#     )\n",
    "#     # stack1 = StackingClassifier(\n",
    "#     #     estimators=[(\"gbc\", GaussianNB())],\n",
    "#     #     final_estimator=LogisticRegression(),\n",
    "#     # )\n",
    "#     # å¤šå±‚å †å \n",
    "#     ##å®šä¹‰æœ€åä¸€å±‚\n",
    "#     stack_final_layer = StackingClassifier(\n",
    "#         estimators=[(\"gbc\", GradientBoostingClassifier()), (\"svc\", SVC())],\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#     )\n",
    "#     ##å †å äºŒå±‚(å®¹æ˜“è¿‡æ‹Ÿåˆ)\n",
    "#     stack_multilayer = StackingClassifier(\n",
    "#         estimators=estimators_dt, final_estimator=stack_final_layer\n",
    "#     )\n",
    "#     stack = StackingClassifier(estimators=estimators_dt, final_estimator=SVC())\n",
    "\n",
    "#     # return stack_dt_linear\n",
    "#     # return stack_mix_linear\n",
    "#     # return stack_mix_svm\n",
    "#     # return stack_basic_dt_mix\n",
    "#     return stack_dmb_mix\n",
    "\n",
    "\n",
    "def estrs_basic_mlp_mix():\n",
    "    estimators_basic_dt_mpl_mix = estrs_basic_mix() + [\n",
    "        (\n",
    "            \"mpl\",\n",
    "            MLPClassifier(\n",
    "                alpha=0.01,\n",
    "                batch_size=512,\n",
    "                hidden_layer_sizes=(300,),\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=400,\n",
    "                random_state=random_state,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return estimators_basic_dt_mpl_mix\n",
    "\n",
    "\n",
    "def estrs_esdt():\n",
    "    estimators_dt = [\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=10, max_depth=3, random_state=random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"adab\",\n",
    "            AdaBoostClassifier(\n",
    "                n_estimators=10, learning_rate=0.1, random_state=random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\"gradb\", GradientBoostingClassifier()),\n",
    "    ]\n",
    "\n",
    "    return estimators_dt\n",
    "\n",
    "\n",
    "def estrs_basic_esdt_mix():\n",
    "    res = estrs_basic_mix() + estrs_esdt()\n",
    "    return res\n",
    "\n",
    "def estrs_simple():\n",
    "    res=[\n",
    "         (\"knc\", KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")),\n",
    "         (\"gnb\",GaussianNB())\n",
    "    ]\n",
    "    return res\n",
    "def estrs_basic_mix():\n",
    "    \"\"\"åŒ…å«å¸¸ç”¨çš„ä¸ªä½“å­¦ä¹ å™¨,å¯ä»¥ä½œä¸ºStackingçš„ç¬¬ä¸€å±‚\n",
    "    è¿˜å¯ä»¥ä½œä¸ºæ›´åŠ å¤æ‚çš„ç¬¬ä¸€å±‚çš„åŸºç¡€éƒ¨åˆ†,é‡‡ç”¨åˆ—è¡¨ç›¸åŠ çš„æ–¹å¼è¿›ä¸€æ­¥æ‰©å……\n",
    "    ä¾‹å¦‚æ·»åŠ é›†æˆå­¦ä¹ (éšæœºæ£®æ—,æ¢¯åº¦æå‡ç­‰)\n",
    "\n",
    "    å…·ä½“åŒ…æ‹¬ä»¥ä¸‹æ¨¡å‹:\n",
    "\n",
    "    - çº¿æ€§æ¨¡å‹(lsvr,rdcv,logistic)\n",
    "    - kè¿‘é‚»(knc)\n",
    "    - è´å¶æ–¯å†³ç­–(gnb)\n",
    "    - å†³ç­–æ ‘(dt)\n",
    "\n",
    "    è¿™äº›ä¸ªä½“å­¦ä¹ å™¨è¾ƒä¸ºå¤šæ ·,ç†è®ºä¸Šæœ‰åˆ©äºæé«˜é›†æˆå­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[estimator]\n",
    "        åŸºç¡€ä¸ªä½“å­¦ä¹ å™¨åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    estimators_basic_mix = [\n",
    "        # (\"svc\",(SVC(C=10, gamma=0.001,random_state=random_state))),\n",
    "        # (\"lsvr\", make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))),\n",
    "        (\"lsvr\", (LinearSVC(max_iter=5000,random_state=random_state))),\n",
    "        (\"rdcv\", RidgeClassifierCV()),\n",
    "        (\"logistic\", LogisticRegression()),\n",
    "        (\"knc\", KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")),\n",
    "        (\"gnb\", GaussianNB()),\n",
    "        (\n",
    "            \"dtc\",\n",
    "            DecisionTreeClassifier(\n",
    "                criterion=\"entropy\", max_depth=7, max_features=\"sqrt\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return estimators_basic_mix\n",
    "\n",
    "\n",
    "def get_clfs():\n",
    "    \"\"\"éœ€è¦è¯„ä¼°å’Œåˆ†æå¯¹æ¯”çš„ä¼°è®¡å™¨\n",
    "    svc = SVC(C=0.001, gamma=0.001, kernel=\"poly\", probability=True)\n",
    "    knn=KNeighborsClassifier(n_neighbors=3, p=1, weights='distance')\n",
    "    Best for DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "    dt = DecisionTreeClassifier(\n",
    "        criterion=\"entropy\",\n",
    "        max_depth=7,\n",
    "        max_features=None,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=2\n",
    "    )\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[estimator]\n",
    "        sklearn.estimatorä¼°è®¡å™¨åˆ—è¡¨\n",
    "    \"\"\"\n",
    "\n",
    "    rfc = RandomForestClassifier()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    lsvc = LinearSVC(max_iter=5000, random_state=random_state)\n",
    "    svc = SVC()\n",
    "    mlp = MLPClassifier(max_iter=3000)\n",
    "    rdcv = RidgeClassifierCV()\n",
    "    gnb = GaussianNB()\n",
    "    plsvc = make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))\n",
    "\n",
    "    # stack = get_stacking_clf()\n",
    "    stack_basic_svc=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=SVC()\n",
    "    )\n",
    "    stack_basic_rf=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=RandomForestClassifier()\n",
    "    )\n",
    "    stack_basic_mlp=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=MLPClassifier()\n",
    "    )\n",
    "    # estrs_basic_esdt_mixä½œä¸ºç¬¬ä¸€å±‚\n",
    "    stack_basic_esdt_gnb=StackingClassifier(\n",
    "        estimators=estrs_basic_esdt_mix(),\n",
    "        final_estimator=GaussianNB()\n",
    "    )\n",
    "    #estrs_basic_mlp_mixä½œä¸ºç¬¬ä¸€å±‚\n",
    "\n",
    "    stack_basic_mlp_gnb=StackingClassifier(\n",
    "        estimators=estrs_basic_mlp_mix(),\n",
    "        final_estimator=GaussianNB()\n",
    "    )\n",
    "    stack_basic_mlp_lr=StackingClassifier(\n",
    "        estimators=estrs_basic_mlp_mix(),\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "    stack_simple=StackingClassifier(\n",
    "        estimators=estrs_simple(),\n",
    "        final_estimator=LinearSVC()\n",
    "    )\n",
    "    stack_multilayer=StackingClassifier(\n",
    "        estimators=estrs_basic_esdt_mix(),\n",
    "        final_estimator=stack_simple\n",
    "    )\n",
    "    # é…ç½®å¾…è¯„ä¼°çš„åˆ†ç±»å™¨çš„åˆ—è¡¨\n",
    "    clfs = [\n",
    "        stack_basic_svc,\n",
    "        stack_basic_rf,\n",
    "        stack_basic_mlp,\n",
    "        stack_basic_esdt_gnb,\n",
    "        stack_basic_mlp_gnb,\n",
    "        stack_basic_mlp_lr,\n",
    "        stack_simple,\n",
    "        stack_multilayer,\n",
    "        gnb,\n",
    "        rfc,\n",
    "        rdcv,\n",
    "        dt,\n",
    "        lsvc,\n",
    "        svc,\n",
    "        mlp,\n",
    "        plsvc,\n",
    "    ]\n",
    "    return clfs\n",
    "\n",
    "\n",
    "def main():\n",
    "    clfs = get_clfs()\n",
    "\n",
    "    passive_emo = [\"angry\", \"sad\"]\n",
    "    passive_emo_others = passive_emo + [\"others\"]\n",
    "    typical_emo = [\n",
    "        \"happy\",\n",
    "        #    \"neutral\",\n",
    "        \"sad\",\n",
    "    ]\n",
    "    AHSO = [\"angry\", \"neutral\", \"sad\", \"others\"]\n",
    "\n",
    "    AHNS = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "    e_config = typical_emo\n",
    "    f_config = [\"mfcc\"]\n",
    "    # f_config=f_config_def\n",
    "    \n",
    "    # model = get_stacking_clf()\n",
    "    # model=rf\n",
    "\n",
    "    # é…ç½®è¯­æ–™åº“\n",
    "    ## åŒåº“å®éªŒ\n",
    "    # meta_dict = mp.get_single_db_pair_dict(emodb)\n",
    "    ## è·¨åº“å®éªŒ\n",
    "    meta_dict = mp.emodb_savee\n",
    "    res_list = []\n",
    "    for i, clf in enumerate(clfs):\n",
    "        res_dict = assess_model(e_config, f_config, clf, meta_dict)\n",
    "        res_list.append(res_dict)\n",
    "    res_list.sort(key=lambda res_dict: res_dict[\"test_score\"], reverse=True)\n",
    "\n",
    "    for i, res_dict in enumerate(res_list):\n",
    "        print(i + 1, \"--\" * 30, \"\\n\")\n",
    "        for key, value in res_dict.items():\n",
    "            if key == \"er\":\n",
    "                print(key, \":\", value.model)\n",
    "            elif key in [\"report\", \"confusion_matrix\"]:\n",
    "                print(key, \":\\n\", value)\n",
    "            else:\n",
    "                print(key, \":\", value)\n",
    "        # print(model, \"@{model}\")\n",
    "        # print(f\"{train_score=}\")\n",
    "        # print(f\"{test_score=}\")\n",
    "        # # æŸ¥çœ‹æ··æ·†çŸ©é˜µ\n",
    "        # print(confusion_matrix)\n",
    "        # # æŸ¥çœ‹è¾…åŠ©æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š\n",
    "        # print(report)\n",
    "\n",
    "\n",
    "def assess_model(e_config, f_config, model, meta_dict):\n",
    "    er = EmotionRecognizer(\n",
    "        model=model,\n",
    "        **meta_dict,\n",
    "        e_config=e_config,\n",
    "        f_config=f_config,\n",
    "        balance=True,\n",
    "        cross=True,  # æ‰§è¡Œè·¨åº“ä»»åŠ¡,è°ƒæ•´æµ‹è¯•æ•°æ®é›†è¯»å…¥\n",
    "        verbose=0,\n",
    "        # std_scaler=False,\n",
    "        # pca_params=dict(n_components=39)\n",
    "        # std_scaler=False,\n",
    "        # pca={\"n_components\":\"mle\"}\n",
    "        # pca={'n_components': 60}\n",
    "    )\n",
    "    # æ˜¾ç¤ºè°ƒç”¨è®­ç»ƒæ–¹æ³•(ç›¸å½“äºè°ƒç”¨sklearn.estimator.fit)\n",
    "    er.train()\n",
    "    # è¯„ä¼°æ¨¡å‹çš„å„é¡¹æ€§èƒ½æŒ‡æ ‡\n",
    "    train_score = er.train_score()\n",
    "    test_score = er.test_score()\n",
    "    confusion_matrix = er.confusion_matrix()\n",
    "    report = er.check_report()\n",
    "    # cv_score = er.model_cv_score()\n",
    "\n",
    "    # print(model, \"@{model}\")\n",
    "    # print(f\"{train_score=}\")\n",
    "    # print(f\"{test_score=}\")\n",
    "    # # æŸ¥çœ‹æ··æ·†çŸ©é˜µ\n",
    "    # print(confusion_matrix)\n",
    "    # # æŸ¥çœ‹è¾…åŠ©æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š\n",
    "    # print(report)\n",
    "    # äº¤å‰éªŒè¯å¾—åˆ†\n",
    "    # print(f\"{cv_score=}\")\n",
    "\n",
    "    return dict(\n",
    "        er=er,\n",
    "        train_score=train_score,\n",
    "        test_score=test_score,\n",
    "        confusion_matrix=confusion_matrix,\n",
    "        report=report,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        er = main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=3,\n",
      "                                                       n_estimators=10,\n",
      "                                                       random_state=42)),\n",
      "                               ('adab',\n",
      "                                AdaBoostClassifier(learning_rate=0.1,\n",
      "                                                   n_estimators=10,\n",
      "                                                   random_state=42)),\n",
      "                               ('gradb', GradientBoostingClassifier())],\n",
      "                   final_estimator=StackingClassifier(estimators=[('knc',\n",
      "                                                                   KNeighborsClassifier(n_neighbors=3,\n",
      "                                                                                        p=1,\n",
      "                                                                                        weights='distance')),\n",
      "                                                                  ('gnb',\n",
      "                                                                   GaussianNB())],\n",
      "                                                      final_estimator=LinearSVC()))\n",
      "train_score : 1.0\n",
      "test_score : 0.7282608695652174\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        45.652176      54.347824\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.46      0.63        46\n",
      "         sad       0.65      1.00      0.79        46\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.82      0.73      0.71        92\n",
      "weighted avg       0.82      0.73      0.71        92\n",
      "\n",
      "2 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=RandomForestClassifier())\n",
      "train_score : 1.0\n",
      "test_score : 0.6739130434782609\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        34.782608      65.217392\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.35      0.52        46\n",
      "         sad       0.61      1.00      0.75        46\n",
      "\n",
      "    accuracy                           0.67        92\n",
      "   macro avg       0.80      0.67      0.64        92\n",
      "weighted avg       0.80      0.67      0.64        92\n",
      "\n",
      "3 ------------------------------------------------------------ \n",
      "\n",
      "er : LinearSVC(max_iter=5000, random_state=42)\n",
      "train_score : 1.0\n",
      "test_score : 0.6630434782608695\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        32.608696      67.391304\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.33      0.49        46\n",
      "         sad       0.60      1.00      0.75        46\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.80      0.66      0.62        92\n",
      "weighted avg       0.80      0.66      0.62        92\n",
      "\n",
      "4 ------------------------------------------------------------ \n",
      "\n",
      "er : RandomForestClassifier()\n",
      "train_score : 1.0\n",
      "test_score : 0.6086956521739131\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         21.73913      78.260872\n",
      "true_sad            0.00000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.22      0.36        46\n",
      "         sad       0.56      1.00      0.72        46\n",
      "\n",
      "    accuracy                           0.61        92\n",
      "   macro avg       0.78      0.61      0.54        92\n",
      "weighted avg       0.78      0.61      0.54        92\n",
      "\n",
      "5 ------------------------------------------------------------ \n",
      "\n",
      "er : RidgeClassifierCV()\n",
      "train_score : 1.0\n",
      "test_score : 0.5978260869565217\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        21.739130      78.260872\n",
      "true_sad           2.173913      97.826088\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.91      0.22      0.35        46\n",
      "         sad       0.56      0.98      0.71        46\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.73      0.60      0.53        92\n",
      "weighted avg       0.73      0.60      0.53        92\n",
      "\n",
      "6 ------------------------------------------------------------ \n",
      "\n",
      "er : DecisionTreeClassifier()\n",
      "train_score : 1.0\n",
      "test_score : 0.5978260869565217\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        43.478260      56.521736\n",
      "true_sad          23.913044      76.086960\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.65      0.43      0.52        46\n",
      "         sad       0.57      0.76      0.65        46\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.61      0.60      0.59        92\n",
      "weighted avg       0.61      0.60      0.59        92\n",
      "\n",
      "7 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=SVC())\n",
      "train_score : 1.0\n",
      "test_score : 0.5760869565217391\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        15.217391      84.782608\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.15      0.26        46\n",
      "         sad       0.54      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.77      0.58      0.48        92\n",
      "weighted avg       0.77      0.58      0.48        92\n",
      "\n",
      "8 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=MLPClassifier())\n",
      "train_score : 1.0\n",
      "test_score : 0.5760869565217391\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        15.217391      84.782608\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.15      0.26        46\n",
      "         sad       0.54      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.77      0.58      0.48        92\n",
      "weighted avg       0.77      0.58      0.48        92\n",
      "\n",
      "9 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('mpl',\n",
      "                                MLPClassifier(alpha=0.01, batch_size=512,\n",
      "                                              hidden_layer_sizes=(300,),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              max_iter=400, random_state=42))],\n",
      "                   final_estimator=LogisticRegression())\n",
      "train_score : 1.0\n",
      "test_score : 0.5760869565217391\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        15.217391      84.782608\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.15      0.26        46\n",
      "         sad       0.54      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.77      0.58      0.48        92\n",
      "weighted avg       0.77      0.58      0.48        92\n",
      "\n",
      "10 ------------------------------------------------------------ \n",
      "\n",
      "er : GaussianNB()\n",
      "train_score : 0.9895833333333334\n",
      "test_score : 0.5652173913043478\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        13.043478       86.95652\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.13      0.23        46\n",
      "         sad       0.53      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.57        92\n",
      "   macro avg       0.77      0.57      0.46        92\n",
      "weighted avg       0.77      0.57      0.46        92\n",
      "\n",
      "11 ------------------------------------------------------------ \n",
      "\n",
      "er : Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearsvc', LinearSVC(random_state=42))])\n",
      "train_score : 1.0\n",
      "test_score : 0.5652173913043478\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        13.043478       86.95652\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.13      0.23        46\n",
      "         sad       0.53      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.57        92\n",
      "   macro avg       0.77      0.57      0.46        92\n",
      "weighted avg       0.77      0.57      0.46        92\n",
      "\n",
      "12 ------------------------------------------------------------ \n",
      "\n",
      "er : MLPClassifier(max_iter=3000)\n",
      "train_score : 0.9791666666666666\n",
      "test_score : 0.5543478260869565\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        10.869565       89.13044\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.11      0.20        46\n",
      "         sad       0.53      1.00      0.69        46\n",
      "\n",
      "    accuracy                           0.55        92\n",
      "   macro avg       0.76      0.55      0.44        92\n",
      "weighted avg       0.76      0.55      0.44        92\n",
      "\n",
      "13 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=3,\n",
      "                                                       n_estimators=10,\n",
      "                                                       random_state=42)),\n",
      "                               ('adab',\n",
      "                                AdaBoostClassifier(learning_rate=0.1,\n",
      "                                                   n_estimators=10,\n",
      "                                                   random_state=42)),\n",
      "                               ('gradb', GradientBoostingClassifier())],\n",
      "                   final_estimator=GaussianNB())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "14 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('mpl',\n",
      "                                MLPClassifier(alpha=0.01, batch_size=512,\n",
      "                                              hidden_layer_sizes=(300,),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              max_iter=400, random_state=42))],\n",
      "                   final_estimator=GaussianNB())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "15 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB())],\n",
      "                   final_estimator=LinearSVC())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "16 ------------------------------------------------------------ \n",
      "\n",
      "er : SVC()\n",
      "train_score : 0.9895833333333334\n",
      "test_score : 0.5\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy              0.0          100.0\n",
      "true_sad                0.0          100.0\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.00      0.00      0.00        46\n",
      "         sad       0.50      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.50        92\n",
      "   macro avg       0.25      0.50      0.33        92\n",
      "weighted avg       0.25      0.50      0.33        92\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# file=r'D:\\repos\\CCSER\\SER\\data\\savee\\AudioData\\DC\\h01.wav'\n",
    "# file=meta.speech_dbs_dir/emodb/r'wav/03a01Fa.wav'\n",
    "# predict_res=er.predict(file)\n",
    "# print(f\"{predict_res=}\")\n",
    "# predict_proba=er.predict_proba(file)\n",
    "# print(f\"{predict_proba=}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# data=rec.load_data()\n",
    "\n",
    "# data = load_data(\n",
    "#     train_meta_files=train_meta_files,\n",
    "#     test_meta_files=test_meta_files,\n",
    "#     balance=False,\n",
    "# )\n",
    "\n",
    "# X_train = data[\"X_train\"]\n",
    "# y_train = data[\"y_train\"]\n",
    "# rec.train(X_train=X_train, y_train=y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# from typing_extensions import deprecated\n",
    "import warnings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import config.MetaPath as mp\n",
    "from config.MetaPath import test_emodb_csv\n",
    "from config.algoparams import ava_ML_algorithms\n",
    "import random\n",
    "from time import time\n",
    "from config.algoparams import ava_cv_modes\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from deprecated import deprecated\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    fbeta_score,\n",
    "    make_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ShuffleSplit,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "from audio.extractor import AudioExtractor, load_data_from_meta\n",
    "import config.EF as ef\n",
    "from config.EF import AHNPS, e_config_def, f_config_def, validate_emotions\n",
    "from config.MetaPath import (\n",
    "    emodb,\n",
    "    meta_paths_of_db,\n",
    "    ravdess,\n",
    "    savee,\n",
    "    validate_partition,\n",
    "    project_dir,\n",
    ")\n",
    "import config.MetaPath as meta\n",
    "from audio.core import best_estimators, extract_feature_of_audio\n",
    "from config.algoparams import random_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "class EmotionRecognizer:\n",
    "    \"\"\"A class for training, testing ,predicting,anaylzing emotions based on\n",
    "    speech's features that are extracted and fed into `sklearn` model\n",
    "\n",
    "    examples\n",
    "    -\n",
    "    rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=[ravdess],train_dbs=[ravdess], verbose=1)\n",
    "\n",
    "    rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=emodb,train_dbs=emodb, verbose=1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        classification_task=True,\n",
    "        dbs=None,\n",
    "        e_config=None,\n",
    "        f_config=None,\n",
    "        train_dbs=None,\n",
    "        test_dbs=None,\n",
    "        balance=False,\n",
    "        shuffle=True,\n",
    "        override_csv=True,\n",
    "        cross=False,  # è¡¨ç¤ºè·¨åº“(è¿™å›è®©å…¶è¯»å–train_db_econfig.csvä½œä¸ºæµ‹è¯•é›†,æ ·ä¾‹æ›´ä¸°å¯Œ,æ›´åˆç†,å½“ç„¶è¿˜å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›,è¯»å–all_db_econfg.csv,ä½†æ˜¯è¿™é‡Œæš‚ä¸æ‰§è¡Œ)\n",
    "        verbose=0,\n",
    "        **feature_transforms,\n",
    "        # **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            model (sklearn model): the model used to detect emotions. If `model` is None, then self.determine_best_model()\n",
    "                will be automatically called\n",
    "                è¿™ä¸ªå‚æ•°å…¶å®å°±æ˜¯sklearnä¸­çš„Estimatorå¯¹è±¡,ä¾‹å¦‚SVC()ç¤ºä¾‹åŒ–å‡ºæ¥çš„å¯¹è±¡\n",
    "            emotions (list): list of emotions to be used. Note that these emotions must be available in\n",
    "                RAVDESS & EMODB Datasets, available nine emotions are the following:\n",
    "                    'neutral', 'calm', 'happy', 'sad', 'angry', 'fear', 'disgust', 'ps' ( pleasant surprised ), 'boredom'.\n",
    "                é»˜è®¤è¯†åˆ«ä¸‰ç§æƒ…æ„Ÿ\n",
    "                Default is [\"sad\", \"neutral\", \"happy\"].\n",
    "            æƒ…æ„Ÿæ•°æ®åº“çš„ä½¿ç”¨å¼€å…³:\n",
    "            ravdess (bool): whether to use RAVDESS Speech datasets, default is True\n",
    "            emodb (bool): whether to use EMO-DB Speech dataset, default is True,\n",
    "            custom_db (bool): whether to use custom Speech dataset that is located in `data/train-custom`\n",
    "                and `data/test-custom`, default is True\n",
    "            è¾“å‡ºæ–‡ä»¶åçš„æŒ‡å®š(åº”è¯¥æŒ‡å®šä¸ºcsvæ–‡ä»¶,å³å‚æ•°å¸¦æœ‰æ‰©å±•åç¼€csv)\n",
    "            ravdess_name (str): the name of the output CSV file for RAVDESS dataset, default is \"ravdess.csv\"\n",
    "            emodb_name (str): the name of the output CSV file for EMO-DB dataset, default is \"emodb.csv\"\n",
    "            custom_db_name (str): the name of the output CSV file for the custom dataset, default is \"custom.csv\"\n",
    "            æŒ‡å®šéœ€è¦æå–çš„æƒ…æ„Ÿç‰¹å¾,é»˜è®¤ä¸‰ç§:mfcc,chroma,mel\n",
    "            features (list): list of speech features to use, default is [\"mfcc\", \"chroma\", \"mel\"]\n",
    "                (i.e MFCC, Chroma and MEL spectrogram )\n",
    "            æŒ‡å®šè¦ä½¿ç”¨çš„åˆ†ç±»æ¨¡å‹è¿˜æ˜¯å›å½’æ¨¡å‹,é»˜è®¤ä½¿ç”¨åˆ†ç±»æ¨¡å‹\n",
    "            classification (bool): whether to use classification or regression, default is True\n",
    "            balance (bool): whether to balance the dataset ( both training and testing ), default is True\n",
    "            verbose (bool/int): whether to print messages on certain tasks, default is 1\n",
    "        Note that when `ravdess`, `emodb` and `custom_db` are set to `False`, `ravdess` will be set to True\n",
    "        automatically.\n",
    "        \"\"\"\n",
    "        # emotions\n",
    "        self.e_config = e_config if e_config else e_config_def\n",
    "        # make sure that there are only available emotions\n",
    "        validate_emotions(self.e_config)\n",
    "        self.f_config = f_config if f_config else f_config_def\n",
    "        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼(å¾…ä¼˜åŒ–)\n",
    "        # @deprecated(version='1.0', reason='è¯·ä½¿ç”¨ new_function() ä»£æ›¿')\n",
    "        # self._f_config_dict: dict[str, bool] = get_f_config_dict(self.f_config)\n",
    "        self.train_dbs = train_dbs\n",
    "        self.test_dbs = test_dbs\n",
    "\n",
    "        # print(self.train_meta_files, self.test_meta_files)\n",
    "        self.feature_transforms = feature_transforms\n",
    "\n",
    "        # å¯ä»¥ä½¿ç”¨python é»˜è®¤å‚æ•°æ¥æ”¹é€ å†™æ³•\n",
    "        # é»˜è®¤æ‰§è¡Œåˆ†ç±»ä»»åŠ¡\n",
    "        self.classification_task = classification_task\n",
    "        self.verbose = verbose\n",
    "        # boolean attributes\n",
    "        self.override_csv = override_csv\n",
    "        self.shuffle = shuffle\n",
    "        self.balance = balance\n",
    "        self.cross = cross\n",
    "        # éæ„é€ å™¨åˆå§‹åŒ–å˜é‡\n",
    "        self.data_loaded = False\n",
    "        self.model_trained = False\n",
    "        self.ae = None\n",
    "        self.dbs = dbs if dbs else [ravdess]\n",
    "        # é‰´äºæ•°æ®é›†(ç‰¹å¾å’Œæ ‡ç­¾)åœ¨è¯„ä¼°æ–¹æ³•æ—¶å°†åå¤ç”¨åˆ°,å› æ­¤è¿™é‡Œå°†è®¾ç½®ç›¸åº”çš„å±æ€§æ¥ä¿å­˜å®ƒä»¬\n",
    "        # å¦ä¸€æ–¹é¢,å¦‚æœæ¨¡ä»¿sklearnä¸­çš„ç¼–å†™é£æ ¼,å…¶å®æ˜¯å°†æ•°æ®å’Œæ¨¡å‹è®¡ç®—åˆ†å¸ƒåœ¨ä¸åŒçš„æ¨¡å—(ç±»)ä¸­,æ¯”å¦‚\n",
    "        # sklearn.datasetsè´Ÿè´£æ•°æ®é›†ç”Ÿæˆ\n",
    "        # sklearn.model_selectionè´Ÿè´£åˆ’åˆ†æ•°æ®é›†å’Œè®­ç»ƒé›†\n",
    "        # sklearn.algorithms* è´Ÿè´£åˆ›å»ºæ¨¡å‹\n",
    "        # sklearn.metrics è´Ÿè´£è¯„ä¼°æ¨¡å‹\n",
    "        # è®¾ç½®ç›¸åº”çš„å±æ€§çš„æ–¹ä¾¿ä¹‹å¤„åœ¨äºæ–¹æ³•çš„è°ƒç”¨å¯ä»¥å°‘ä¼ å‚\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "        self.y_pred = []\n",
    "        self.train_audio_paths = []\n",
    "        self.test_audio_paths = []\n",
    "        # å¼€å§‹å¡«å……æ•°æ®(æœ€å…ˆå¼€å§‹çš„æ­¥éª¤,æ”¾åœ¨initä¸­éšç€åˆå§‹åŒ–å®ä¾‹çš„æ—¶å€™æ‰§è¡Œ)\n",
    "        # self.load_data()\n",
    "        # å±æ€§çš„å…ˆåä½ç½®ä¼šå½±å“ç¨‹åºçš„è¿è¡Œ\n",
    "        # print(\"@{model}\")\n",
    "        # print(model,\"\\ncomparing and choosing the best model...\")\n",
    "        # !RandomForestClassifierå®ä¾‹ä¸èƒ½ç›´æ¥ç”¨bool()æ¥åˆ¤æ–­,ä¼šæç¤ºestimators_ä¸å­˜åœ¨\n",
    "        self.model = model\n",
    "        # if self.model is None:\n",
    "        # ä¾èµ–äºboolean attributes\n",
    "\n",
    "        self.train_meta_files = meta_paths_of_db(\n",
    "            db=self.train_dbs,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=\"train\",\n",
    "        )\n",
    "        # åˆ¤æ–­è·¨åº“ä»»åŠ¡\n",
    "        test_meta_partition = \"test\"\n",
    "        if self.cross:\n",
    "            test_meta_partition = \"train\"\n",
    "        self.test_meta_files = meta_paths_of_db(\n",
    "            db=self.test_dbs,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=test_meta_partition,\n",
    "        )\n",
    "\n",
    "    # def prepare():\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        å¯¼å…¥æŒ‡å®šçš„è¯­æ–™åº“æ•°æ®,å¹¶æå–ç‰¹å¾\n",
    "        Loads and extracts features from the audio files for the db's specified\n",
    "        - æ³¨æ„,ç”±äºbalanceæ“ä½œå¯¹äºæ•°æ®é›†åˆ’åˆ†æœ‰ä¸€å®šè¦æ±‚,ä¸æ˜¯ä»»ä½•æ•°æ®é›†éƒ½å¯ä»¥æ‰§è¡Œbalanceæ“ä½œ(ä¾‹å¦‚ä¸‰åˆ†ç±»ä¸­,test setä¸­åªç¼ºå¤±äº†æŸä¸€ä¸ªç±»åˆ«çš„æ ·æœ¬,è¿™ä¸­æƒ…å†µä¸‹æ‰§è¡Œbalance,å°†å¯¼è‡´æµ‹è¯•é›†æ ·æœ¬æ•°é‡ä¸ºç©º)\n",
    "        \"\"\"\n",
    "        # åˆ¤æ–­æ˜¯å¦å·²ç»å¯¼å…¥è¿‡æ•°æ®.å¦‚æœå·²ç»å¯¼å…¥,åˆ™è·³è¿‡,å¦åˆ™æ‰§è¡Œå¯¼å…¥\n",
    "        if not self.data_loaded:\n",
    "            # è°ƒç”¨extractorä¸­çš„æ•°æ®å¯¼å…¥å‡½æ•°\n",
    "            data = load_data_from_meta(\n",
    "                train_meta_files=self.train_meta_files,\n",
    "                test_meta_files=self.test_meta_files,\n",
    "                f_config=self.f_config,\n",
    "                e_config=self.e_config,\n",
    "                classification_task=self.classification_task,\n",
    "                balance=self.balance,\n",
    "                shuffle=self.shuffle,\n",
    "                feature_transforms=self.feature_transforms,\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            # è®¾ç½®å®ä¾‹çš„å„ä¸ªå±æ€§\n",
    "            # äº‹å®ä¸Š,ä¹Ÿå¯ä»¥ç›´æ¥ç”¨load_data_from_metaè¿”å›çš„ç»“æœä¸­çš„aeå¯¹è±¡,èµ‹å€¼ERå¯¹è±¡(self.ae=data[\"ae\"])\n",
    "            self.ae = data[\"ae\"]\n",
    "            self.X_train = data[\"X_train\"]\n",
    "            self.X_test = data[\"X_test\"]\n",
    "            self.y_train = data[\"y_train\"]\n",
    "            self.y_test = data[\"y_test\"]\n",
    "            self.train_audio_paths = data[\"train_audio_paths\"]\n",
    "            self.test_audio_paths = data[\"test_audio_paths\"]\n",
    "\n",
    "            self.balanced_success(data)\n",
    "            if self.verbose:\n",
    "                print(\"[I] Data loaded\\n\")\n",
    "                print(f\"{self.ae=}\")\n",
    "                print(f\"{self.ae.pca=}ğŸˆ\")\n",
    "            self.data_loaded = True\n",
    "            # print(id(self))\n",
    "            if self.verbose > 1:\n",
    "                print(vars(self))\n",
    "\n",
    "    def balanced_success(self, res):\n",
    "        self.balance = res[\"balance\"]\n",
    "\n",
    "    def train(self, choosing=False, verbose=1):\n",
    "        \"\"\"\n",
    "        è½½å…¥æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹(sklearn.estimator.fit)\n",
    "        Train the model, if data isn't loaded, it will be loaded automatically\n",
    "\n",
    "        X_train=None, y_train=None\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            # if data isn't loaded yet, load it then\n",
    "            self.load_data()\n",
    "        if self.verbose > 1:\n",
    "            print(\"@{self.model}:\")\n",
    "            print(self.model)\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        if not self.model_trained or choosing:\n",
    "            X_train = self.X_train\n",
    "            y_train = self.y_train\n",
    "            # estimatorè®­ç»ƒ(fit)æ¨¡å‹\n",
    "            model.fit(X=X_train, y=y_train)\n",
    "            self.model_trained = True\n",
    "        if self.verbose > 1:\n",
    "            if choosing == True:\n",
    "                print(\n",
    "                    f\"[I] Model trained with{choosing=},choosing the best model,override the trained model..\"\n",
    "                )\n",
    "\n",
    "    def predict(self, audio_path):\n",
    "        \"\"\"\n",
    "        é¢„æµ‹å•ä¸ªéŸ³é¢‘çš„æƒ…æ„Ÿ\n",
    "        ç”±äºæ˜¯å•ä¸ªéŸ³é¢‘çš„æƒ…æ„Ÿé¢„æµ‹,å› æ­¤ä¸éœ€è¦è€ƒè™‘shuffleå’Œbalanceè¿™äº›æ“ä½œ,åªéœ€è¦æå–è¯­éŸ³ç‰¹å¾,ç„¶åè¿›è¡Œè°ƒç”¨æ¨¡å‹é¢„æµ‹å³å¯\n",
    "        given an `audio_path`, this method extracts the features\n",
    "        and predicts the emotion\n",
    "\n",
    "        ä»¥ä¸‹è¯­å¥ä¸å†é€‚åˆå…·æœ‰pcaé™ç»´æ“ä½œä¸‹çš„æƒ…å½¢\n",
    "        feature_audio = extract_feature_of_audio(audio_path, self.f_config)\n",
    "        print(feature1.shape)\n",
    "        print(feature1,\"@{feature1}\",feature1.shape)\n",
    "        feature2=feature1.T\n",
    "        print(feature2,\"@{feature2}\",feature2.shape)\n",
    "        print(feature3,\"@{feature3}\",feature3.shape)\n",
    "        \"\"\"\n",
    "        feature_audio = self.extract_feature_single_audio(audio_path)\n",
    "\n",
    "        feature = feature_audio.reshape(1, -1)\n",
    "        model = self.model if self.model else self.best_model()\n",
    "        res = model.predict(feature)\n",
    "        # reså¯èƒ½æ˜¯ä¸ªåˆ—è¡¨\n",
    "        # print(res, \"@{res}\")\n",
    "        return res[0]\n",
    "\n",
    "    def extract_feature_single_audio(self, audio_path):\n",
    "        \"\"\"extract a single audio file feature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        audio_path : path\n",
    "            audio path\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            audio feature\n",
    "        \"\"\"\n",
    "        ae: AudioExtractor = self.ae\n",
    "        pca = ae.pca\n",
    "        print(pca, \"@{pca} in 'predict' method\")\n",
    "        # if pca:\n",
    "        #     feature_audio=pca.transform(feature_audio)\n",
    "        #     print(feature_audio.shape, \"@{feature_audio.shape}\")\n",
    "        feature_audio = ae.extract_features(partition=\"test\", audio_paths=[audio_path])\n",
    "        return feature_audio\n",
    "        # return self.model.predict(feature2)[0]\n",
    "\n",
    "    def peek_test_set(self, n=5):\n",
    "        res = [\n",
    "            self.test_audio_paths[:n],\n",
    "            self.X_test[:n],\n",
    "            self.y_test[:n],\n",
    "            self.y_pred[:n],\n",
    "        ]\n",
    "        return res\n",
    "\n",
    "    def predict_proba(self, audio_path):\n",
    "        \"\"\"\n",
    "        Predicts the probability of each emotion.\n",
    "        \"\"\"\n",
    "        if self.classification_task:\n",
    "            # feature = extract_feature_of_audio(audio_path, self.f_config).reshape(1, -1)\n",
    "            feature = self.extract_feature_single_audio(audio_path)\n",
    "            proba = self.model.predict_proba(feature)[0]\n",
    "            result = {}\n",
    "            for emotion, prob in zip(self.model.classes_, proba):\n",
    "                result[emotion] = prob\n",
    "            return result\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Probability prediction doesn't make sense for regression\"\n",
    "            )\n",
    "\n",
    "    def show_second(self):\n",
    "        peeker = er.peek_test_set(2)\n",
    "        feature = peeker[1][1]\n",
    "        audio_path = peeker[0][1]\n",
    "        feature_pred = self.predict(audio_path)\n",
    "        print(feature[:5], feature_pred[:5])\n",
    "\n",
    "    def grid_search(self, params, n_jobs=2, verbose=3):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨ç½‘æ ¼åŒ–æœç´¢çš„æ–¹å¼æœç´¢æœ€ä¼˜è¶…å‚æ•°\n",
    "        Performs GridSearchCV on `params` passed on the `self.model`\n",
    "        And returns the tuple: (best_estimator, best_params, best_score).\n",
    "        \"\"\"\n",
    "        score = accuracy_score if self.classification_task else mean_absolute_error\n",
    "        grid = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=params,\n",
    "            scoring=make_scorer(score),\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "            cv=3,\n",
    "        )\n",
    "        # è°ƒç”¨fitå¼€å§‹ä¼ å…¥æ•°æ®é›†å¹¶æœç´¢\n",
    "        X_train, y_train = self.X_train, self.y_train\n",
    "        if X_train is None or y_train is None:\n",
    "            raise ValueError(\"X_train and y_train are None\")\n",
    "        # fitè¿‡ç¨‹æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹\n",
    "        grid_result = grid.fit(X_train, y_train)\n",
    "        return (\n",
    "            grid_result.best_estimator_,\n",
    "            grid_result.best_params_,\n",
    "            grid_result.best_score_,\n",
    "        )\n",
    "\n",
    "    def best_model(self):\n",
    "        \"\"\"\n",
    "        ä»å¸¸è§çš„æ¨¡å‹ä¸­è®¡ç®—å‡ºæœ€å¥½çš„Estimator(model)\n",
    "        è®¡ç®—æœ€ä¼˜modelæ—¶,ä¹Ÿå¯ä»¥è€ƒè™‘åˆ›å»ºæ–°çš„ERå®ä¾‹æ¥åšè®¡ç®—æœ€ä¼˜modelçš„ç”¨é€”,ä½†ä¼šå¢åŠ å¼€é”€\n",
    "\n",
    "        Loads best estimators and determine which is best for test data,\n",
    "        and then set it to `self.model`.\n",
    "        # ä½¿ç”¨MSEæ¥è¯„ä»·å›å½’æ¨¡å‹,ä½¿ç”¨accuracyæ¥è¯„ä»·åˆ†ç±»æ¨¡å‹\n",
    "        In case of regression, the metric used is MSE(å‡æ–¹è¯¯å·®) and accuracy for classification.\n",
    "\n",
    "        Note that the execution of this method may take several minutes due\n",
    "        to training all estimators (stored in `grid` folder) for determining the best possible one.\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            self.load_data()\n",
    "\n",
    "        # loads estimators\n",
    "        estimators = best_estimators()\n",
    "\n",
    "        result = []\n",
    "\n",
    "        if self.verbose:\n",
    "            # æ§åˆ¶æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "            # é€šè¿‡tqdmå°è£…estimatorè¿™ä¸ªå¯è¿­ä»£å¯¹è±¡,å°±å¯ä»¥åœ¨éå†estimatoræ—¶,æ§åˆ¶è¿›åº¦æ¡çš„æ˜¾ç¤º\n",
    "            estimators = tqdm(estimators)\n",
    "\n",
    "        for epc in estimators:\n",
    "            estimator, params_, cv_score_ = epc\n",
    "            ecn = estimator.__class__.__name__\n",
    "\n",
    "            if self.verbose:\n",
    "                # å¦‚æœå¯ç”¨verboseé€‰é¡¹,é‚£ä¹ˆestimatorsä¼šè¢«tqdmåŒ…è£…\n",
    "                # æ­¤æ—¶å¯ä»¥é€šè¿‡set_descriptionæ–¹æ³•æ¥ä¿®æ”¹è¿›åº¦æ¡çš„æè¿°ä¿¡æ¯\n",
    "                # æ¯”å¦‚,estimators.set_description(f\"Evaluating {estimator.__class__.__name__}\")\n",
    "                estimators.set_description(f\"Evaluating <{ecn}>\")\n",
    "            # ä¸ºä¾‹é¿å…ç›¸äº’å¹²æ‰°,æ¯æµ‹è¯•æ¨¡å‹å°±åˆ›å»ºä¸€ä¸ªERå¯¹è±¡(er)\n",
    "            er = EmotionRecognizer(\n",
    "                model=estimator,\n",
    "                emotions=self.e_config,\n",
    "                classification_task=self.classification_task,\n",
    "                f_config=self.f_config,\n",
    "                balance=self.balance,\n",
    "                override_csv=False,\n",
    "                verbose=0,\n",
    "            )\n",
    "            # data already loaded\n",
    "            er.X_train = self.X_train\n",
    "            er.X_test = self.X_test\n",
    "            er.y_train = self.y_train\n",
    "            er.y_test = self.y_test\n",
    "            er.data_loaded = True\n",
    "            # train the model\n",
    "            er.train(verbose=0)\n",
    "            # get test accuracy\n",
    "            accuracy = er.test_score()\n",
    "            # append to result\n",
    "            result.append((er.model, accuracy))\n",
    "\n",
    "            # æ–¹æ³•2:(å°å¿ƒä½¿ç”¨)\n",
    "            # ä½¿ç”¨æœ¬å¯¹è±¡selfè€Œä¸æ˜¯åœ¨åˆ›å»ºä¸€ä¸ªERå¯¹è±¡\n",
    "            # self.model = estimator\n",
    "            # er = self\n",
    "            # ä»¥ä¸‹çš„è®¡ç®—æ˜¯ç”¨æ¥é€‰å‡ºmodelçš„,è€Œä¸æ˜¯ç›´æ¥ä½œä¸ºselfå¯¹è±¡çš„å±æ€§,è¿™é‡Œå°†selfèµ‹å€¼ç»™er,ä»¥ç¤ºåŒºåˆ«\n",
    "            # train(fit) the model\n",
    "            # å¦‚æœè®¾ç½®verbose=1,åˆ™ä¼šé€ä¸ªæ‰“å°å½“å‰è®¡ç®—çš„æ¨¡å‹(è¿›åº¦ä¸æ˜¯åŒä¸€æ¡)\n",
    "            # er.train(choosing=True, verbose=0)\n",
    "            # train(fit) the model\n",
    "            # self.train(verbose=1)\n",
    "            # accuracy = er.test_score(choosing=True)\n",
    "            # append to result\n",
    "            # result.append((estimator, accuracy))\n",
    "\n",
    "            print(f\"\\n[I] {ecn} with {accuracy} test accuracy\")\n",
    "\n",
    "        # sort the result\n",
    "        # regression: best is the lower, not the higher\n",
    "        # classification: best is higher, not the lower\n",
    "        result = sorted(\n",
    "            result, key=lambda item: item[1], reverse=self.classification_task\n",
    "        )\n",
    "        best_estimator = result[0][0]\n",
    "        accuracy = result[0][1]\n",
    "\n",
    "        self.model = best_estimator\n",
    "        self.model_trained = True\n",
    "        if self.verbose:\n",
    "            if self.classification_task:\n",
    "                print(\n",
    "                    f\"[ğŸˆ] Best model : {self.model.__class__.__name__} with {accuracy * 100:.3f}% test accuracy\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[I] Best model : {self.model.__class__.__name__} with {accuracy:.5f} mean absolute error\"\n",
    "                )\n",
    "        return best_estimator\n",
    "\n",
    "    def test_score(self, choosing=False, verbose=0, report=False):\n",
    "        \"\"\"\n",
    "        Calculates score on testing data\n",
    "        Please call the `train` method before call this method.\n",
    "\n",
    "        just like sklearn convention:call estimator.call `fit` at first,then call `predict` or `score` method\n",
    "\n",
    "        if `self.classification` is True, the metric used is accuracy,\n",
    "        Mean-Squared-Error is used otherwise (regression)\n",
    "\n",
    "        1.è°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
    "\n",
    "        2.å¦‚æœmodelæ˜¯None,é‚£ä¹ˆè°ƒç”¨best_modelè·å–æœ€ä¼˜æ¨¡å‹(è¿™ä¸ªè¿‡ç¨‹ä¼šéå†ä¸€ä¸ªå¯ç”¨æ¨¡å‹åˆ—è¡¨,æ˜¯é€šè¿‡è°ƒç”¨ERå®ä¾‹çš„test_score()æ–¹æ³•æ¥è®¡ç®—,ä¸è¿‡è¿™é‡Œä¸ä¼šé‡åˆ°Noneçš„æƒ…å†µ,å› æ­¤é—´æ¥é€’å½’è°ƒç”¨ä¸è¶…è¿‡2å±‚)\n",
    "        \"\"\"\n",
    "        X_test = self.X_test\n",
    "        y_test = self.y_test\n",
    "\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        self.validate_empty_array(X_test=X_test, y_test=y_test)\n",
    "\n",
    "        # é¢„æµ‹è®¡ç®—\n",
    "        if verbose > 1:\n",
    "            print(X_test.shape, y_test.shape, \"ğŸˆ\")\n",
    "        # æ ¹æ®å½“å‰æ¨¡å‹è¿›è¡Œé¢„æµ‹(ç›´æ¥è°ƒç”¨estimatorçš„predictæ–¹æ³•)\n",
    "        y_pred = model.predict(X_test)  # type: ignore\n",
    "        # å¦‚æœå¤„äºbest_modelçš„è¿‡ç¨‹ä¸­è°ƒç”¨æœ¬æ–¹æ³•(choosing=True),åˆ™ä¸Šè¿°é¢„æµ‹å†…å®¹ä½œä¸ºä¸´æ—¶ç»“æœä¸å†™å…¥å¯¹è±¡å±æ€§ä¿å­˜;å¦åˆ™ä½œä¸ºæœ€ç»ˆç»“æœå†™å…¥å±æ€§ä¿å­˜\n",
    "        # é»˜è®¤choosing=False,ä¹Ÿå°±æ˜¯å°†ç»“æœä¿å­˜åˆ°å¯¹è±¡å±æ€§ä¸­\n",
    "        if choosing == False:\n",
    "            self.y_pred = np.array(y_pred)\n",
    "\n",
    "        if self.classification_task:\n",
    "            res = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "            # ç»“æœå’Œ:er.model.score(er.X_test,er.y_test)ä¸€æ ·,ä½†æ˜¯è¿™ç§åšæ³•å›ç‹¬ç«‹å°†X_testé¢„æµ‹ä¸€é,è€Œä¸ä¿å­˜é¢„æµ‹ç»“æœ,åªç»™å‡ºå¾—åˆ†\n",
    "        else:\n",
    "            res = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "        if report:\n",
    "            self.check_report(y_test, y_pred)\n",
    "        return res\n",
    "\n",
    "    def check_report(self):\n",
    "        \"\"\"è¾“å‡ºæ¨¡å‹å½“å‰ç»“æœçš„å¤šä¸ªæŒ‡æ ‡æŠ¥å‘Š\n",
    "        å¯¹äºåˆ†ç±»ä»»åŠ¡,åŒ…æ‹¬precision(æŸ¥å‡†ç‡),recall(å›å¬æˆ–æŸ¥å…¨ç‡),f1-scoreä»¥åŠå„ç±»åˆ«çš„æ ·æœ¬æ•°é‡\n",
    "        å¯¹äºå¤šåˆ†ç±»,è¿˜æœ‰ä¸€äº›ç»¼åˆçš„æŒ‡æ ‡(macro,weighted),æ¯ä¸ªå•å…ƒæ ¼ç»“åˆä¸¤ä¸ªç»´åº¦çš„è¡¨å¤´è¿›è¡Œç†è§£å’Œé˜…è¯»\n",
    "\n",
    "        ç”±äºè·¨åº“å®éªŒæ¯”è¾ƒå›°éš¾,æœ‰çš„æ ·æœ¬ç±»åˆ«æ— æ³•è¢«æ­£ç¡®åˆ†ç±»(æ‰€æœ‰è¯¥ç±»åˆ«éƒ½è¢«é”™è¯¯åˆ†ç±»),æ­¤æ—¶classification_reportæ–¹æ³•ä¼šæå‡ºè­¦å‘Š,é™¤éä½¿ç”¨zero_divisionå‚æ•°æ›¿æ¢æ‰é»˜è®¤çš„warn.\n",
    "        \"\"\"\n",
    "        y_test = self.y_test\n",
    "        y_pred = self.y_pred\n",
    "\n",
    "        report = classification_report(y_true=y_test, y_pred=y_pred, zero_division=0)\n",
    "        # print(report, self.model.__class__.__name__)\n",
    "        return report\n",
    "\n",
    "    def model_cv_score(\n",
    "        self,\n",
    "        choosing=False,\n",
    "        verbose=1,\n",
    "        mean_only=True,\n",
    "        n_splits=5,\n",
    "        test_size=0.2,\n",
    "        cv_mode=\"sss\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨äº¤å‰éªŒè¯çš„æ–¹å¼æ¥è¯„ä¼°æ¨¡å‹\n",
    "        Calculates score on testing data\n",
    "        \"\"\"\n",
    "        X_train = self.X_train\n",
    "        y_train = self.y_train\n",
    "        # è°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        self.validate_empty_array(X_train, y_train)\n",
    "\n",
    "        # é¢„æµ‹è®¡ç®—\n",
    "        if verbose > 1:\n",
    "            print(X_train.shape, y_train.shape, \"ğŸˆ\")\n",
    "            print(f\"{n_splits=}\")\n",
    "        n_splits = int(n_splits)\n",
    "\n",
    "        y_pred = model.predict(X_train)  # type: ignore\n",
    "        if choosing == False:\n",
    "            self.y_pred = np.array(y_pred)\n",
    "        # äº¤å‰éªŒè¯çš„æ–¹å¼è¯„ä¼°æ¨¡å‹çš„å¾—åˆ†\n",
    "        cv_mode_dict = dict(\n",
    "            sss=StratifiedShuffleSplit(\n",
    "                n_splits=n_splits, test_size=test_size, random_state=0\n",
    "            ),\n",
    "            ss=ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=0),\n",
    "            kfold=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "            skfold=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "        )\n",
    "        cv_mode_selected = cv_mode_dict[cv_mode]\n",
    "        if verbose > 1:\n",
    "            print(f\"{cv_mode=}ğŸˆ\")\n",
    "        res = [0]\n",
    "        if self.classification_task:\n",
    "            # res = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "            # å°†äº¤å‰éªŒè¯å™¨cvä¼ é€’ç»™cross_val_scoreå‡½æ•°æ‰§è¡Œè¯„ä¼°æ“ä½œ\n",
    "            # è€Œéè‡ªå·±ä½¿ç”¨æ¥å®ŒæˆkæŠ˜äº¤å‰éªŒè¯\n",
    "            # æ‰€æœ‰å¯¹è±¡éƒ½æœ‰__class__å±æ€§ä»¥åŠ__name__äºŒçº§å±æ€§\n",
    "            if model.__class__.__name__ in ava_ML_algorithms:\n",
    "                res = cross_val_score(model, X_train, y_train, cv=cv_mode_selected)\n",
    "                if mean_only:\n",
    "                    res = res.mean()\n",
    "\n",
    "        else:\n",
    "            # ä½¿ç”¨å›å½’å™¨çš„æƒ…å†µ\n",
    "            res = mean_squared_error(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "        if self.verbose > 2:\n",
    "            report = classification_report(\n",
    "                y_true=y_train, y_pred=y_pred\n",
    "            )  # è®­ç»ƒé›†ä¸Š,å‡ ä¹æ€»æ˜¯æ˜¯æ»¡åˆ†\n",
    "            print(report, self.model.__class__.__name__)\n",
    "        return res\n",
    "\n",
    "    def validate_empty_array(self, X_test=[], y_test=[]):\n",
    "        if len(X_test) == 0:\n",
    "            raise ValueError(\"X is empty\")\n",
    "        if len(y_test) == 0:\n",
    "            raise ValueError(\"y is empty\")\n",
    "\n",
    "    def meta_paths_of_db(self, db, partition=\"test\"):\n",
    "        res = meta_paths_of_db(\n",
    "            db=db,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=partition,\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    def update_test_set(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def update_test_set_by_meta(self, test_meta):\n",
    "        \"\"\"\n",
    "        è¿™ä¸ªå‡½æ•°è®¾è®¡ç”¨æ¥åšè·¨åº“è¯†åˆ«è¯•éªŒ\n",
    "        ä»…ä»…æ›¿æ¢æµ‹è¯•é›†ä¸ºä¸åŒåº“,æœ¬èº«æ²¡æœ‰é’ˆå¯¹è·¨åº“è¿›è¡Œä¼˜åŒ–\n",
    "\n",
    "\n",
    "        Load test data from given test metadata file paths and update instance's test set attributes.\n",
    "\n",
    "        Args:\n",
    "            test_meta (list of str): List of file paths of test metadata files.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        examples:\n",
    "        >>> rec = EmotionRecognizer(model=my_model,**meta_dict, verbose=1)\n",
    "        >>> rec.train()\n",
    "        >>> rec.update_test_set_by_meta(test_emodb_csv)\n",
    "        >>> rec.test_meta_files\n",
    "        >>> 'meta_files\\\\test_emodb_HNS.csv'\n",
    "        >>> rec.X_test.shape\n",
    "        >>> (43,180)\n",
    "\n",
    "        >>> rec.test_score()\n",
    "        >>> 0.4651\n",
    "\n",
    "        \"\"\"\n",
    "        # rec.update_test_set_by_meta(test_emodb_csv)\n",
    "        self.test_meta_files = test_meta\n",
    "        print(test_meta, \"@{test_meta}\")\n",
    "        test_data = load_data_from_meta(\n",
    "            test_meta_files=test_meta, e_config=self.e_config, f_config=self.f_config\n",
    "        )\n",
    "\n",
    "        X_test = test_data[\"X_test\"]\n",
    "        y_test = test_data[\"y_test\"]\n",
    "        # è®¾ç½®å®ä¾‹çš„å„ä¸ªå±æ€§\n",
    "        self.test_audio_paths = test_data[\"test_audio_paths\"]\n",
    "        self.update_test_set(X_test, y_test)\n",
    "\n",
    "    def train_score(self, X_train=None, y_train=None):\n",
    "        \"\"\"\n",
    "        Calculates accuracy score on training data\n",
    "        if `self.classification` is True, the metric used is accuracy,\n",
    "        Mean-Squared-Error is used otherwise (regression)\n",
    "        \"\"\"\n",
    "        if X_train is None or y_train is None:\n",
    "            X_train = self.X_train\n",
    "            y_train = self.y_train\n",
    "        y_pred = self.model.predict(X_train)\n",
    "        if self.classification_task:\n",
    "            return accuracy_score(y_true=y_train, y_pred=y_pred)\n",
    "        else:\n",
    "            return mean_squared_error(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    def train_fbeta_score(self, beta):\n",
    "        y_pred = self.model.predict(self.X_train)\n",
    "        y_train = self.y_train\n",
    "        if y_train is None:\n",
    "            raise ValueError(\"y_train is None\")\n",
    "\n",
    "        return fbeta_score(y_true=y_train, y_pred=y_pred, beta=beta, average=\"micro\")\n",
    "\n",
    "    def test_fbeta_score(self, beta):\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        if y_test is None:\n",
    "            raise ValueError(\"y_test is None\")\n",
    "        return fbeta_score(y_true=y_test, y_pred=y_pred, beta=beta, average=\"micro\")\n",
    "\n",
    "    def confusion_matrix(self, percentage=True, labeled=True):\n",
    "        \"\"\"\n",
    "        Computes confusion matrix to evaluate the test accuracy of the classification\n",
    "        and returns it as numpy matrix or pandas dataframe (depends on params).\n",
    "        params:\n",
    "            percentage (bool): whether to use percentage instead of number of samples, default is True.\n",
    "            labeled (bool): whether to label the columns and indexes in the dataframe.\n",
    "        \"\"\"\n",
    "        if not self.classification_task:\n",
    "            raise NotImplementedError(\n",
    "                \"Confusion matrix works only when it is a classification problem\"\n",
    "            )\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        if y_test is None:\n",
    "            raise ValueError(\"y_test is None\")\n",
    "        matrix = confusion_matrix(\n",
    "            y_true=y_test, y_pred=y_pred, labels=self.e_config\n",
    "        ).astype(np.float32)\n",
    "        if percentage:\n",
    "            for i in range(len(matrix)):\n",
    "                matrix[i] = matrix[i] / np.sum(matrix[i])\n",
    "            # make it percentage\n",
    "            matrix *= 100\n",
    "        if labeled:\n",
    "            matrix_df = pd.DataFrame(\n",
    "                matrix,\n",
    "                index=[f\"true_{e}\" for e in self.e_config],\n",
    "                columns=[f\"predicted_{e}\" for e in self.e_config],\n",
    "            )\n",
    "        return matrix_df\n",
    "\n",
    "    def draw_confusion_matrix(self):\n",
    "        \"\"\"Calculates the confusion matrix and shows it\"\"\"\n",
    "        matrix = self.confusion_matrix(percentage=False, labeled=False)\n",
    "        # TODO: add labels, title, legends, etc.\n",
    "        pl.imshow(matrix, cmap=\"binary\")\n",
    "        pl.show()\n",
    "\n",
    "    def count_samples_in_partition(self, emotion, partition):\n",
    "        \"\"\"\n",
    "        Get the number of data samples of the `emotion` class in a particular `partition` ('test' or 'train').\n",
    "\n",
    "        :param emotion: The emotion class to count.\n",
    "        :param partition: The partition to count samples in ('test' or 'train').\n",
    "        :return: The number of data samples of the `emotion` class in the `partition`.\n",
    "        :raises ValueError: If `y_test` or `y_train` is `None`.\n",
    "        \"\"\"\n",
    "        partition = validate_partition(partition, Noneable=False)\n",
    "        if partition == \"test\":\n",
    "            if self.y_test is None:\n",
    "                raise ValueError(\"y_test is None\")\n",
    "            count = sum(1 for y in self.y_test if y == emotion)\n",
    "        else:\n",
    "            if self.y_train is None:\n",
    "                raise ValueError(\"y_train is None\")\n",
    "            count = sum(1 for y in self.y_train if y == emotion)\n",
    "        return count\n",
    "\n",
    "    def count_samples_by_class(self):\n",
    "        \"\"\"\n",
    "        Returns a dataframe that contains the number of training\n",
    "        and testing samples for all emotions.\n",
    "        Note that if data isn't loaded yet, it'll be loaded\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            self.load_data()\n",
    "        train_samples = []\n",
    "        test_samples = []\n",
    "        total = []\n",
    "        for emotion in self.e_config:\n",
    "            n_train = self.count_samples_in_partition(emotion, \"train\")\n",
    "            n_test = self.count_samples_in_partition(emotion, \"test\")\n",
    "            train_samples.append(n_train)\n",
    "            test_samples.append(n_test)\n",
    "            total.append(n_train + n_test)\n",
    "\n",
    "        # get total\n",
    "        total.append(sum(train_samples) + sum(test_samples))\n",
    "        train_samples.append(sum(train_samples))\n",
    "        test_samples.append(sum(test_samples))\n",
    "        return pd.DataFrame(\n",
    "            data={\"train\": train_samples, \"test\": test_samples, \"total\": total},\n",
    "            index=self.e_config + [\"total\"],\n",
    "        )\n",
    "\n",
    "    def get_random_emotion_index(self, emotion, partition=\"train\"):\n",
    "        \"\"\"\n",
    "        Returns a random index of a `partition` sample with the given `emotion`.\n",
    "\n",
    "        Args:\n",
    "            emotion (str): The name of the emotion to look for.\n",
    "            partition (str): The partition to sample from. Only \"train\" or \"test\" are accepted.\n",
    "\n",
    "        Returns:\n",
    "            int: The index of a random sample with the given `emotion` in the specified `partition`.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If `partition` is not \"train\" or \"test\".\n",
    "        \"\"\"\n",
    "        partition = validate_partition(partition, Noneable=False)\n",
    "        indices = []\n",
    "        if partition == \"train\":\n",
    "            indices = [i for i, y in enumerate(self.y_train) if y == emotion]\n",
    "        elif partition == \"test\":\n",
    "            indices = [i for i, y in enumerate(self.y_test) if y == emotion]\n",
    "\n",
    "        return random.choice(indices)\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# def get_stacking_clf():\n",
    "#     random_state = 42\n",
    "#     # å®šä¹‰åˆçº§å­¦ä¹ å™¨\n",
    "#     estimators_basic_mix = estrs_basic_mix( )\n",
    "#     estimators_dt = estrs_esdt(random_state)\n",
    "#     estimators_basic_dt_mix = estrs_basic_dt_mix( )\n",
    "#     estimators_basic_dt_mpl_mix = estrs_basic_dt_mpl_mix(\n",
    "#         random_state, estimators_basic_dt_mix\n",
    "#     )\n",
    "#     estimators_basic_dmb_mix = estimators_basic_dt_mix + [(\"gnb\", GaussianNB())]\n",
    "#     # ç®€å•å †å \n",
    "#     stack_dt_linear = StackingClassifier(\n",
    "#         estimators=estimators_dt, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     stack_mix_linear = StackingClassifier(\n",
    "#         estimators=estimators_basic_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     # æ”¶æ•›å›°éš¾(æ•ˆæœä¸å¦‚mix_linear)\n",
    "#     stack_mix_svm = StackingClassifier(\n",
    "#         estimators=estimators_basic_mix,\n",
    "#         final_estimator=SVC(),\n",
    "#     )\n",
    "#     stack_basic_dt_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dt_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     # æ•°æ®é‡å¤ªå°\n",
    "#     stack_basic_dt_mlp_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dt_mpl_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     stack_dmb_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dmb_mix,\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#     )\n",
    "#     # stack1 = StackingClassifier(\n",
    "#     #     estimators=[(\"gbc\", GaussianNB())],\n",
    "#     #     final_estimator=LogisticRegression(),\n",
    "#     # )\n",
    "#     # å¤šå±‚å †å \n",
    "#     ##å®šä¹‰æœ€åä¸€å±‚\n",
    "#     stack_final_layer = StackingClassifier(\n",
    "#         estimators=[(\"gbc\", GradientBoostingClassifier()), (\"svc\", SVC())],\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#     )\n",
    "#     ##å †å äºŒå±‚(å®¹æ˜“è¿‡æ‹Ÿåˆ)\n",
    "#     stack_multilayer = StackingClassifier(\n",
    "#         estimators=estimators_dt, final_estimator=stack_final_layer\n",
    "#     )\n",
    "#     stack = StackingClassifier(estimators=estimators_dt, final_estimator=SVC())\n",
    "\n",
    "#     # return stack_dt_linear\n",
    "#     # return stack_mix_linear\n",
    "#     # return stack_mix_svm\n",
    "#     # return stack_basic_dt_mix\n",
    "#     return stack_dmb_mix\n",
    "\n",
    "\n",
    "def estrs_basic_mlp_mix():\n",
    "    estimators_basic_dt_mpl_mix = estrs_basic_mix() + [\n",
    "        (\n",
    "            \"mpl\",\n",
    "            MLPClassifier(\n",
    "                alpha=0.01,\n",
    "                batch_size=512,\n",
    "                hidden_layer_sizes=(300,),\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=400,\n",
    "                random_state=random_state,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return estimators_basic_dt_mpl_mix\n",
    "\n",
    "\n",
    "def estrs_esdt():\n",
    "    estimators_dt = [\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=10, max_depth=3, random_state=random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"adab\",\n",
    "            AdaBoostClassifier(\n",
    "                n_estimators=10, learning_rate=0.1, random_state=random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\"gradb\", GradientBoostingClassifier()),\n",
    "    ]\n",
    "\n",
    "    return estimators_dt\n",
    "\n",
    "\n",
    "def estrs_basic_esdt_mix():\n",
    "    res = estrs_basic_mix() + estrs_esdt()\n",
    "    return res\n",
    "\n",
    "def estrs_simple():\n",
    "    res=[\n",
    "         (\"knc\", KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")),\n",
    "         (\"gnb\",GaussianNB())\n",
    "    ]\n",
    "    return res\n",
    "def estrs_basic_mix():\n",
    "    \"\"\"åŒ…å«å¸¸ç”¨çš„ä¸ªä½“å­¦ä¹ å™¨,å¯ä»¥ä½œä¸ºStackingçš„ç¬¬ä¸€å±‚\n",
    "    è¿˜å¯ä»¥ä½œä¸ºæ›´åŠ å¤æ‚çš„ç¬¬ä¸€å±‚çš„åŸºç¡€éƒ¨åˆ†,é‡‡ç”¨åˆ—è¡¨ç›¸åŠ çš„æ–¹å¼è¿›ä¸€æ­¥æ‰©å……\n",
    "    ä¾‹å¦‚æ·»åŠ é›†æˆå­¦ä¹ (éšæœºæ£®æ—,æ¢¯åº¦æå‡ç­‰)\n",
    "\n",
    "    å…·ä½“åŒ…æ‹¬ä»¥ä¸‹æ¨¡å‹:\n",
    "\n",
    "    - çº¿æ€§æ¨¡å‹(lsvr,rdcv,logistic)\n",
    "    - kè¿‘é‚»(knc)\n",
    "    - è´å¶æ–¯å†³ç­–(gnb)\n",
    "    - å†³ç­–æ ‘(dt)\n",
    "\n",
    "    è¿™äº›ä¸ªä½“å­¦ä¹ å™¨è¾ƒä¸ºå¤šæ ·,ç†è®ºä¸Šæœ‰åˆ©äºæé«˜é›†æˆå­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[estimator]\n",
    "        åŸºç¡€ä¸ªä½“å­¦ä¹ å™¨åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    estimators_basic_mix = [\n",
    "        # (\"svc\",(SVC(C=10, gamma=0.001,random_state=random_state))),\n",
    "        # (\"lsvr\", make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))),\n",
    "        (\"lsvr\", (LinearSVC(max_iter=5000,random_state=random_state))),\n",
    "        (\"rdcv\", RidgeClassifierCV()),\n",
    "        (\"logistic\", LogisticRegression()),\n",
    "        (\"knc\", KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")),\n",
    "        (\"gnb\", GaussianNB()),\n",
    "        (\n",
    "            \"dtc\",\n",
    "            DecisionTreeClassifier(\n",
    "                criterion=\"entropy\", max_depth=7, max_features=\"sqrt\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return estimators_basic_mix\n",
    "\n",
    "\n",
    "def get_clfs():\n",
    "    \"\"\"éœ€è¦è¯„ä¼°å’Œåˆ†æå¯¹æ¯”çš„ä¼°è®¡å™¨\n",
    "    svc = SVC(C=0.001, gamma=0.001, kernel=\"poly\", probability=True)\n",
    "    knn=KNeighborsClassifier(n_neighbors=3, p=1, weights='distance')\n",
    "    Best for DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "    dt = DecisionTreeClassifier(\n",
    "        criterion=\"entropy\",\n",
    "        max_depth=7,\n",
    "        max_features=None,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=2\n",
    "    )\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[estimator]\n",
    "        sklearn.estimatorä¼°è®¡å™¨åˆ—è¡¨\n",
    "    \"\"\"\n",
    "\n",
    "    rfc = RandomForestClassifier()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    lsvc = LinearSVC(max_iter=5000, random_state=random_state)\n",
    "    svc = SVC()\n",
    "    mlp = MLPClassifier(max_iter=3000)\n",
    "    rdcv = RidgeClassifierCV()\n",
    "    gnb = GaussianNB()\n",
    "    plsvc = make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))\n",
    "\n",
    "    # stack = get_stacking_clf()\n",
    "    stack_basic_svc=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=SVC()\n",
    "    )\n",
    "    stack_basic_rf=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=RandomForestClassifier()\n",
    "    )\n",
    "    stack_basic_mlp=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=MLPClassifier()\n",
    "    )\n",
    "    # estrs_basic_esdt_mixä½œä¸ºç¬¬ä¸€å±‚\n",
    "    stack_basic_esdt_gnb=StackingClassifier(\n",
    "        estimators=estrs_basic_esdt_mix(),\n",
    "        final_estimator=GaussianNB()\n",
    "    )\n",
    "    #estrs_basic_mlp_mixä½œä¸ºç¬¬ä¸€å±‚\n",
    "\n",
    "    stack_basic_mlp_gnb=StackingClassifier(\n",
    "        estimators=estrs_basic_mlp_mix(),\n",
    "        final_estimator=GaussianNB()\n",
    "    )\n",
    "    stack_basic_mlp_lr=StackingClassifier(\n",
    "        estimators=estrs_basic_mlp_mix(),\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "    stack_simple=StackingClassifier(\n",
    "        estimators=estrs_simple(),\n",
    "        final_estimator=LinearSVC()\n",
    "    )\n",
    "    stack_multilayer=StackingClassifier(\n",
    "        estimators=estrs_basic_esdt_mix(),\n",
    "        final_estimator=stack_simple\n",
    "    )\n",
    "    # é…ç½®å¾…è¯„ä¼°çš„åˆ†ç±»å™¨çš„åˆ—è¡¨\n",
    "    clfs = [\n",
    "        stack_basic_svc,\n",
    "        stack_basic_rf,\n",
    "        stack_basic_mlp,\n",
    "        stack_basic_esdt_gnb,\n",
    "        stack_basic_mlp_gnb,\n",
    "        stack_basic_mlp_lr,\n",
    "        stack_simple,\n",
    "        stack_multilayer,\n",
    "        gnb,\n",
    "        rfc,\n",
    "        rdcv,\n",
    "        dt,\n",
    "        lsvc,\n",
    "        svc,\n",
    "        mlp,\n",
    "        plsvc,\n",
    "    ]\n",
    "    return clfs\n",
    "\n",
    "\n",
    "def main():\n",
    "    clfs = get_clfs()\n",
    "\n",
    "    passive_emo = [\"angry\", \"sad\"]\n",
    "    passive_emo_others = passive_emo + [\"others\"]\n",
    "    typical_emo = [\n",
    "        \"happy\",\n",
    "        #    \"neutral\",\n",
    "        \"sad\",\n",
    "    ]\n",
    "    AHSO = [\"angry\", \"neutral\", \"sad\", \"others\"]\n",
    "\n",
    "    AHNS = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "    e_config = typical_emo\n",
    "    f_config = [\"mfcc\"]\n",
    "    # f_config=f_config_def\n",
    "    \n",
    "    # model = get_stacking_clf()\n",
    "    # model=rf\n",
    "\n",
    "    # é…ç½®è¯­æ–™åº“\n",
    "    ## åŒåº“å®éªŒ\n",
    "    # meta_dict = mp.get_single_db_pair_dict(emodb)\n",
    "    ## è·¨åº“å®éªŒ\n",
    "    meta_dict = mp.emodb_savee\n",
    "    res_list = []\n",
    "    for i, clf in enumerate(clfs):\n",
    "        res_dict = assess_model(e_config, f_config, clf, meta_dict)\n",
    "        res_list.append(res_dict)\n",
    "    res_list.sort(key=lambda res_dict: res_dict[\"test_score\"], reverse=True)\n",
    "\n",
    "    for i, res_dict in enumerate(res_list):\n",
    "        print(i + 1, \"--\" * 30, \"\\n\")\n",
    "        for key, value in res_dict.items():\n",
    "            if key == \"er\":\n",
    "                print(key, \":\", value.model)\n",
    "            elif key in [\"report\", \"confusion_matrix\"]:\n",
    "                print(key, \":\\n\", value)\n",
    "            else:\n",
    "                print(key, \":\", value)\n",
    "        # print(model, \"@{model}\")\n",
    "        # print(f\"{train_score=}\")\n",
    "        # print(f\"{test_score=}\")\n",
    "        # # æŸ¥çœ‹æ··æ·†çŸ©é˜µ\n",
    "        # print(confusion_matrix)\n",
    "        # # æŸ¥çœ‹è¾…åŠ©æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š\n",
    "        # print(report)\n",
    "\n",
    "\n",
    "def assess_model(e_config, f_config, model, meta_dict):\n",
    "    er = EmotionRecognizer(\n",
    "        model=model,\n",
    "        **meta_dict,\n",
    "        e_config=e_config,\n",
    "        f_config=f_config,\n",
    "        balance=True,\n",
    "        cross=True,  # æ‰§è¡Œè·¨åº“ä»»åŠ¡,è°ƒæ•´æµ‹è¯•æ•°æ®é›†è¯»å…¥\n",
    "        verbose=0,\n",
    "        # std_scaler=False,\n",
    "        # pca_params=dict(n_components=39)\n",
    "        # std_scaler=False,\n",
    "        # pca={\"n_components\":\"mle\"}\n",
    "        # pca={'n_components': 60}\n",
    "    )\n",
    "    # æ˜¾ç¤ºè°ƒç”¨è®­ç»ƒæ–¹æ³•(ç›¸å½“äºè°ƒç”¨sklearn.estimator.fit)\n",
    "    er.train()\n",
    "    # è¯„ä¼°æ¨¡å‹çš„å„é¡¹æ€§èƒ½æŒ‡æ ‡\n",
    "    train_score = er.train_score()\n",
    "    test_score = er.test_score()\n",
    "    confusion_matrix = er.confusion_matrix()\n",
    "    report = er.check_report()\n",
    "    # cv_score = er.model_cv_score()\n",
    "\n",
    "    # print(model, \"@{model}\")\n",
    "    # print(f\"{train_score=}\")\n",
    "    # print(f\"{test_score=}\")\n",
    "    # # æŸ¥çœ‹æ··æ·†çŸ©é˜µ\n",
    "    # print(confusion_matrix)\n",
    "    # # æŸ¥çœ‹è¾…åŠ©æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š\n",
    "    # print(report)\n",
    "    # äº¤å‰éªŒè¯å¾—åˆ†\n",
    "    # print(f\"{cv_score=}\")\n",
    "\n",
    "    return dict(\n",
    "        er=er,\n",
    "        train_score=train_score,\n",
    "        test_score=test_score,\n",
    "        confusion_matrix=confusion_matrix,\n",
    "        report=report,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        er = main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=RandomForestClassifier())\n",
      "train_score : 1.0\n",
      "test_score : 0.6847826086956522\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         36.95652       63.04348\n",
      "true_sad            0.00000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.37      0.54        46\n",
      "         sad       0.61      1.00      0.76        46\n",
      "\n",
      "    accuracy                           0.68        92\n",
      "   macro avg       0.81      0.68      0.65        92\n",
      "weighted avg       0.81      0.68      0.65        92\n",
      "\n",
      "2 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=3,\n",
      "                                                       n_estimators=10,\n",
      "                                                       random_state=42)),\n",
      "                               ('adab',\n",
      "                                AdaBoostClassifier(learning_rate=0.1,\n",
      "                                                   n_estimators=10,\n",
      "                                                   random_state=42)),\n",
      "                               ('gradb', GradientBoostingClassifier())],\n",
      "                   final_estimator=StackingClassifier(estimators=[('knc',\n",
      "                                                                   KNeighborsClassifier(n_neighbors=3,\n",
      "                                                                                        p=1,\n",
      "                                                                                        weights='distance')),\n",
      "                                                                  ('gnb',\n",
      "                                                                   GaussianNB())],\n",
      "                                                      final_estimator=LinearSVC()))\n",
      "train_score : 1.0\n",
      "test_score : 0.6630434782608695\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        32.608696      67.391304\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.33      0.49        46\n",
      "         sad       0.60      1.00      0.75        46\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.80      0.66      0.62        92\n",
      "weighted avg       0.80      0.66      0.62        92\n",
      "\n",
      "3 ------------------------------------------------------------ \n",
      "\n",
      "er : LinearSVC(max_iter=5000, random_state=42)\n",
      "train_score : 1.0\n",
      "test_score : 0.6630434782608695\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        32.608696      67.391304\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.33      0.49        46\n",
      "         sad       0.60      1.00      0.75        46\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.80      0.66      0.62        92\n",
      "weighted avg       0.80      0.66      0.62        92\n",
      "\n",
      "4 ------------------------------------------------------------ \n",
      "\n",
      "er : RidgeClassifierCV()\n",
      "train_score : 1.0\n",
      "test_score : 0.5978260869565217\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        21.739130      78.260872\n",
      "true_sad           2.173913      97.826088\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.91      0.22      0.35        46\n",
      "         sad       0.56      0.98      0.71        46\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.73      0.60      0.53        92\n",
      "weighted avg       0.73      0.60      0.53        92\n",
      "\n",
      "5 ------------------------------------------------------------ \n",
      "\n",
      "er : DecisionTreeClassifier()\n",
      "train_score : 1.0\n",
      "test_score : 0.5978260869565217\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        43.478260      56.521736\n",
      "true_sad          23.913044      76.086960\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.65      0.43      0.52        46\n",
      "         sad       0.57      0.76      0.65        46\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.61      0.60      0.59        92\n",
      "weighted avg       0.61      0.60      0.59        92\n",
      "\n",
      "6 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=SVC())\n",
      "train_score : 1.0\n",
      "test_score : 0.5869565217391305\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        17.391304      82.608696\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.17      0.30        46\n",
      "         sad       0.55      1.00      0.71        46\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.77      0.59      0.50        92\n",
      "weighted avg       0.77      0.59      0.50        92\n",
      "\n",
      "7 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('mpl',\n",
      "                                MLPClassifier(alpha=0.01, batch_size=512,\n",
      "                                              hidden_layer_sizes=(300,),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              max_iter=400, random_state=42))],\n",
      "                   final_estimator=LogisticRegression())\n",
      "train_score : 1.0\n",
      "test_score : 0.5869565217391305\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        17.391304      82.608696\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.17      0.30        46\n",
      "         sad       0.55      1.00      0.71        46\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.77      0.59      0.50        92\n",
      "weighted avg       0.77      0.59      0.50        92\n",
      "\n",
      "8 ------------------------------------------------------------ \n",
      "\n",
      "er : RandomForestClassifier()\n",
      "train_score : 1.0\n",
      "test_score : 0.5869565217391305\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        17.391304      82.608696\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.17      0.30        46\n",
      "         sad       0.55      1.00      0.71        46\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.77      0.59      0.50        92\n",
      "weighted avg       0.77      0.59      0.50        92\n",
      "\n",
      "9 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=MLPClassifier())\n",
      "train_score : 1.0\n",
      "test_score : 0.5760869565217391\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        15.217391      84.782608\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.15      0.26        46\n",
      "         sad       0.54      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.77      0.58      0.48        92\n",
      "weighted avg       0.77      0.58      0.48        92\n",
      "\n",
      "10 ------------------------------------------------------------ \n",
      "\n",
      "er : GaussianNB()\n",
      "train_score : 0.9895833333333334\n",
      "test_score : 0.5652173913043478\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        13.043478       86.95652\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.13      0.23        46\n",
      "         sad       0.53      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.57        92\n",
      "   macro avg       0.77      0.57      0.46        92\n",
      "weighted avg       0.77      0.57      0.46        92\n",
      "\n",
      "11 ------------------------------------------------------------ \n",
      "\n",
      "er : Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearsvc', LinearSVC(random_state=42))])\n",
      "train_score : 1.0\n",
      "test_score : 0.5652173913043478\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        13.043478       86.95652\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.13      0.23        46\n",
      "         sad       0.53      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.57        92\n",
      "   macro avg       0.77      0.57      0.46        92\n",
      "weighted avg       0.77      0.57      0.46        92\n",
      "\n",
      "12 ------------------------------------------------------------ \n",
      "\n",
      "er : MLPClassifier(max_iter=3000)\n",
      "train_score : 1.0\n",
      "test_score : 0.5217391304347826\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        32.608696      67.391304\n",
      "true_sad          28.260868      71.739128\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.54      0.33      0.41        46\n",
      "         sad       0.52      0.72      0.60        46\n",
      "\n",
      "    accuracy                           0.52        92\n",
      "   macro avg       0.53      0.52      0.50        92\n",
      "weighted avg       0.53      0.52      0.50        92\n",
      "\n",
      "13 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=3,\n",
      "                                                       n_estimators=10,\n",
      "                                                       random_state=42)),\n",
      "                               ('adab',\n",
      "                                AdaBoostClassifier(learning_rate=0.1,\n",
      "                                                   n_estimators=10,\n",
      "                                                   random_state=42)),\n",
      "                               ('gradb', GradientBoostingClassifier())],\n",
      "                   final_estimator=GaussianNB())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "14 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('mpl',\n",
      "                                MLPClassifier(alpha=0.01, batch_size=512,\n",
      "                                              hidden_layer_sizes=(300,),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              max_iter=400, random_state=42))],\n",
      "                   final_estimator=GaussianNB())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "15 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB())],\n",
      "                   final_estimator=LinearSVC())\n",
      "train_score : 1.0\n",
      "test_score : 0.5\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy              0.0          100.0\n",
      "true_sad                0.0          100.0\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.00      0.00      0.00        46\n",
      "         sad       0.50      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.50        92\n",
      "   macro avg       0.25      0.50      0.33        92\n",
      "weighted avg       0.25      0.50      0.33        92\n",
      "\n",
      "16 ------------------------------------------------------------ \n",
      "\n",
      "er : SVC()\n",
      "train_score : 0.9895833333333334\n",
      "test_score : 0.5\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy              0.0          100.0\n",
      "true_sad                0.0          100.0\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.00      0.00      0.00        46\n",
      "         sad       0.50      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.50        92\n",
      "   macro avg       0.25      0.50      0.33        92\n",
      "weighted avg       0.25      0.50      0.33        92\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# file=r'D:\\repos\\CCSER\\SER\\data\\savee\\AudioData\\DC\\h01.wav'\n",
    "# file=meta.speech_dbs_dir/emodb/r'wav/03a01Fa.wav'\n",
    "# predict_res=er.predict(file)\n",
    "# print(f\"{predict_res=}\")\n",
    "# predict_proba=er.predict_proba(file)\n",
    "# print(f\"{predict_proba=}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# data=rec.load_data()\n",
    "\n",
    "# data = load_data(\n",
    "#     train_meta_files=train_meta_files,\n",
    "#     test_meta_files=test_meta_files,\n",
    "#     balance=False,\n",
    "# )\n",
    "\n",
    "# X_train = data[\"X_train\"]\n",
    "# y_train = data[\"y_train\"]\n",
    "# rec.train(X_train=X_train, y_train=y_train)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}