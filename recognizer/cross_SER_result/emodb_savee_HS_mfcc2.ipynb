{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# from typing_extensions import deprecated\n",
    "import warnings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import config.MetaPath as mp\n",
    "from config.MetaPath import test_emodb_csv\n",
    "from config.algoparams import ava_ML_algorithms\n",
    "import random\n",
    "from time import time\n",
    "from config.algoparams import ava_cv_modes\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from deprecated import deprecated\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    fbeta_score,\n",
    "    make_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ShuffleSplit,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "from audio.extractor import AudioExtractor, load_data_from_meta\n",
    "import config.EF as ef\n",
    "from config.EF import AHNPS, e_config_def, f_config_def, validate_emotions\n",
    "from config.MetaPath import (\n",
    "    emodb,\n",
    "    meta_paths_of_db,\n",
    "    ravdess,\n",
    "    savee,\n",
    "    validate_partition,\n",
    "    project_dir,\n",
    ")\n",
    "import config.MetaPath as meta\n",
    "from audio.core import best_estimators, extract_feature_of_audio\n",
    "from config.algoparams import random_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "class EmotionRecognizer:\n",
    "    \"\"\"A class for training, testing ,predicting,anaylzing emotions based on\n",
    "    speech's features that are extracted and fed into `sklearn` model\n",
    "\n",
    "    examples\n",
    "    -\n",
    "    rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=[ravdess],train_dbs=[ravdess], verbose=1)\n",
    "\n",
    "    rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=emodb,train_dbs=emodb, verbose=1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        classification_task=True,\n",
    "        dbs=None,\n",
    "        e_config=None,\n",
    "        f_config=None,\n",
    "        train_dbs=None,\n",
    "        test_dbs=None,\n",
    "        balance=False,\n",
    "        shuffle=True,\n",
    "        override_csv=True,\n",
    "        cross=False,  # 表示跨库(这回让其读取train_db_econfig.csv作为测试集,样例更丰富,更合理,当然还可以进一步改进,读取all_db_econfg.csv,但是这里暂不执行)\n",
    "        verbose=0,\n",
    "        **feature_transforms,\n",
    "        # **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            model (sklearn model): the model used to detect emotions. If `model` is None, then self.determine_best_model()\n",
    "                will be automatically called\n",
    "                这个参数其实就是sklearn中的Estimator对象,例如SVC()示例化出来的对象\n",
    "            emotions (list): list of emotions to be used. Note that these emotions must be available in\n",
    "                RAVDESS & EMODB Datasets, available nine emotions are the following:\n",
    "                    'neutral', 'calm', 'happy', 'sad', 'angry', 'fear', 'disgust', 'ps' ( pleasant surprised ), 'boredom'.\n",
    "                默认识别三种情感\n",
    "                Default is [\"sad\", \"neutral\", \"happy\"].\n",
    "            情感数据库的使用开关:\n",
    "            ravdess (bool): whether to use RAVDESS Speech datasets, default is True\n",
    "            emodb (bool): whether to use EMO-DB Speech dataset, default is True,\n",
    "            custom_db (bool): whether to use custom Speech dataset that is located in `data/train-custom`\n",
    "                and `data/test-custom`, default is True\n",
    "            输出文件名的指定(应该指定为csv文件,即参数带有扩展后缀csv)\n",
    "            ravdess_name (str): the name of the output CSV file for RAVDESS dataset, default is \"ravdess.csv\"\n",
    "            emodb_name (str): the name of the output CSV file for EMO-DB dataset, default is \"emodb.csv\"\n",
    "            custom_db_name (str): the name of the output CSV file for the custom dataset, default is \"custom.csv\"\n",
    "            指定需要提取的情感特征,默认三种:mfcc,chroma,mel\n",
    "            features (list): list of speech features to use, default is [\"mfcc\", \"chroma\", \"mel\"]\n",
    "                (i.e MFCC, Chroma and MEL spectrogram )\n",
    "            指定要使用的分类模型还是回归模型,默认使用分类模型\n",
    "            classification (bool): whether to use classification or regression, default is True\n",
    "            balance (bool): whether to balance the dataset ( both training and testing ), default is True\n",
    "            verbose (bool/int): whether to print messages on certain tasks, default is 1\n",
    "        Note that when `ravdess`, `emodb` and `custom_db` are set to `False`, `ravdess` will be set to True\n",
    "        automatically.\n",
    "        \"\"\"\n",
    "        # emotions\n",
    "        self.e_config = e_config if e_config else e_config_def\n",
    "        # make sure that there are only available emotions\n",
    "        validate_emotions(self.e_config)\n",
    "        self.f_config = f_config if f_config else f_config_def\n",
    "        # 转换为字典格式(待优化)\n",
    "        # @deprecated(version='1.0', reason='请使用 new_function() 代替')\n",
    "        # self._f_config_dict: dict[str, bool] = get_f_config_dict(self.f_config)\n",
    "        self.train_dbs = train_dbs\n",
    "        self.test_dbs = test_dbs\n",
    "\n",
    "        # print(self.train_meta_files, self.test_meta_files)\n",
    "        self.feature_transforms = feature_transforms\n",
    "\n",
    "        # 可以使用python 默认参数来改造写法\n",
    "        # 默认执行分类任务\n",
    "        self.classification_task = classification_task\n",
    "        self.verbose = verbose\n",
    "        # boolean attributes\n",
    "        self.override_csv = override_csv\n",
    "        self.shuffle = shuffle\n",
    "        self.balance = balance\n",
    "        self.cross = cross\n",
    "        # 非构造器初始化变量\n",
    "        self.data_loaded = False\n",
    "        self.model_trained = False\n",
    "        self.ae = None\n",
    "        self.dbs = dbs if dbs else [ravdess]\n",
    "        # 鉴于数据集(特征和标签)在评估方法时将反复用到,因此这里将设置相应的属性来保存它们\n",
    "        # 另一方面,如果模仿sklearn中的编写风格,其实是将数据和模型计算分布在不同的模块(类)中,比如\n",
    "        # sklearn.datasets负责数据集生成\n",
    "        # sklearn.model_selection负责划分数据集和训练集\n",
    "        # sklearn.algorithms* 负责创建模型\n",
    "        # sklearn.metrics 负责评估模型\n",
    "        # 设置相应的属性的方便之处在于方法的调用可以少传参\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "        self.y_pred = []\n",
    "        self.train_audio_paths = []\n",
    "        self.test_audio_paths = []\n",
    "        # 开始填充数据(最先开始的步骤,放在init中随着初始化实例的时候执行)\n",
    "        # self.load_data()\n",
    "        # 属性的先后位置会影响程序的运行\n",
    "        # print(\"@{model}\")\n",
    "        # print(model,\"\\ncomparing and choosing the best model...\")\n",
    "        # !RandomForestClassifier实例不能直接用bool()来判断,会提示estimators_不存在\n",
    "        self.model = model\n",
    "        # if self.model is None:\n",
    "        # 依赖于boolean attributes\n",
    "\n",
    "        self.train_meta_files = meta_paths_of_db(\n",
    "            db=self.train_dbs,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=\"train\",\n",
    "        )\n",
    "        # 判断跨库任务\n",
    "        test_meta_partition = \"test\"\n",
    "        if self.cross:\n",
    "            test_meta_partition = \"train\"\n",
    "        self.test_meta_files = meta_paths_of_db(\n",
    "            db=self.test_dbs,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=test_meta_partition,\n",
    "        )\n",
    "\n",
    "    # def prepare():\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        导入指定的语料库数据,并提取特征\n",
    "        Loads and extracts features from the audio files for the db's specified\n",
    "        - 注意,由于balance操作对于数据集划分有一定要求,不是任何数据集都可以执行balance操作(例如三分类中,test set中只缺失了某一个类别的样本,这中情况下执行balance,将导致测试集样本数量为空)\n",
    "        \"\"\"\n",
    "        # 判断是否已经导入过数据.如果已经导入,则跳过,否则执行导入\n",
    "        if not self.data_loaded:\n",
    "            # 调用extractor中的数据导入函数\n",
    "            data = load_data_from_meta(\n",
    "                train_meta_files=self.train_meta_files,\n",
    "                test_meta_files=self.test_meta_files,\n",
    "                f_config=self.f_config,\n",
    "                e_config=self.e_config,\n",
    "                classification_task=self.classification_task,\n",
    "                balance=self.balance,\n",
    "                shuffle=self.shuffle,\n",
    "                feature_transforms=self.feature_transforms,\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            # 设置实例的各个属性\n",
    "            # 事实上,也可以直接用load_data_from_meta返回的结果中的ae对象,赋值ER对象(self.ae=data[\"ae\"])\n",
    "            self.ae = data[\"ae\"]\n",
    "            self.X_train = data[\"X_train\"]\n",
    "            self.X_test = data[\"X_test\"]\n",
    "            self.y_train = data[\"y_train\"]\n",
    "            self.y_test = data[\"y_test\"]\n",
    "            self.train_audio_paths = data[\"train_audio_paths\"]\n",
    "            self.test_audio_paths = data[\"test_audio_paths\"]\n",
    "\n",
    "            self.balanced_success(data)\n",
    "            if self.verbose:\n",
    "                print(\"[I] Data loaded\\n\")\n",
    "                print(f\"{self.ae=}\")\n",
    "                print(f\"{self.ae.pca=}🎈\")\n",
    "            self.data_loaded = True\n",
    "            # print(id(self))\n",
    "            if self.verbose > 1:\n",
    "                print(vars(self))\n",
    "\n",
    "    def balanced_success(self, res):\n",
    "        self.balance = res[\"balance\"]\n",
    "\n",
    "    def train(self, choosing=False, verbose=1):\n",
    "        \"\"\"\n",
    "        载入数据并训练模型(sklearn.estimator.fit)\n",
    "        Train the model, if data isn't loaded, it will be loaded automatically\n",
    "\n",
    "        X_train=None, y_train=None\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            # if data isn't loaded yet, load it then\n",
    "            self.load_data()\n",
    "        if self.verbose > 1:\n",
    "            print(\"@{self.model}:\")\n",
    "            print(self.model)\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        if not self.model_trained or choosing:\n",
    "            X_train = self.X_train\n",
    "            y_train = self.y_train\n",
    "            # estimator训练(fit)模型\n",
    "            model.fit(X=X_train, y=y_train)\n",
    "            self.model_trained = True\n",
    "        if self.verbose > 1:\n",
    "            if choosing == True:\n",
    "                print(\n",
    "                    f\"[I] Model trained with{choosing=},choosing the best model,override the trained model..\"\n",
    "                )\n",
    "\n",
    "    def predict(self, audio_path):\n",
    "        \"\"\"\n",
    "        预测单个音频的情感\n",
    "        由于是单个音频的情感预测,因此不需要考虑shuffle和balance这些操作,只需要提取语音特征,然后进行调用模型预测即可\n",
    "        given an `audio_path`, this method extracts the features\n",
    "        and predicts the emotion\n",
    "\n",
    "        以下语句不再适合具有pca降维操作下的情形\n",
    "        feature_audio = extract_feature_of_audio(audio_path, self.f_config)\n",
    "        print(feature1.shape)\n",
    "        print(feature1,\"@{feature1}\",feature1.shape)\n",
    "        feature2=feature1.T\n",
    "        print(feature2,\"@{feature2}\",feature2.shape)\n",
    "        print(feature3,\"@{feature3}\",feature3.shape)\n",
    "        \"\"\"\n",
    "        feature_audio = self.extract_feature_single_audio(audio_path)\n",
    "\n",
    "        feature = feature_audio.reshape(1, -1)\n",
    "        model = self.model if self.model else self.best_model()\n",
    "        res = model.predict(feature)\n",
    "        # res可能是个列表\n",
    "        # print(res, \"@{res}\")\n",
    "        return res[0]\n",
    "\n",
    "    def extract_feature_single_audio(self, audio_path):\n",
    "        \"\"\"extract a single audio file feature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        audio_path : path\n",
    "            audio path\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            audio feature\n",
    "        \"\"\"\n",
    "        ae: AudioExtractor = self.ae\n",
    "        pca = ae.pca\n",
    "        print(pca, \"@{pca} in 'predict' method\")\n",
    "        # if pca:\n",
    "        #     feature_audio=pca.transform(feature_audio)\n",
    "        #     print(feature_audio.shape, \"@{feature_audio.shape}\")\n",
    "        feature_audio = ae.extract_features(partition=\"test\", audio_paths=[audio_path])\n",
    "        return feature_audio\n",
    "        # return self.model.predict(feature2)[0]\n",
    "\n",
    "    def peek_test_set(self, n=5):\n",
    "        res = [\n",
    "            self.test_audio_paths[:n],\n",
    "            self.X_test[:n],\n",
    "            self.y_test[:n],\n",
    "            self.y_pred[:n],\n",
    "        ]\n",
    "        return res\n",
    "\n",
    "    def predict_proba(self, audio_path):\n",
    "        \"\"\"\n",
    "        Predicts the probability of each emotion.\n",
    "        \"\"\"\n",
    "        if self.classification_task:\n",
    "            # feature = extract_feature_of_audio(audio_path, self.f_config).reshape(1, -1)\n",
    "            feature = self.extract_feature_single_audio(audio_path)\n",
    "            proba = self.model.predict_proba(feature)[0]\n",
    "            result = {}\n",
    "            for emotion, prob in zip(self.model.classes_, proba):\n",
    "                result[emotion] = prob\n",
    "            return result\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Probability prediction doesn't make sense for regression\"\n",
    "            )\n",
    "\n",
    "    def show_second(self):\n",
    "        peeker = er.peek_test_set(2)\n",
    "        feature = peeker[1][1]\n",
    "        audio_path = peeker[0][1]\n",
    "        feature_pred = self.predict(audio_path)\n",
    "        print(feature[:5], feature_pred[:5])\n",
    "\n",
    "    def grid_search(self, params, n_jobs=2, verbose=3):\n",
    "        \"\"\"\n",
    "        使用网格化搜索的方式搜索最优超参数\n",
    "        Performs GridSearchCV on `params` passed on the `self.model`\n",
    "        And returns the tuple: (best_estimator, best_params, best_score).\n",
    "        \"\"\"\n",
    "        score = accuracy_score if self.classification_task else mean_absolute_error\n",
    "        grid = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=params,\n",
    "            scoring=make_scorer(score),\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "            cv=3,\n",
    "        )\n",
    "        # 调用fit开始传入数据集并搜索\n",
    "        X_train, y_train = self.X_train, self.y_train\n",
    "        if X_train is None or y_train is None:\n",
    "            raise ValueError(\"X_train and y_train are None\")\n",
    "        # fit过程是一个耗时的过程\n",
    "        grid_result = grid.fit(X_train, y_train)\n",
    "        return (\n",
    "            grid_result.best_estimator_,\n",
    "            grid_result.best_params_,\n",
    "            grid_result.best_score_,\n",
    "        )\n",
    "\n",
    "    def best_model(self):\n",
    "        \"\"\"\n",
    "        从常见的模型中计算出最好的Estimator(model)\n",
    "        计算最优model时,也可以考虑创建新的ER实例来做计算最优model的用途,但会增加开销\n",
    "\n",
    "        Loads best estimators and determine which is best for test data,\n",
    "        and then set it to `self.model`.\n",
    "        # 使用MSE来评价回归模型,使用accuracy来评价分类模型\n",
    "        In case of regression, the metric used is MSE(均方误差) and accuracy for classification.\n",
    "\n",
    "        Note that the execution of this method may take several minutes due\n",
    "        to training all estimators (stored in `grid` folder) for determining the best possible one.\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            self.load_data()\n",
    "\n",
    "        # loads estimators\n",
    "        estimators = best_estimators()\n",
    "\n",
    "        result = []\n",
    "\n",
    "        if self.verbose:\n",
    "            # 控制是否显示进度条\n",
    "            # 通过tqdm封装estimator这个可迭代对象,就可以在遍历estimator时,控制进度条的显示\n",
    "            estimators = tqdm(estimators)\n",
    "\n",
    "        for epc in estimators:\n",
    "            estimator, params_, cv_score_ = epc\n",
    "            ecn = estimator.__class__.__name__\n",
    "\n",
    "            if self.verbose:\n",
    "                # 如果启用verbose选项,那么estimators会被tqdm包装\n",
    "                # 此时可以通过set_description方法来修改进度条的描述信息\n",
    "                # 比如,estimators.set_description(f\"Evaluating {estimator.__class__.__name__}\")\n",
    "                estimators.set_description(f\"Evaluating <{ecn}>\")\n",
    "            # 为例避免相互干扰,每测试模型就创建一个ER对象(er)\n",
    "            er = EmotionRecognizer(\n",
    "                model=estimator,\n",
    "                emotions=self.e_config,\n",
    "                classification_task=self.classification_task,\n",
    "                f_config=self.f_config,\n",
    "                balance=self.balance,\n",
    "                override_csv=False,\n",
    "                verbose=0,\n",
    "            )\n",
    "            # data already loaded\n",
    "            er.X_train = self.X_train\n",
    "            er.X_test = self.X_test\n",
    "            er.y_train = self.y_train\n",
    "            er.y_test = self.y_test\n",
    "            er.data_loaded = True\n",
    "            # train the model\n",
    "            er.train(verbose=0)\n",
    "            # get test accuracy\n",
    "            accuracy = er.test_score()\n",
    "            # append to result\n",
    "            result.append((er.model, accuracy))\n",
    "\n",
    "            # 方法2:(小心使用)\n",
    "            # 使用本对象self而不是在创建一个ER对象\n",
    "            # self.model = estimator\n",
    "            # er = self\n",
    "            # 以下的计算是用来选出model的,而不是直接作为self对象的属性,这里将self赋值给er,以示区别\n",
    "            # train(fit) the model\n",
    "            # 如果设置verbose=1,则会逐个打印当前计算的模型(进度不是同一条)\n",
    "            # er.train(choosing=True, verbose=0)\n",
    "            # train(fit) the model\n",
    "            # self.train(verbose=1)\n",
    "            # accuracy = er.test_score(choosing=True)\n",
    "            # append to result\n",
    "            # result.append((estimator, accuracy))\n",
    "\n",
    "            print(f\"\\n[I] {ecn} with {accuracy} test accuracy\")\n",
    "\n",
    "        # sort the result\n",
    "        # regression: best is the lower, not the higher\n",
    "        # classification: best is higher, not the lower\n",
    "        result = sorted(\n",
    "            result, key=lambda item: item[1], reverse=self.classification_task\n",
    "        )\n",
    "        best_estimator = result[0][0]\n",
    "        accuracy = result[0][1]\n",
    "\n",
    "        self.model = best_estimator\n",
    "        self.model_trained = True\n",
    "        if self.verbose:\n",
    "            if self.classification_task:\n",
    "                print(\n",
    "                    f\"[🎈] Best model : {self.model.__class__.__name__} with {accuracy * 100:.3f}% test accuracy\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[I] Best model : {self.model.__class__.__name__} with {accuracy:.5f} mean absolute error\"\n",
    "                )\n",
    "        return best_estimator\n",
    "\n",
    "    def test_score(self, choosing=False, verbose=0, report=False):\n",
    "        \"\"\"\n",
    "        Calculates score on testing data\n",
    "        Please call the `train` method before call this method.\n",
    "\n",
    "        just like sklearn convention:call estimator.call `fit` at first,then call `predict` or `score` method\n",
    "\n",
    "        if `self.classification` is True, the metric used is accuracy,\n",
    "        Mean-Squared-Error is used otherwise (regression)\n",
    "\n",
    "        1.调用训练好的模型进行预测\n",
    "\n",
    "        2.如果model是None,那么调用best_model获取最优模型(这个过程会遍历一个可用模型列表,是通过调用ER实例的test_score()方法来计算,不过这里不会遇到None的情况,因此间接递归调用不超过2层)\n",
    "        \"\"\"\n",
    "        X_test = self.X_test\n",
    "        y_test = self.y_test\n",
    "\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        self.validate_empty_array(X_test=X_test, y_test=y_test)\n",
    "\n",
    "        # 预测计算\n",
    "        if verbose > 1:\n",
    "            print(X_test.shape, y_test.shape, \"🎈\")\n",
    "        # 根据当前模型进行预测(直接调用estimator的predict方法)\n",
    "        y_pred = model.predict(X_test)  # type: ignore\n",
    "        # 如果处于best_model的过程中调用本方法(choosing=True),则上述预测内容作为临时结果不写入对象属性保存;否则作为最终结果写入属性保存\n",
    "        # 默认choosing=False,也就是将结果保存到对象属性中\n",
    "        if choosing == False:\n",
    "            self.y_pred = np.array(y_pred)\n",
    "\n",
    "        if self.classification_task:\n",
    "            res = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "            # 结果和:er.model.score(er.X_test,er.y_test)一样,但是这种做法回独立将X_test预测一遍,而不保存预测结果,只给出得分\n",
    "        else:\n",
    "            res = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "        if report:\n",
    "            self.check_report(y_test, y_pred)\n",
    "        return res\n",
    "\n",
    "    def check_report(self):\n",
    "        \"\"\"输出模型当前结果的多个指标报告\n",
    "        对于分类任务,包括precision(查准率),recall(回召或查全率),f1-score以及各类别的样本数量\n",
    "        对于多分类,还有一些综合的指标(macro,weighted),每个单元格结合两个维度的表头进行理解和阅读\n",
    "\n",
    "        由于跨库实验比较困难,有的样本类别无法被正确分类(所有该类别都被错误分类),此时classification_report方法会提出警告,除非使用zero_division参数替换掉默认的warn.\n",
    "        \"\"\"\n",
    "        y_test = self.y_test\n",
    "        y_pred = self.y_pred\n",
    "\n",
    "        report = classification_report(y_true=y_test, y_pred=y_pred, zero_division=0)\n",
    "        # print(report, self.model.__class__.__name__)\n",
    "        return report\n",
    "\n",
    "    def model_cv_score(\n",
    "        self,\n",
    "        choosing=False,\n",
    "        verbose=1,\n",
    "        mean_only=True,\n",
    "        n_splits=5,\n",
    "        test_size=0.2,\n",
    "        cv_mode=\"sss\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        使用交叉验证的方式来评估模型\n",
    "        Calculates score on testing data\n",
    "        \"\"\"\n",
    "        X_train = self.X_train\n",
    "        y_train = self.y_train\n",
    "        # 调用训练好的模型进行预测\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        self.validate_empty_array(X_train, y_train)\n",
    "\n",
    "        # 预测计算\n",
    "        if verbose > 1:\n",
    "            print(X_train.shape, y_train.shape, \"🎈\")\n",
    "            print(f\"{n_splits=}\")\n",
    "        n_splits = int(n_splits)\n",
    "\n",
    "        y_pred = model.predict(X_train)  # type: ignore\n",
    "        if choosing == False:\n",
    "            self.y_pred = np.array(y_pred)\n",
    "        # 交叉验证的方式评估模型的得分\n",
    "        cv_mode_dict = dict(\n",
    "            sss=StratifiedShuffleSplit(\n",
    "                n_splits=n_splits, test_size=test_size, random_state=0\n",
    "            ),\n",
    "            ss=ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=0),\n",
    "            kfold=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "            skfold=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "        )\n",
    "        cv_mode_selected = cv_mode_dict[cv_mode]\n",
    "        if verbose > 1:\n",
    "            print(f\"{cv_mode=}🎈\")\n",
    "        res = [0]\n",
    "        if self.classification_task:\n",
    "            # res = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "            # 将交叉验证器cv传递给cross_val_score函数执行评估操作\n",
    "            # 而非自己使用来完成k折交叉验证\n",
    "            # 所有对象都有__class__属性以及__name__二级属性\n",
    "            if model.__class__.__name__ in ava_ML_algorithms:\n",
    "                res = cross_val_score(model, X_train, y_train, cv=cv_mode_selected)\n",
    "                if mean_only:\n",
    "                    res = res.mean()\n",
    "\n",
    "        else:\n",
    "            # 使用回归器的情况\n",
    "            res = mean_squared_error(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "        if self.verbose > 2:\n",
    "            report = classification_report(\n",
    "                y_true=y_train, y_pred=y_pred\n",
    "            )  # 训练集上,几乎总是是满分\n",
    "            print(report, self.model.__class__.__name__)\n",
    "        return res\n",
    "\n",
    "    def validate_empty_array(self, X_test=[], y_test=[]):\n",
    "        if len(X_test) == 0:\n",
    "            raise ValueError(\"X is empty\")\n",
    "        if len(y_test) == 0:\n",
    "            raise ValueError(\"y is empty\")\n",
    "\n",
    "    def meta_paths_of_db(self, db, partition=\"test\"):\n",
    "        res = meta_paths_of_db(\n",
    "            db=db,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=partition,\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    def update_test_set(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def update_test_set_by_meta(self, test_meta):\n",
    "        \"\"\"\n",
    "        这个函数设计用来做跨库识别试验\n",
    "        仅仅替换测试集为不同库,本身没有针对跨库进行优化\n",
    "\n",
    "\n",
    "        Load test data from given test metadata file paths and update instance's test set attributes.\n",
    "\n",
    "        Args:\n",
    "            test_meta (list of str): List of file paths of test metadata files.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        examples:\n",
    "        >>> rec = EmotionRecognizer(model=my_model,**meta_dict, verbose=1)\n",
    "        >>> rec.train()\n",
    "        >>> rec.update_test_set_by_meta(test_emodb_csv)\n",
    "        >>> rec.test_meta_files\n",
    "        >>> 'meta_files\\\\test_emodb_HNS.csv'\n",
    "        >>> rec.X_test.shape\n",
    "        >>> (43,180)\n",
    "\n",
    "        >>> rec.test_score()\n",
    "        >>> 0.4651\n",
    "\n",
    "        \"\"\"\n",
    "        # rec.update_test_set_by_meta(test_emodb_csv)\n",
    "        self.test_meta_files = test_meta\n",
    "        print(test_meta, \"@{test_meta}\")\n",
    "        test_data = load_data_from_meta(\n",
    "            test_meta_files=test_meta, e_config=self.e_config, f_config=self.f_config\n",
    "        )\n",
    "\n",
    "        X_test = test_data[\"X_test\"]\n",
    "        y_test = test_data[\"y_test\"]\n",
    "        # 设置实例的各个属性\n",
    "        self.test_audio_paths = test_data[\"test_audio_paths\"]\n",
    "        self.update_test_set(X_test, y_test)\n",
    "\n",
    "    def train_score(self, X_train=None, y_train=None):\n",
    "        \"\"\"\n",
    "        Calculates accuracy score on training data\n",
    "        if `self.classification` is True, the metric used is accuracy,\n",
    "        Mean-Squared-Error is used otherwise (regression)\n",
    "        \"\"\"\n",
    "        if X_train is None or y_train is None:\n",
    "            X_train = self.X_train\n",
    "            y_train = self.y_train\n",
    "        y_pred = self.model.predict(X_train)\n",
    "        if self.classification_task:\n",
    "            return accuracy_score(y_true=y_train, y_pred=y_pred)\n",
    "        else:\n",
    "            return mean_squared_error(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    def train_fbeta_score(self, beta):\n",
    "        y_pred = self.model.predict(self.X_train)\n",
    "        y_train = self.y_train\n",
    "        if y_train is None:\n",
    "            raise ValueError(\"y_train is None\")\n",
    "\n",
    "        return fbeta_score(y_true=y_train, y_pred=y_pred, beta=beta, average=\"micro\")\n",
    "\n",
    "    def test_fbeta_score(self, beta):\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        if y_test is None:\n",
    "            raise ValueError(\"y_test is None\")\n",
    "        return fbeta_score(y_true=y_test, y_pred=y_pred, beta=beta, average=\"micro\")\n",
    "\n",
    "    def confusion_matrix(self, percentage=True, labeled=True):\n",
    "        \"\"\"\n",
    "        Computes confusion matrix to evaluate the test accuracy of the classification\n",
    "        and returns it as numpy matrix or pandas dataframe (depends on params).\n",
    "        params:\n",
    "            percentage (bool): whether to use percentage instead of number of samples, default is True.\n",
    "            labeled (bool): whether to label the columns and indexes in the dataframe.\n",
    "        \"\"\"\n",
    "        if not self.classification_task:\n",
    "            raise NotImplementedError(\n",
    "                \"Confusion matrix works only when it is a classification problem\"\n",
    "            )\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        if y_test is None:\n",
    "            raise ValueError(\"y_test is None\")\n",
    "        matrix = confusion_matrix(\n",
    "            y_true=y_test, y_pred=y_pred, labels=self.e_config\n",
    "        ).astype(np.float32)\n",
    "        if percentage:\n",
    "            for i in range(len(matrix)):\n",
    "                matrix[i] = matrix[i] / np.sum(matrix[i])\n",
    "            # make it percentage\n",
    "            matrix *= 100\n",
    "        if labeled:\n",
    "            matrix_df = pd.DataFrame(\n",
    "                matrix,\n",
    "                index=[f\"true_{e}\" for e in self.e_config],\n",
    "                columns=[f\"predicted_{e}\" for e in self.e_config],\n",
    "            )\n",
    "        return matrix_df\n",
    "\n",
    "    def draw_confusion_matrix(self):\n",
    "        \"\"\"Calculates the confusion matrix and shows it\"\"\"\n",
    "        matrix = self.confusion_matrix(percentage=False, labeled=False)\n",
    "        # TODO: add labels, title, legends, etc.\n",
    "        pl.imshow(matrix, cmap=\"binary\")\n",
    "        pl.show()\n",
    "\n",
    "    def count_samples_in_partition(self, emotion, partition):\n",
    "        \"\"\"\n",
    "        Get the number of data samples of the `emotion` class in a particular `partition` ('test' or 'train').\n",
    "\n",
    "        :param emotion: The emotion class to count.\n",
    "        :param partition: The partition to count samples in ('test' or 'train').\n",
    "        :return: The number of data samples of the `emotion` class in the `partition`.\n",
    "        :raises ValueError: If `y_test` or `y_train` is `None`.\n",
    "        \"\"\"\n",
    "        partition = validate_partition(partition, Noneable=False)\n",
    "        if partition == \"test\":\n",
    "            if self.y_test is None:\n",
    "                raise ValueError(\"y_test is None\")\n",
    "            count = sum(1 for y in self.y_test if y == emotion)\n",
    "        else:\n",
    "            if self.y_train is None:\n",
    "                raise ValueError(\"y_train is None\")\n",
    "            count = sum(1 for y in self.y_train if y == emotion)\n",
    "        return count\n",
    "\n",
    "    def count_samples_by_class(self):\n",
    "        \"\"\"\n",
    "        Returns a dataframe that contains the number of training\n",
    "        and testing samples for all emotions.\n",
    "        Note that if data isn't loaded yet, it'll be loaded\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            self.load_data()\n",
    "        train_samples = []\n",
    "        test_samples = []\n",
    "        total = []\n",
    "        for emotion in self.e_config:\n",
    "            n_train = self.count_samples_in_partition(emotion, \"train\")\n",
    "            n_test = self.count_samples_in_partition(emotion, \"test\")\n",
    "            train_samples.append(n_train)\n",
    "            test_samples.append(n_test)\n",
    "            total.append(n_train + n_test)\n",
    "\n",
    "        # get total\n",
    "        total.append(sum(train_samples) + sum(test_samples))\n",
    "        train_samples.append(sum(train_samples))\n",
    "        test_samples.append(sum(test_samples))\n",
    "        return pd.DataFrame(\n",
    "            data={\"train\": train_samples, \"test\": test_samples, \"total\": total},\n",
    "            index=self.e_config + [\"total\"],\n",
    "        )\n",
    "\n",
    "    def get_random_emotion_index(self, emotion, partition=\"train\"):\n",
    "        \"\"\"\n",
    "        Returns a random index of a `partition` sample with the given `emotion`.\n",
    "\n",
    "        Args:\n",
    "            emotion (str): The name of the emotion to look for.\n",
    "            partition (str): The partition to sample from. Only \"train\" or \"test\" are accepted.\n",
    "\n",
    "        Returns:\n",
    "            int: The index of a random sample with the given `emotion` in the specified `partition`.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If `partition` is not \"train\" or \"test\".\n",
    "        \"\"\"\n",
    "        partition = validate_partition(partition, Noneable=False)\n",
    "        indices = []\n",
    "        if partition == \"train\":\n",
    "            indices = [i for i, y in enumerate(self.y_train) if y == emotion]\n",
    "        elif partition == \"test\":\n",
    "            indices = [i for i, y in enumerate(self.y_test) if y == emotion]\n",
    "\n",
    "        return random.choice(indices)\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# def get_stacking_clf():\n",
    "#     random_state = 42\n",
    "#     # 定义初级学习器\n",
    "#     estimators_basic_mix = estrs_basic_mix( )\n",
    "#     estimators_dt = estrs_esdt(random_state)\n",
    "#     estimators_basic_dt_mix = estrs_basic_dt_mix( )\n",
    "#     estimators_basic_dt_mpl_mix = estrs_basic_dt_mpl_mix(\n",
    "#         random_state, estimators_basic_dt_mix\n",
    "#     )\n",
    "#     estimators_basic_dmb_mix = estimators_basic_dt_mix + [(\"gnb\", GaussianNB())]\n",
    "#     # 简单堆叠\n",
    "#     stack_dt_linear = StackingClassifier(\n",
    "#         estimators=estimators_dt, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     stack_mix_linear = StackingClassifier(\n",
    "#         estimators=estimators_basic_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     # 收敛困难(效果不如mix_linear)\n",
    "#     stack_mix_svm = StackingClassifier(\n",
    "#         estimators=estimators_basic_mix,\n",
    "#         final_estimator=SVC(),\n",
    "#     )\n",
    "#     stack_basic_dt_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dt_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     # 数据量太小\n",
    "#     stack_basic_dt_mlp_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dt_mpl_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     stack_dmb_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dmb_mix,\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#     )\n",
    "#     # stack1 = StackingClassifier(\n",
    "#     #     estimators=[(\"gbc\", GaussianNB())],\n",
    "#     #     final_estimator=LogisticRegression(),\n",
    "#     # )\n",
    "#     # 多层堆叠\n",
    "#     ##定义最后一层\n",
    "#     stack_final_layer = StackingClassifier(\n",
    "#         estimators=[(\"gbc\", GradientBoostingClassifier()), (\"svc\", SVC())],\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#     )\n",
    "#     ##堆叠二层(容易过拟合)\n",
    "#     stack_multilayer = StackingClassifier(\n",
    "#         estimators=estimators_dt, final_estimator=stack_final_layer\n",
    "#     )\n",
    "#     stack = StackingClassifier(estimators=estimators_dt, final_estimator=SVC())\n",
    "\n",
    "#     # return stack_dt_linear\n",
    "#     # return stack_mix_linear\n",
    "#     # return stack_mix_svm\n",
    "#     # return stack_basic_dt_mix\n",
    "#     return stack_dmb_mix\n",
    "\n",
    "\n",
    "def estrs_basic_mlp_mix():\n",
    "    estimators_basic_dt_mpl_mix = estrs_basic_mix() + [\n",
    "        (\n",
    "            \"mpl\",\n",
    "            MLPClassifier(\n",
    "                alpha=0.01,\n",
    "                batch_size=512,\n",
    "                hidden_layer_sizes=(300,),\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=400,\n",
    "                random_state=random_state,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return estimators_basic_dt_mpl_mix\n",
    "\n",
    "\n",
    "def estrs_esdt():\n",
    "    estimators_dt = [\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=10, max_depth=3, random_state=random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"adab\",\n",
    "            AdaBoostClassifier(\n",
    "                n_estimators=10, learning_rate=0.1, random_state=random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\"gradb\", GradientBoostingClassifier()),\n",
    "    ]\n",
    "\n",
    "    return estimators_dt\n",
    "\n",
    "\n",
    "def estrs_basic_esdt_mix():\n",
    "    res = estrs_basic_mix() + estrs_esdt()\n",
    "    return res\n",
    "\n",
    "def estrs_simple():\n",
    "    res=[\n",
    "         (\"knc\", KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")),\n",
    "         (\"gnb\",GaussianNB())\n",
    "    ]\n",
    "    return res\n",
    "def estrs_basic_mix():\n",
    "    \"\"\"包含常用的个体学习器,可以作为Stacking的第一层\n",
    "    还可以作为更加复杂的第一层的基础部分,采用列表相加的方式进一步扩充\n",
    "    例如添加集成学习(随机森林,梯度提升等)\n",
    "\n",
    "    具体包括以下模型:\n",
    "\n",
    "    - 线性模型(lsvr,rdcv,logistic)\n",
    "    - k近邻(knc)\n",
    "    - 贝叶斯决策(gnb)\n",
    "    - 决策树(dt)\n",
    "\n",
    "    这些个体学习器较为多样,理论上有利于提高集成学习的泛化能力\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[estimator]\n",
    "        基础个体学习器列表\n",
    "    \"\"\"\n",
    "    estimators_basic_mix = [\n",
    "        # (\"svc\",(SVC(C=10, gamma=0.001,random_state=random_state))),\n",
    "        # (\"lsvr\", make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))),\n",
    "        (\"lsvr\", (LinearSVC(max_iter=5000,random_state=random_state))),\n",
    "        (\"rdcv\", RidgeClassifierCV()),\n",
    "        (\"logistic\", LogisticRegression()),\n",
    "        (\"knc\", KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")),\n",
    "        (\"gnb\", GaussianNB()),\n",
    "        (\n",
    "            \"dtc\",\n",
    "            DecisionTreeClassifier(\n",
    "                criterion=\"entropy\", max_depth=7, max_features=\"sqrt\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return estimators_basic_mix\n",
    "\n",
    "\n",
    "def get_clfs():\n",
    "    \"\"\"需要评估和分析对比的估计器\n",
    "    svc = SVC(C=0.001, gamma=0.001, kernel=\"poly\", probability=True)\n",
    "    knn=KNeighborsClassifier(n_neighbors=3, p=1, weights='distance')\n",
    "    Best for DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "    dt = DecisionTreeClassifier(\n",
    "        criterion=\"entropy\",\n",
    "        max_depth=7,\n",
    "        max_features=None,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=2\n",
    "    )\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[estimator]\n",
    "        sklearn.estimator估计器列表\n",
    "    \"\"\"\n",
    "\n",
    "    rfc = RandomForestClassifier()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    lsvc = LinearSVC(max_iter=5000, random_state=random_state)\n",
    "    svc = SVC()\n",
    "    mlp = MLPClassifier(max_iter=3000)\n",
    "    rdcv = RidgeClassifierCV()\n",
    "    gnb = GaussianNB()\n",
    "    plsvc = make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))\n",
    "\n",
    "    # stack = get_stacking_clf()\n",
    "    stack_basic_svc=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=SVC()\n",
    "    )\n",
    "    stack_basic_rf=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=RandomForestClassifier()\n",
    "    )\n",
    "    stack_basic_mlp=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=MLPClassifier()\n",
    "    )\n",
    "    # estrs_basic_esdt_mix作为第一层\n",
    "    stack_basic_esdt_gnb=StackingClassifier(\n",
    "        estimators=estrs_basic_esdt_mix(),\n",
    "        final_estimator=GaussianNB()\n",
    "    )\n",
    "    #estrs_basic_mlp_mix作为第一层\n",
    "\n",
    "    stack_basic_mlp_gnb=StackingClassifier(\n",
    "        estimators=estrs_basic_mlp_mix(),\n",
    "        final_estimator=GaussianNB()\n",
    "    )\n",
    "    stack_basic_mlp_lr=StackingClassifier(\n",
    "        estimators=estrs_basic_mlp_mix(),\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "    stack_simple=StackingClassifier(\n",
    "        estimators=estrs_simple(),\n",
    "        final_estimator=LinearSVC()\n",
    "    )\n",
    "    stack_multilayer=StackingClassifier(\n",
    "        estimators=estrs_basic_esdt_mix(),\n",
    "        final_estimator=stack_simple\n",
    "    )\n",
    "    # 配置待评估的分类器的列表\n",
    "    clfs = [\n",
    "        stack_basic_svc,\n",
    "        stack_basic_rf,\n",
    "        stack_basic_mlp,\n",
    "        stack_basic_esdt_gnb,\n",
    "        stack_basic_mlp_gnb,\n",
    "        stack_basic_mlp_lr,\n",
    "        stack_simple,\n",
    "        stack_multilayer,\n",
    "        gnb,\n",
    "        rfc,\n",
    "        rdcv,\n",
    "        dt,\n",
    "        lsvc,\n",
    "        svc,\n",
    "        mlp,\n",
    "        plsvc,\n",
    "    ]\n",
    "    return clfs\n",
    "\n",
    "\n",
    "def main():\n",
    "    clfs = get_clfs()\n",
    "\n",
    "    passive_emo = [\"angry\", \"sad\"]\n",
    "    passive_emo_others = passive_emo + [\"others\"]\n",
    "    typical_emo = [\n",
    "        \"happy\",\n",
    "        #    \"neutral\",\n",
    "        \"sad\",\n",
    "    ]\n",
    "    AHSO = [\"angry\", \"neutral\", \"sad\", \"others\"]\n",
    "\n",
    "    AHNS = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "    e_config = typical_emo\n",
    "    f_config = [\"mfcc\"]\n",
    "    # f_config=f_config_def\n",
    "    \n",
    "    # model = get_stacking_clf()\n",
    "    # model=rf\n",
    "\n",
    "    # 配置语料库\n",
    "    ## 同库实验\n",
    "    # meta_dict = mp.get_single_db_pair_dict(emodb)\n",
    "    ## 跨库实验\n",
    "    meta_dict = mp.emodb_savee\n",
    "    res_list = []\n",
    "    for i, clf in enumerate(clfs):\n",
    "        res_dict = assess_model(e_config, f_config, clf, meta_dict)\n",
    "        res_list.append(res_dict)\n",
    "    res_list.sort(key=lambda res_dict: res_dict[\"test_score\"], reverse=True)\n",
    "\n",
    "    for i, res_dict in enumerate(res_list):\n",
    "        print(i + 1, \"--\" * 30, \"\\n\")\n",
    "        for key, value in res_dict.items():\n",
    "            if key == \"er\":\n",
    "                print(key, \":\", value.model)\n",
    "            elif key in [\"report\", \"confusion_matrix\"]:\n",
    "                print(key, \":\\n\", value)\n",
    "            else:\n",
    "                print(key, \":\", value)\n",
    "        # print(model, \"@{model}\")\n",
    "        # print(f\"{train_score=}\")\n",
    "        # print(f\"{test_score=}\")\n",
    "        # # 查看混淆矩阵\n",
    "        # print(confusion_matrix)\n",
    "        # # 查看辅助性能指标报告\n",
    "        # print(report)\n",
    "\n",
    "\n",
    "def assess_model(e_config, f_config, model, meta_dict):\n",
    "    er = EmotionRecognizer(\n",
    "        model=model,\n",
    "        **meta_dict,\n",
    "        e_config=e_config,\n",
    "        f_config=f_config,\n",
    "        balance=True,\n",
    "        cross=True,  # 执行跨库任务,调整测试数据集读入\n",
    "        verbose=0,\n",
    "        # std_scaler=False,\n",
    "        # pca_params=dict(n_components=39)\n",
    "        # std_scaler=False,\n",
    "        # pca={\"n_components\":\"mle\"}\n",
    "        # pca={'n_components': 60}\n",
    "    )\n",
    "    # 显示调用训练方法(相当于调用sklearn.estimator.fit)\n",
    "    er.train()\n",
    "    # 评估模型的各项性能指标\n",
    "    train_score = er.train_score()\n",
    "    test_score = er.test_score()\n",
    "    confusion_matrix = er.confusion_matrix()\n",
    "    report = er.check_report()\n",
    "    # cv_score = er.model_cv_score()\n",
    "\n",
    "    # print(model, \"@{model}\")\n",
    "    # print(f\"{train_score=}\")\n",
    "    # print(f\"{test_score=}\")\n",
    "    # # 查看混淆矩阵\n",
    "    # print(confusion_matrix)\n",
    "    # # 查看辅助性能指标报告\n",
    "    # print(report)\n",
    "    # 交叉验证得分\n",
    "    # print(f\"{cv_score=}\")\n",
    "\n",
    "    return dict(\n",
    "        er=er,\n",
    "        train_score=train_score,\n",
    "        test_score=test_score,\n",
    "        confusion_matrix=confusion_matrix,\n",
    "        report=report,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        er = main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=3,\n",
      "                                                       n_estimators=10,\n",
      "                                                       random_state=42)),\n",
      "                               ('adab',\n",
      "                                AdaBoostClassifier(learning_rate=0.1,\n",
      "                                                   n_estimators=10,\n",
      "                                                   random_state=42)),\n",
      "                               ('gradb', GradientBoostingClassifier())],\n",
      "                   final_estimator=StackingClassifier(estimators=[('knc',\n",
      "                                                                   KNeighborsClassifier(n_neighbors=3,\n",
      "                                                                                        p=1,\n",
      "                                                                                        weights='distance')),\n",
      "                                                                  ('gnb',\n",
      "                                                                   GaussianNB())],\n",
      "                                                      final_estimator=LinearSVC()))\n",
      "train_score : 1.0\n",
      "test_score : 0.7282608695652174\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        45.652176      54.347824\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.46      0.63        46\n",
      "         sad       0.65      1.00      0.79        46\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.82      0.73      0.71        92\n",
      "weighted avg       0.82      0.73      0.71        92\n",
      "\n",
      "2 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=RandomForestClassifier())\n",
      "train_score : 1.0\n",
      "test_score : 0.6739130434782609\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        34.782608      65.217392\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.35      0.52        46\n",
      "         sad       0.61      1.00      0.75        46\n",
      "\n",
      "    accuracy                           0.67        92\n",
      "   macro avg       0.80      0.67      0.64        92\n",
      "weighted avg       0.80      0.67      0.64        92\n",
      "\n",
      "3 ------------------------------------------------------------ \n",
      "\n",
      "er : LinearSVC(max_iter=5000, random_state=42)\n",
      "train_score : 1.0\n",
      "test_score : 0.6630434782608695\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        32.608696      67.391304\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.33      0.49        46\n",
      "         sad       0.60      1.00      0.75        46\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.80      0.66      0.62        92\n",
      "weighted avg       0.80      0.66      0.62        92\n",
      "\n",
      "4 ------------------------------------------------------------ \n",
      "\n",
      "er : RandomForestClassifier()\n",
      "train_score : 1.0\n",
      "test_score : 0.6086956521739131\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         21.73913      78.260872\n",
      "true_sad            0.00000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.22      0.36        46\n",
      "         sad       0.56      1.00      0.72        46\n",
      "\n",
      "    accuracy                           0.61        92\n",
      "   macro avg       0.78      0.61      0.54        92\n",
      "weighted avg       0.78      0.61      0.54        92\n",
      "\n",
      "5 ------------------------------------------------------------ \n",
      "\n",
      "er : RidgeClassifierCV()\n",
      "train_score : 1.0\n",
      "test_score : 0.5978260869565217\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        21.739130      78.260872\n",
      "true_sad           2.173913      97.826088\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.91      0.22      0.35        46\n",
      "         sad       0.56      0.98      0.71        46\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.73      0.60      0.53        92\n",
      "weighted avg       0.73      0.60      0.53        92\n",
      "\n",
      "6 ------------------------------------------------------------ \n",
      "\n",
      "er : DecisionTreeClassifier()\n",
      "train_score : 1.0\n",
      "test_score : 0.5978260869565217\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        43.478260      56.521736\n",
      "true_sad          23.913044      76.086960\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.65      0.43      0.52        46\n",
      "         sad       0.57      0.76      0.65        46\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.61      0.60      0.59        92\n",
      "weighted avg       0.61      0.60      0.59        92\n",
      "\n",
      "7 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=SVC())\n",
      "train_score : 1.0\n",
      "test_score : 0.5760869565217391\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        15.217391      84.782608\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.15      0.26        46\n",
      "         sad       0.54      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.77      0.58      0.48        92\n",
      "weighted avg       0.77      0.58      0.48        92\n",
      "\n",
      "8 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=MLPClassifier())\n",
      "train_score : 1.0\n",
      "test_score : 0.5760869565217391\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        15.217391      84.782608\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.15      0.26        46\n",
      "         sad       0.54      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.77      0.58      0.48        92\n",
      "weighted avg       0.77      0.58      0.48        92\n",
      "\n",
      "9 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('mpl',\n",
      "                                MLPClassifier(alpha=0.01, batch_size=512,\n",
      "                                              hidden_layer_sizes=(300,),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              max_iter=400, random_state=42))],\n",
      "                   final_estimator=LogisticRegression())\n",
      "train_score : 1.0\n",
      "test_score : 0.5760869565217391\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        15.217391      84.782608\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.15      0.26        46\n",
      "         sad       0.54      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.77      0.58      0.48        92\n",
      "weighted avg       0.77      0.58      0.48        92\n",
      "\n",
      "10 ------------------------------------------------------------ \n",
      "\n",
      "er : GaussianNB()\n",
      "train_score : 0.9895833333333334\n",
      "test_score : 0.5652173913043478\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        13.043478       86.95652\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.13      0.23        46\n",
      "         sad       0.53      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.57        92\n",
      "   macro avg       0.77      0.57      0.46        92\n",
      "weighted avg       0.77      0.57      0.46        92\n",
      "\n",
      "11 ------------------------------------------------------------ \n",
      "\n",
      "er : Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearsvc', LinearSVC(random_state=42))])\n",
      "train_score : 1.0\n",
      "test_score : 0.5652173913043478\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        13.043478       86.95652\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.13      0.23        46\n",
      "         sad       0.53      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.57        92\n",
      "   macro avg       0.77      0.57      0.46        92\n",
      "weighted avg       0.77      0.57      0.46        92\n",
      "\n",
      "12 ------------------------------------------------------------ \n",
      "\n",
      "er : MLPClassifier(max_iter=3000)\n",
      "train_score : 0.9791666666666666\n",
      "test_score : 0.5543478260869565\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        10.869565       89.13044\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.11      0.20        46\n",
      "         sad       0.53      1.00      0.69        46\n",
      "\n",
      "    accuracy                           0.55        92\n",
      "   macro avg       0.76      0.55      0.44        92\n",
      "weighted avg       0.76      0.55      0.44        92\n",
      "\n",
      "13 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=3,\n",
      "                                                       n_estimators=10,\n",
      "                                                       random_state=42)),\n",
      "                               ('adab',\n",
      "                                AdaBoostClassifier(learning_rate=0.1,\n",
      "                                                   n_estimators=10,\n",
      "                                                   random_state=42)),\n",
      "                               ('gradb', GradientBoostingClassifier())],\n",
      "                   final_estimator=GaussianNB())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "14 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('mpl',\n",
      "                                MLPClassifier(alpha=0.01, batch_size=512,\n",
      "                                              hidden_layer_sizes=(300,),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              max_iter=400, random_state=42))],\n",
      "                   final_estimator=GaussianNB())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "15 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB())],\n",
      "                   final_estimator=LinearSVC())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "16 ------------------------------------------------------------ \n",
      "\n",
      "er : SVC()\n",
      "train_score : 0.9895833333333334\n",
      "test_score : 0.5\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy              0.0          100.0\n",
      "true_sad                0.0          100.0\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.00      0.00      0.00        46\n",
      "         sad       0.50      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.50        92\n",
      "   macro avg       0.25      0.50      0.33        92\n",
      "weighted avg       0.25      0.50      0.33        92\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# file=r'D:\\repos\\CCSER\\SER\\data\\savee\\AudioData\\DC\\h01.wav'\n",
    "# file=meta.speech_dbs_dir/emodb/r'wav/03a01Fa.wav'\n",
    "# predict_res=er.predict(file)\n",
    "# print(f\"{predict_res=}\")\n",
    "# predict_proba=er.predict_proba(file)\n",
    "# print(f\"{predict_proba=}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# data=rec.load_data()\n",
    "\n",
    "# data = load_data(\n",
    "#     train_meta_files=train_meta_files,\n",
    "#     test_meta_files=test_meta_files,\n",
    "#     balance=False,\n",
    "# )\n",
    "\n",
    "# X_train = data[\"X_train\"]\n",
    "# y_train = data[\"y_train\"]\n",
    "# rec.train(X_train=X_train, y_train=y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# from typing_extensions import deprecated\n",
    "import warnings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import config.MetaPath as mp\n",
    "from config.MetaPath import test_emodb_csv\n",
    "from config.algoparams import ava_ML_algorithms\n",
    "import random\n",
    "from time import time\n",
    "from config.algoparams import ava_cv_modes\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from deprecated import deprecated\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    fbeta_score,\n",
    "    make_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ShuffleSplit,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "from audio.extractor import AudioExtractor, load_data_from_meta\n",
    "import config.EF as ef\n",
    "from config.EF import AHNPS, e_config_def, f_config_def, validate_emotions\n",
    "from config.MetaPath import (\n",
    "    emodb,\n",
    "    meta_paths_of_db,\n",
    "    ravdess,\n",
    "    savee,\n",
    "    validate_partition,\n",
    "    project_dir,\n",
    ")\n",
    "import config.MetaPath as meta\n",
    "from audio.core import best_estimators, extract_feature_of_audio\n",
    "from config.algoparams import random_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "class EmotionRecognizer:\n",
    "    \"\"\"A class for training, testing ,predicting,anaylzing emotions based on\n",
    "    speech's features that are extracted and fed into `sklearn` model\n",
    "\n",
    "    examples\n",
    "    -\n",
    "    rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=[ravdess],train_dbs=[ravdess], verbose=1)\n",
    "\n",
    "    rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=emodb,train_dbs=emodb, verbose=1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        classification_task=True,\n",
    "        dbs=None,\n",
    "        e_config=None,\n",
    "        f_config=None,\n",
    "        train_dbs=None,\n",
    "        test_dbs=None,\n",
    "        balance=False,\n",
    "        shuffle=True,\n",
    "        override_csv=True,\n",
    "        cross=False,  # 表示跨库(这回让其读取train_db_econfig.csv作为测试集,样例更丰富,更合理,当然还可以进一步改进,读取all_db_econfg.csv,但是这里暂不执行)\n",
    "        verbose=0,\n",
    "        **feature_transforms,\n",
    "        # **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            model (sklearn model): the model used to detect emotions. If `model` is None, then self.determine_best_model()\n",
    "                will be automatically called\n",
    "                这个参数其实就是sklearn中的Estimator对象,例如SVC()示例化出来的对象\n",
    "            emotions (list): list of emotions to be used. Note that these emotions must be available in\n",
    "                RAVDESS & EMODB Datasets, available nine emotions are the following:\n",
    "                    'neutral', 'calm', 'happy', 'sad', 'angry', 'fear', 'disgust', 'ps' ( pleasant surprised ), 'boredom'.\n",
    "                默认识别三种情感\n",
    "                Default is [\"sad\", \"neutral\", \"happy\"].\n",
    "            情感数据库的使用开关:\n",
    "            ravdess (bool): whether to use RAVDESS Speech datasets, default is True\n",
    "            emodb (bool): whether to use EMO-DB Speech dataset, default is True,\n",
    "            custom_db (bool): whether to use custom Speech dataset that is located in `data/train-custom`\n",
    "                and `data/test-custom`, default is True\n",
    "            输出文件名的指定(应该指定为csv文件,即参数带有扩展后缀csv)\n",
    "            ravdess_name (str): the name of the output CSV file for RAVDESS dataset, default is \"ravdess.csv\"\n",
    "            emodb_name (str): the name of the output CSV file for EMO-DB dataset, default is \"emodb.csv\"\n",
    "            custom_db_name (str): the name of the output CSV file for the custom dataset, default is \"custom.csv\"\n",
    "            指定需要提取的情感特征,默认三种:mfcc,chroma,mel\n",
    "            features (list): list of speech features to use, default is [\"mfcc\", \"chroma\", \"mel\"]\n",
    "                (i.e MFCC, Chroma and MEL spectrogram )\n",
    "            指定要使用的分类模型还是回归模型,默认使用分类模型\n",
    "            classification (bool): whether to use classification or regression, default is True\n",
    "            balance (bool): whether to balance the dataset ( both training and testing ), default is True\n",
    "            verbose (bool/int): whether to print messages on certain tasks, default is 1\n",
    "        Note that when `ravdess`, `emodb` and `custom_db` are set to `False`, `ravdess` will be set to True\n",
    "        automatically.\n",
    "        \"\"\"\n",
    "        # emotions\n",
    "        self.e_config = e_config if e_config else e_config_def\n",
    "        # make sure that there are only available emotions\n",
    "        validate_emotions(self.e_config)\n",
    "        self.f_config = f_config if f_config else f_config_def\n",
    "        # 转换为字典格式(待优化)\n",
    "        # @deprecated(version='1.0', reason='请使用 new_function() 代替')\n",
    "        # self._f_config_dict: dict[str, bool] = get_f_config_dict(self.f_config)\n",
    "        self.train_dbs = train_dbs\n",
    "        self.test_dbs = test_dbs\n",
    "\n",
    "        # print(self.train_meta_files, self.test_meta_files)\n",
    "        self.feature_transforms = feature_transforms\n",
    "\n",
    "        # 可以使用python 默认参数来改造写法\n",
    "        # 默认执行分类任务\n",
    "        self.classification_task = classification_task\n",
    "        self.verbose = verbose\n",
    "        # boolean attributes\n",
    "        self.override_csv = override_csv\n",
    "        self.shuffle = shuffle\n",
    "        self.balance = balance\n",
    "        self.cross = cross\n",
    "        # 非构造器初始化变量\n",
    "        self.data_loaded = False\n",
    "        self.model_trained = False\n",
    "        self.ae = None\n",
    "        self.dbs = dbs if dbs else [ravdess]\n",
    "        # 鉴于数据集(特征和标签)在评估方法时将反复用到,因此这里将设置相应的属性来保存它们\n",
    "        # 另一方面,如果模仿sklearn中的编写风格,其实是将数据和模型计算分布在不同的模块(类)中,比如\n",
    "        # sklearn.datasets负责数据集生成\n",
    "        # sklearn.model_selection负责划分数据集和训练集\n",
    "        # sklearn.algorithms* 负责创建模型\n",
    "        # sklearn.metrics 负责评估模型\n",
    "        # 设置相应的属性的方便之处在于方法的调用可以少传参\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "        self.y_pred = []\n",
    "        self.train_audio_paths = []\n",
    "        self.test_audio_paths = []\n",
    "        # 开始填充数据(最先开始的步骤,放在init中随着初始化实例的时候执行)\n",
    "        # self.load_data()\n",
    "        # 属性的先后位置会影响程序的运行\n",
    "        # print(\"@{model}\")\n",
    "        # print(model,\"\\ncomparing and choosing the best model...\")\n",
    "        # !RandomForestClassifier实例不能直接用bool()来判断,会提示estimators_不存在\n",
    "        self.model = model\n",
    "        # if self.model is None:\n",
    "        # 依赖于boolean attributes\n",
    "\n",
    "        self.train_meta_files = meta_paths_of_db(\n",
    "            db=self.train_dbs,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=\"train\",\n",
    "        )\n",
    "        # 判断跨库任务\n",
    "        test_meta_partition = \"test\"\n",
    "        if self.cross:\n",
    "            test_meta_partition = \"train\"\n",
    "        self.test_meta_files = meta_paths_of_db(\n",
    "            db=self.test_dbs,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=test_meta_partition,\n",
    "        )\n",
    "\n",
    "    # def prepare():\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        导入指定的语料库数据,并提取特征\n",
    "        Loads and extracts features from the audio files for the db's specified\n",
    "        - 注意,由于balance操作对于数据集划分有一定要求,不是任何数据集都可以执行balance操作(例如三分类中,test set中只缺失了某一个类别的样本,这中情况下执行balance,将导致测试集样本数量为空)\n",
    "        \"\"\"\n",
    "        # 判断是否已经导入过数据.如果已经导入,则跳过,否则执行导入\n",
    "        if not self.data_loaded:\n",
    "            # 调用extractor中的数据导入函数\n",
    "            data = load_data_from_meta(\n",
    "                train_meta_files=self.train_meta_files,\n",
    "                test_meta_files=self.test_meta_files,\n",
    "                f_config=self.f_config,\n",
    "                e_config=self.e_config,\n",
    "                classification_task=self.classification_task,\n",
    "                balance=self.balance,\n",
    "                shuffle=self.shuffle,\n",
    "                feature_transforms=self.feature_transforms,\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            # 设置实例的各个属性\n",
    "            # 事实上,也可以直接用load_data_from_meta返回的结果中的ae对象,赋值ER对象(self.ae=data[\"ae\"])\n",
    "            self.ae = data[\"ae\"]\n",
    "            self.X_train = data[\"X_train\"]\n",
    "            self.X_test = data[\"X_test\"]\n",
    "            self.y_train = data[\"y_train\"]\n",
    "            self.y_test = data[\"y_test\"]\n",
    "            self.train_audio_paths = data[\"train_audio_paths\"]\n",
    "            self.test_audio_paths = data[\"test_audio_paths\"]\n",
    "\n",
    "            self.balanced_success(data)\n",
    "            if self.verbose:\n",
    "                print(\"[I] Data loaded\\n\")\n",
    "                print(f\"{self.ae=}\")\n",
    "                print(f\"{self.ae.pca=}🎈\")\n",
    "            self.data_loaded = True\n",
    "            # print(id(self))\n",
    "            if self.verbose > 1:\n",
    "                print(vars(self))\n",
    "\n",
    "    def balanced_success(self, res):\n",
    "        self.balance = res[\"balance\"]\n",
    "\n",
    "    def train(self, choosing=False, verbose=1):\n",
    "        \"\"\"\n",
    "        载入数据并训练模型(sklearn.estimator.fit)\n",
    "        Train the model, if data isn't loaded, it will be loaded automatically\n",
    "\n",
    "        X_train=None, y_train=None\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            # if data isn't loaded yet, load it then\n",
    "            self.load_data()\n",
    "        if self.verbose > 1:\n",
    "            print(\"@{self.model}:\")\n",
    "            print(self.model)\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        if not self.model_trained or choosing:\n",
    "            X_train = self.X_train\n",
    "            y_train = self.y_train\n",
    "            # estimator训练(fit)模型\n",
    "            model.fit(X=X_train, y=y_train)\n",
    "            self.model_trained = True\n",
    "        if self.verbose > 1:\n",
    "            if choosing == True:\n",
    "                print(\n",
    "                    f\"[I] Model trained with{choosing=},choosing the best model,override the trained model..\"\n",
    "                )\n",
    "\n",
    "    def predict(self, audio_path):\n",
    "        \"\"\"\n",
    "        预测单个音频的情感\n",
    "        由于是单个音频的情感预测,因此不需要考虑shuffle和balance这些操作,只需要提取语音特征,然后进行调用模型预测即可\n",
    "        given an `audio_path`, this method extracts the features\n",
    "        and predicts the emotion\n",
    "\n",
    "        以下语句不再适合具有pca降维操作下的情形\n",
    "        feature_audio = extract_feature_of_audio(audio_path, self.f_config)\n",
    "        print(feature1.shape)\n",
    "        print(feature1,\"@{feature1}\",feature1.shape)\n",
    "        feature2=feature1.T\n",
    "        print(feature2,\"@{feature2}\",feature2.shape)\n",
    "        print(feature3,\"@{feature3}\",feature3.shape)\n",
    "        \"\"\"\n",
    "        feature_audio = self.extract_feature_single_audio(audio_path)\n",
    "\n",
    "        feature = feature_audio.reshape(1, -1)\n",
    "        model = self.model if self.model else self.best_model()\n",
    "        res = model.predict(feature)\n",
    "        # res可能是个列表\n",
    "        # print(res, \"@{res}\")\n",
    "        return res[0]\n",
    "\n",
    "    def extract_feature_single_audio(self, audio_path):\n",
    "        \"\"\"extract a single audio file feature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        audio_path : path\n",
    "            audio path\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            audio feature\n",
    "        \"\"\"\n",
    "        ae: AudioExtractor = self.ae\n",
    "        pca = ae.pca\n",
    "        print(pca, \"@{pca} in 'predict' method\")\n",
    "        # if pca:\n",
    "        #     feature_audio=pca.transform(feature_audio)\n",
    "        #     print(feature_audio.shape, \"@{feature_audio.shape}\")\n",
    "        feature_audio = ae.extract_features(partition=\"test\", audio_paths=[audio_path])\n",
    "        return feature_audio\n",
    "        # return self.model.predict(feature2)[0]\n",
    "\n",
    "    def peek_test_set(self, n=5):\n",
    "        res = [\n",
    "            self.test_audio_paths[:n],\n",
    "            self.X_test[:n],\n",
    "            self.y_test[:n],\n",
    "            self.y_pred[:n],\n",
    "        ]\n",
    "        return res\n",
    "\n",
    "    def predict_proba(self, audio_path):\n",
    "        \"\"\"\n",
    "        Predicts the probability of each emotion.\n",
    "        \"\"\"\n",
    "        if self.classification_task:\n",
    "            # feature = extract_feature_of_audio(audio_path, self.f_config).reshape(1, -1)\n",
    "            feature = self.extract_feature_single_audio(audio_path)\n",
    "            proba = self.model.predict_proba(feature)[0]\n",
    "            result = {}\n",
    "            for emotion, prob in zip(self.model.classes_, proba):\n",
    "                result[emotion] = prob\n",
    "            return result\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Probability prediction doesn't make sense for regression\"\n",
    "            )\n",
    "\n",
    "    def show_second(self):\n",
    "        peeker = er.peek_test_set(2)\n",
    "        feature = peeker[1][1]\n",
    "        audio_path = peeker[0][1]\n",
    "        feature_pred = self.predict(audio_path)\n",
    "        print(feature[:5], feature_pred[:5])\n",
    "\n",
    "    def grid_search(self, params, n_jobs=2, verbose=3):\n",
    "        \"\"\"\n",
    "        使用网格化搜索的方式搜索最优超参数\n",
    "        Performs GridSearchCV on `params` passed on the `self.model`\n",
    "        And returns the tuple: (best_estimator, best_params, best_score).\n",
    "        \"\"\"\n",
    "        score = accuracy_score if self.classification_task else mean_absolute_error\n",
    "        grid = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=params,\n",
    "            scoring=make_scorer(score),\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "            cv=3,\n",
    "        )\n",
    "        # 调用fit开始传入数据集并搜索\n",
    "        X_train, y_train = self.X_train, self.y_train\n",
    "        if X_train is None or y_train is None:\n",
    "            raise ValueError(\"X_train and y_train are None\")\n",
    "        # fit过程是一个耗时的过程\n",
    "        grid_result = grid.fit(X_train, y_train)\n",
    "        return (\n",
    "            grid_result.best_estimator_,\n",
    "            grid_result.best_params_,\n",
    "            grid_result.best_score_,\n",
    "        )\n",
    "\n",
    "    def best_model(self):\n",
    "        \"\"\"\n",
    "        从常见的模型中计算出最好的Estimator(model)\n",
    "        计算最优model时,也可以考虑创建新的ER实例来做计算最优model的用途,但会增加开销\n",
    "\n",
    "        Loads best estimators and determine which is best for test data,\n",
    "        and then set it to `self.model`.\n",
    "        # 使用MSE来评价回归模型,使用accuracy来评价分类模型\n",
    "        In case of regression, the metric used is MSE(均方误差) and accuracy for classification.\n",
    "\n",
    "        Note that the execution of this method may take several minutes due\n",
    "        to training all estimators (stored in `grid` folder) for determining the best possible one.\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            self.load_data()\n",
    "\n",
    "        # loads estimators\n",
    "        estimators = best_estimators()\n",
    "\n",
    "        result = []\n",
    "\n",
    "        if self.verbose:\n",
    "            # 控制是否显示进度条\n",
    "            # 通过tqdm封装estimator这个可迭代对象,就可以在遍历estimator时,控制进度条的显示\n",
    "            estimators = tqdm(estimators)\n",
    "\n",
    "        for epc in estimators:\n",
    "            estimator, params_, cv_score_ = epc\n",
    "            ecn = estimator.__class__.__name__\n",
    "\n",
    "            if self.verbose:\n",
    "                # 如果启用verbose选项,那么estimators会被tqdm包装\n",
    "                # 此时可以通过set_description方法来修改进度条的描述信息\n",
    "                # 比如,estimators.set_description(f\"Evaluating {estimator.__class__.__name__}\")\n",
    "                estimators.set_description(f\"Evaluating <{ecn}>\")\n",
    "            # 为例避免相互干扰,每测试模型就创建一个ER对象(er)\n",
    "            er = EmotionRecognizer(\n",
    "                model=estimator,\n",
    "                emotions=self.e_config,\n",
    "                classification_task=self.classification_task,\n",
    "                f_config=self.f_config,\n",
    "                balance=self.balance,\n",
    "                override_csv=False,\n",
    "                verbose=0,\n",
    "            )\n",
    "            # data already loaded\n",
    "            er.X_train = self.X_train\n",
    "            er.X_test = self.X_test\n",
    "            er.y_train = self.y_train\n",
    "            er.y_test = self.y_test\n",
    "            er.data_loaded = True\n",
    "            # train the model\n",
    "            er.train(verbose=0)\n",
    "            # get test accuracy\n",
    "            accuracy = er.test_score()\n",
    "            # append to result\n",
    "            result.append((er.model, accuracy))\n",
    "\n",
    "            # 方法2:(小心使用)\n",
    "            # 使用本对象self而不是在创建一个ER对象\n",
    "            # self.model = estimator\n",
    "            # er = self\n",
    "            # 以下的计算是用来选出model的,而不是直接作为self对象的属性,这里将self赋值给er,以示区别\n",
    "            # train(fit) the model\n",
    "            # 如果设置verbose=1,则会逐个打印当前计算的模型(进度不是同一条)\n",
    "            # er.train(choosing=True, verbose=0)\n",
    "            # train(fit) the model\n",
    "            # self.train(verbose=1)\n",
    "            # accuracy = er.test_score(choosing=True)\n",
    "            # append to result\n",
    "            # result.append((estimator, accuracy))\n",
    "\n",
    "            print(f\"\\n[I] {ecn} with {accuracy} test accuracy\")\n",
    "\n",
    "        # sort the result\n",
    "        # regression: best is the lower, not the higher\n",
    "        # classification: best is higher, not the lower\n",
    "        result = sorted(\n",
    "            result, key=lambda item: item[1], reverse=self.classification_task\n",
    "        )\n",
    "        best_estimator = result[0][0]\n",
    "        accuracy = result[0][1]\n",
    "\n",
    "        self.model = best_estimator\n",
    "        self.model_trained = True\n",
    "        if self.verbose:\n",
    "            if self.classification_task:\n",
    "                print(\n",
    "                    f\"[🎈] Best model : {self.model.__class__.__name__} with {accuracy * 100:.3f}% test accuracy\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[I] Best model : {self.model.__class__.__name__} with {accuracy:.5f} mean absolute error\"\n",
    "                )\n",
    "        return best_estimator\n",
    "\n",
    "    def test_score(self, choosing=False, verbose=0, report=False):\n",
    "        \"\"\"\n",
    "        Calculates score on testing data\n",
    "        Please call the `train` method before call this method.\n",
    "\n",
    "        just like sklearn convention:call estimator.call `fit` at first,then call `predict` or `score` method\n",
    "\n",
    "        if `self.classification` is True, the metric used is accuracy,\n",
    "        Mean-Squared-Error is used otherwise (regression)\n",
    "\n",
    "        1.调用训练好的模型进行预测\n",
    "\n",
    "        2.如果model是None,那么调用best_model获取最优模型(这个过程会遍历一个可用模型列表,是通过调用ER实例的test_score()方法来计算,不过这里不会遇到None的情况,因此间接递归调用不超过2层)\n",
    "        \"\"\"\n",
    "        X_test = self.X_test\n",
    "        y_test = self.y_test\n",
    "\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        self.validate_empty_array(X_test=X_test, y_test=y_test)\n",
    "\n",
    "        # 预测计算\n",
    "        if verbose > 1:\n",
    "            print(X_test.shape, y_test.shape, \"🎈\")\n",
    "        # 根据当前模型进行预测(直接调用estimator的predict方法)\n",
    "        y_pred = model.predict(X_test)  # type: ignore\n",
    "        # 如果处于best_model的过程中调用本方法(choosing=True),则上述预测内容作为临时结果不写入对象属性保存;否则作为最终结果写入属性保存\n",
    "        # 默认choosing=False,也就是将结果保存到对象属性中\n",
    "        if choosing == False:\n",
    "            self.y_pred = np.array(y_pred)\n",
    "\n",
    "        if self.classification_task:\n",
    "            res = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "            # 结果和:er.model.score(er.X_test,er.y_test)一样,但是这种做法回独立将X_test预测一遍,而不保存预测结果,只给出得分\n",
    "        else:\n",
    "            res = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "        if report:\n",
    "            self.check_report(y_test, y_pred)\n",
    "        return res\n",
    "\n",
    "    def check_report(self):\n",
    "        \"\"\"输出模型当前结果的多个指标报告\n",
    "        对于分类任务,包括precision(查准率),recall(回召或查全率),f1-score以及各类别的样本数量\n",
    "        对于多分类,还有一些综合的指标(macro,weighted),每个单元格结合两个维度的表头进行理解和阅读\n",
    "\n",
    "        由于跨库实验比较困难,有的样本类别无法被正确分类(所有该类别都被错误分类),此时classification_report方法会提出警告,除非使用zero_division参数替换掉默认的warn.\n",
    "        \"\"\"\n",
    "        y_test = self.y_test\n",
    "        y_pred = self.y_pred\n",
    "\n",
    "        report = classification_report(y_true=y_test, y_pred=y_pred, zero_division=0)\n",
    "        # print(report, self.model.__class__.__name__)\n",
    "        return report\n",
    "\n",
    "    def model_cv_score(\n",
    "        self,\n",
    "        choosing=False,\n",
    "        verbose=1,\n",
    "        mean_only=True,\n",
    "        n_splits=5,\n",
    "        test_size=0.2,\n",
    "        cv_mode=\"sss\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        使用交叉验证的方式来评估模型\n",
    "        Calculates score on testing data\n",
    "        \"\"\"\n",
    "        X_train = self.X_train\n",
    "        y_train = self.y_train\n",
    "        # 调用训练好的模型进行预测\n",
    "        model = self.model if self.model is not None else self.best_model()\n",
    "        self.validate_empty_array(X_train, y_train)\n",
    "\n",
    "        # 预测计算\n",
    "        if verbose > 1:\n",
    "            print(X_train.shape, y_train.shape, \"🎈\")\n",
    "            print(f\"{n_splits=}\")\n",
    "        n_splits = int(n_splits)\n",
    "\n",
    "        y_pred = model.predict(X_train)  # type: ignore\n",
    "        if choosing == False:\n",
    "            self.y_pred = np.array(y_pred)\n",
    "        # 交叉验证的方式评估模型的得分\n",
    "        cv_mode_dict = dict(\n",
    "            sss=StratifiedShuffleSplit(\n",
    "                n_splits=n_splits, test_size=test_size, random_state=0\n",
    "            ),\n",
    "            ss=ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=0),\n",
    "            kfold=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "            skfold=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "        )\n",
    "        cv_mode_selected = cv_mode_dict[cv_mode]\n",
    "        if verbose > 1:\n",
    "            print(f\"{cv_mode=}🎈\")\n",
    "        res = [0]\n",
    "        if self.classification_task:\n",
    "            # res = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "            # 将交叉验证器cv传递给cross_val_score函数执行评估操作\n",
    "            # 而非自己使用来完成k折交叉验证\n",
    "            # 所有对象都有__class__属性以及__name__二级属性\n",
    "            if model.__class__.__name__ in ava_ML_algorithms:\n",
    "                res = cross_val_score(model, X_train, y_train, cv=cv_mode_selected)\n",
    "                if mean_only:\n",
    "                    res = res.mean()\n",
    "\n",
    "        else:\n",
    "            # 使用回归器的情况\n",
    "            res = mean_squared_error(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "        if self.verbose > 2:\n",
    "            report = classification_report(\n",
    "                y_true=y_train, y_pred=y_pred\n",
    "            )  # 训练集上,几乎总是是满分\n",
    "            print(report, self.model.__class__.__name__)\n",
    "        return res\n",
    "\n",
    "    def validate_empty_array(self, X_test=[], y_test=[]):\n",
    "        if len(X_test) == 0:\n",
    "            raise ValueError(\"X is empty\")\n",
    "        if len(y_test) == 0:\n",
    "            raise ValueError(\"y is empty\")\n",
    "\n",
    "    def meta_paths_of_db(self, db, partition=\"test\"):\n",
    "        res = meta_paths_of_db(\n",
    "            db=db,\n",
    "            e_config=self.e_config,\n",
    "            change_type=\"str\",\n",
    "            partition=partition,\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    def update_test_set(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def update_test_set_by_meta(self, test_meta):\n",
    "        \"\"\"\n",
    "        这个函数设计用来做跨库识别试验\n",
    "        仅仅替换测试集为不同库,本身没有针对跨库进行优化\n",
    "\n",
    "\n",
    "        Load test data from given test metadata file paths and update instance's test set attributes.\n",
    "\n",
    "        Args:\n",
    "            test_meta (list of str): List of file paths of test metadata files.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        examples:\n",
    "        >>> rec = EmotionRecognizer(model=my_model,**meta_dict, verbose=1)\n",
    "        >>> rec.train()\n",
    "        >>> rec.update_test_set_by_meta(test_emodb_csv)\n",
    "        >>> rec.test_meta_files\n",
    "        >>> 'meta_files\\\\test_emodb_HNS.csv'\n",
    "        >>> rec.X_test.shape\n",
    "        >>> (43,180)\n",
    "\n",
    "        >>> rec.test_score()\n",
    "        >>> 0.4651\n",
    "\n",
    "        \"\"\"\n",
    "        # rec.update_test_set_by_meta(test_emodb_csv)\n",
    "        self.test_meta_files = test_meta\n",
    "        print(test_meta, \"@{test_meta}\")\n",
    "        test_data = load_data_from_meta(\n",
    "            test_meta_files=test_meta, e_config=self.e_config, f_config=self.f_config\n",
    "        )\n",
    "\n",
    "        X_test = test_data[\"X_test\"]\n",
    "        y_test = test_data[\"y_test\"]\n",
    "        # 设置实例的各个属性\n",
    "        self.test_audio_paths = test_data[\"test_audio_paths\"]\n",
    "        self.update_test_set(X_test, y_test)\n",
    "\n",
    "    def train_score(self, X_train=None, y_train=None):\n",
    "        \"\"\"\n",
    "        Calculates accuracy score on training data\n",
    "        if `self.classification` is True, the metric used is accuracy,\n",
    "        Mean-Squared-Error is used otherwise (regression)\n",
    "        \"\"\"\n",
    "        if X_train is None or y_train is None:\n",
    "            X_train = self.X_train\n",
    "            y_train = self.y_train\n",
    "        y_pred = self.model.predict(X_train)\n",
    "        if self.classification_task:\n",
    "            return accuracy_score(y_true=y_train, y_pred=y_pred)\n",
    "        else:\n",
    "            return mean_squared_error(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    def train_fbeta_score(self, beta):\n",
    "        y_pred = self.model.predict(self.X_train)\n",
    "        y_train = self.y_train\n",
    "        if y_train is None:\n",
    "            raise ValueError(\"y_train is None\")\n",
    "\n",
    "        return fbeta_score(y_true=y_train, y_pred=y_pred, beta=beta, average=\"micro\")\n",
    "\n",
    "    def test_fbeta_score(self, beta):\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        if y_test is None:\n",
    "            raise ValueError(\"y_test is None\")\n",
    "        return fbeta_score(y_true=y_test, y_pred=y_pred, beta=beta, average=\"micro\")\n",
    "\n",
    "    def confusion_matrix(self, percentage=True, labeled=True):\n",
    "        \"\"\"\n",
    "        Computes confusion matrix to evaluate the test accuracy of the classification\n",
    "        and returns it as numpy matrix or pandas dataframe (depends on params).\n",
    "        params:\n",
    "            percentage (bool): whether to use percentage instead of number of samples, default is True.\n",
    "            labeled (bool): whether to label the columns and indexes in the dataframe.\n",
    "        \"\"\"\n",
    "        if not self.classification_task:\n",
    "            raise NotImplementedError(\n",
    "                \"Confusion matrix works only when it is a classification problem\"\n",
    "            )\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        if y_test is None:\n",
    "            raise ValueError(\"y_test is None\")\n",
    "        matrix = confusion_matrix(\n",
    "            y_true=y_test, y_pred=y_pred, labels=self.e_config\n",
    "        ).astype(np.float32)\n",
    "        if percentage:\n",
    "            for i in range(len(matrix)):\n",
    "                matrix[i] = matrix[i] / np.sum(matrix[i])\n",
    "            # make it percentage\n",
    "            matrix *= 100\n",
    "        if labeled:\n",
    "            matrix_df = pd.DataFrame(\n",
    "                matrix,\n",
    "                index=[f\"true_{e}\" for e in self.e_config],\n",
    "                columns=[f\"predicted_{e}\" for e in self.e_config],\n",
    "            )\n",
    "        return matrix_df\n",
    "\n",
    "    def draw_confusion_matrix(self):\n",
    "        \"\"\"Calculates the confusion matrix and shows it\"\"\"\n",
    "        matrix = self.confusion_matrix(percentage=False, labeled=False)\n",
    "        # TODO: add labels, title, legends, etc.\n",
    "        pl.imshow(matrix, cmap=\"binary\")\n",
    "        pl.show()\n",
    "\n",
    "    def count_samples_in_partition(self, emotion, partition):\n",
    "        \"\"\"\n",
    "        Get the number of data samples of the `emotion` class in a particular `partition` ('test' or 'train').\n",
    "\n",
    "        :param emotion: The emotion class to count.\n",
    "        :param partition: The partition to count samples in ('test' or 'train').\n",
    "        :return: The number of data samples of the `emotion` class in the `partition`.\n",
    "        :raises ValueError: If `y_test` or `y_train` is `None`.\n",
    "        \"\"\"\n",
    "        partition = validate_partition(partition, Noneable=False)\n",
    "        if partition == \"test\":\n",
    "            if self.y_test is None:\n",
    "                raise ValueError(\"y_test is None\")\n",
    "            count = sum(1 for y in self.y_test if y == emotion)\n",
    "        else:\n",
    "            if self.y_train is None:\n",
    "                raise ValueError(\"y_train is None\")\n",
    "            count = sum(1 for y in self.y_train if y == emotion)\n",
    "        return count\n",
    "\n",
    "    def count_samples_by_class(self):\n",
    "        \"\"\"\n",
    "        Returns a dataframe that contains the number of training\n",
    "        and testing samples for all emotions.\n",
    "        Note that if data isn't loaded yet, it'll be loaded\n",
    "        \"\"\"\n",
    "        if not self.data_loaded:\n",
    "            self.load_data()\n",
    "        train_samples = []\n",
    "        test_samples = []\n",
    "        total = []\n",
    "        for emotion in self.e_config:\n",
    "            n_train = self.count_samples_in_partition(emotion, \"train\")\n",
    "            n_test = self.count_samples_in_partition(emotion, \"test\")\n",
    "            train_samples.append(n_train)\n",
    "            test_samples.append(n_test)\n",
    "            total.append(n_train + n_test)\n",
    "\n",
    "        # get total\n",
    "        total.append(sum(train_samples) + sum(test_samples))\n",
    "        train_samples.append(sum(train_samples))\n",
    "        test_samples.append(sum(test_samples))\n",
    "        return pd.DataFrame(\n",
    "            data={\"train\": train_samples, \"test\": test_samples, \"total\": total},\n",
    "            index=self.e_config + [\"total\"],\n",
    "        )\n",
    "\n",
    "    def get_random_emotion_index(self, emotion, partition=\"train\"):\n",
    "        \"\"\"\n",
    "        Returns a random index of a `partition` sample with the given `emotion`.\n",
    "\n",
    "        Args:\n",
    "            emotion (str): The name of the emotion to look for.\n",
    "            partition (str): The partition to sample from. Only \"train\" or \"test\" are accepted.\n",
    "\n",
    "        Returns:\n",
    "            int: The index of a random sample with the given `emotion` in the specified `partition`.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If `partition` is not \"train\" or \"test\".\n",
    "        \"\"\"\n",
    "        partition = validate_partition(partition, Noneable=False)\n",
    "        indices = []\n",
    "        if partition == \"train\":\n",
    "            indices = [i for i, y in enumerate(self.y_train) if y == emotion]\n",
    "        elif partition == \"test\":\n",
    "            indices = [i for i, y in enumerate(self.y_test) if y == emotion]\n",
    "\n",
    "        return random.choice(indices)\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# def get_stacking_clf():\n",
    "#     random_state = 42\n",
    "#     # 定义初级学习器\n",
    "#     estimators_basic_mix = estrs_basic_mix( )\n",
    "#     estimators_dt = estrs_esdt(random_state)\n",
    "#     estimators_basic_dt_mix = estrs_basic_dt_mix( )\n",
    "#     estimators_basic_dt_mpl_mix = estrs_basic_dt_mpl_mix(\n",
    "#         random_state, estimators_basic_dt_mix\n",
    "#     )\n",
    "#     estimators_basic_dmb_mix = estimators_basic_dt_mix + [(\"gnb\", GaussianNB())]\n",
    "#     # 简单堆叠\n",
    "#     stack_dt_linear = StackingClassifier(\n",
    "#         estimators=estimators_dt, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     stack_mix_linear = StackingClassifier(\n",
    "#         estimators=estimators_basic_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     # 收敛困难(效果不如mix_linear)\n",
    "#     stack_mix_svm = StackingClassifier(\n",
    "#         estimators=estimators_basic_mix,\n",
    "#         final_estimator=SVC(),\n",
    "#     )\n",
    "#     stack_basic_dt_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dt_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     # 数据量太小\n",
    "#     stack_basic_dt_mlp_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dt_mpl_mix, final_estimator=LogisticRegression()\n",
    "#     )\n",
    "#     stack_dmb_mix = StackingClassifier(\n",
    "#         estimators=estimators_basic_dmb_mix,\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#     )\n",
    "#     # stack1 = StackingClassifier(\n",
    "#     #     estimators=[(\"gbc\", GaussianNB())],\n",
    "#     #     final_estimator=LogisticRegression(),\n",
    "#     # )\n",
    "#     # 多层堆叠\n",
    "#     ##定义最后一层\n",
    "#     stack_final_layer = StackingClassifier(\n",
    "#         estimators=[(\"gbc\", GradientBoostingClassifier()), (\"svc\", SVC())],\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#     )\n",
    "#     ##堆叠二层(容易过拟合)\n",
    "#     stack_multilayer = StackingClassifier(\n",
    "#         estimators=estimators_dt, final_estimator=stack_final_layer\n",
    "#     )\n",
    "#     stack = StackingClassifier(estimators=estimators_dt, final_estimator=SVC())\n",
    "\n",
    "#     # return stack_dt_linear\n",
    "#     # return stack_mix_linear\n",
    "#     # return stack_mix_svm\n",
    "#     # return stack_basic_dt_mix\n",
    "#     return stack_dmb_mix\n",
    "\n",
    "\n",
    "def estrs_basic_mlp_mix():\n",
    "    estimators_basic_dt_mpl_mix = estrs_basic_mix() + [\n",
    "        (\n",
    "            \"mpl\",\n",
    "            MLPClassifier(\n",
    "                alpha=0.01,\n",
    "                batch_size=512,\n",
    "                hidden_layer_sizes=(300,),\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=400,\n",
    "                random_state=random_state,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return estimators_basic_dt_mpl_mix\n",
    "\n",
    "\n",
    "def estrs_esdt():\n",
    "    estimators_dt = [\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=10, max_depth=3, random_state=random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"adab\",\n",
    "            AdaBoostClassifier(\n",
    "                n_estimators=10, learning_rate=0.1, random_state=random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\"gradb\", GradientBoostingClassifier()),\n",
    "    ]\n",
    "\n",
    "    return estimators_dt\n",
    "\n",
    "\n",
    "def estrs_basic_esdt_mix():\n",
    "    res = estrs_basic_mix() + estrs_esdt()\n",
    "    return res\n",
    "\n",
    "def estrs_simple():\n",
    "    res=[\n",
    "         (\"knc\", KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")),\n",
    "         (\"gnb\",GaussianNB())\n",
    "    ]\n",
    "    return res\n",
    "def estrs_basic_mix():\n",
    "    \"\"\"包含常用的个体学习器,可以作为Stacking的第一层\n",
    "    还可以作为更加复杂的第一层的基础部分,采用列表相加的方式进一步扩充\n",
    "    例如添加集成学习(随机森林,梯度提升等)\n",
    "\n",
    "    具体包括以下模型:\n",
    "\n",
    "    - 线性模型(lsvr,rdcv,logistic)\n",
    "    - k近邻(knc)\n",
    "    - 贝叶斯决策(gnb)\n",
    "    - 决策树(dt)\n",
    "\n",
    "    这些个体学习器较为多样,理论上有利于提高集成学习的泛化能力\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[estimator]\n",
    "        基础个体学习器列表\n",
    "    \"\"\"\n",
    "    estimators_basic_mix = [\n",
    "        # (\"svc\",(SVC(C=10, gamma=0.001,random_state=random_state))),\n",
    "        # (\"lsvr\", make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))),\n",
    "        (\"lsvr\", (LinearSVC(max_iter=5000,random_state=random_state))),\n",
    "        (\"rdcv\", RidgeClassifierCV()),\n",
    "        (\"logistic\", LogisticRegression()),\n",
    "        (\"knc\", KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")),\n",
    "        (\"gnb\", GaussianNB()),\n",
    "        (\n",
    "            \"dtc\",\n",
    "            DecisionTreeClassifier(\n",
    "                criterion=\"entropy\", max_depth=7, max_features=\"sqrt\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return estimators_basic_mix\n",
    "\n",
    "\n",
    "def get_clfs():\n",
    "    \"\"\"需要评估和分析对比的估计器\n",
    "    svc = SVC(C=0.001, gamma=0.001, kernel=\"poly\", probability=True)\n",
    "    knn=KNeighborsClassifier(n_neighbors=3, p=1, weights='distance')\n",
    "    Best for DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "    dt = DecisionTreeClassifier(\n",
    "        criterion=\"entropy\",\n",
    "        max_depth=7,\n",
    "        max_features=None,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=2\n",
    "    )\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[estimator]\n",
    "        sklearn.estimator估计器列表\n",
    "    \"\"\"\n",
    "\n",
    "    rfc = RandomForestClassifier()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    lsvc = LinearSVC(max_iter=5000, random_state=random_state)\n",
    "    svc = SVC()\n",
    "    mlp = MLPClassifier(max_iter=3000)\n",
    "    rdcv = RidgeClassifierCV()\n",
    "    gnb = GaussianNB()\n",
    "    plsvc = make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))\n",
    "\n",
    "    # stack = get_stacking_clf()\n",
    "    stack_basic_svc=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=SVC()\n",
    "    )\n",
    "    stack_basic_rf=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=RandomForestClassifier()\n",
    "    )\n",
    "    stack_basic_mlp=StackingClassifier(\n",
    "        estimators=estrs_basic_mix(),\n",
    "        final_estimator=MLPClassifier()\n",
    "    )\n",
    "    # estrs_basic_esdt_mix作为第一层\n",
    "    stack_basic_esdt_gnb=StackingClassifier(\n",
    "        estimators=estrs_basic_esdt_mix(),\n",
    "        final_estimator=GaussianNB()\n",
    "    )\n",
    "    #estrs_basic_mlp_mix作为第一层\n",
    "\n",
    "    stack_basic_mlp_gnb=StackingClassifier(\n",
    "        estimators=estrs_basic_mlp_mix(),\n",
    "        final_estimator=GaussianNB()\n",
    "    )\n",
    "    stack_basic_mlp_lr=StackingClassifier(\n",
    "        estimators=estrs_basic_mlp_mix(),\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "    stack_simple=StackingClassifier(\n",
    "        estimators=estrs_simple(),\n",
    "        final_estimator=LinearSVC()\n",
    "    )\n",
    "    stack_multilayer=StackingClassifier(\n",
    "        estimators=estrs_basic_esdt_mix(),\n",
    "        final_estimator=stack_simple\n",
    "    )\n",
    "    # 配置待评估的分类器的列表\n",
    "    clfs = [\n",
    "        stack_basic_svc,\n",
    "        stack_basic_rf,\n",
    "        stack_basic_mlp,\n",
    "        stack_basic_esdt_gnb,\n",
    "        stack_basic_mlp_gnb,\n",
    "        stack_basic_mlp_lr,\n",
    "        stack_simple,\n",
    "        stack_multilayer,\n",
    "        gnb,\n",
    "        rfc,\n",
    "        rdcv,\n",
    "        dt,\n",
    "        lsvc,\n",
    "        svc,\n",
    "        mlp,\n",
    "        plsvc,\n",
    "    ]\n",
    "    return clfs\n",
    "\n",
    "\n",
    "def main():\n",
    "    clfs = get_clfs()\n",
    "\n",
    "    passive_emo = [\"angry\", \"sad\"]\n",
    "    passive_emo_others = passive_emo + [\"others\"]\n",
    "    typical_emo = [\n",
    "        \"happy\",\n",
    "        #    \"neutral\",\n",
    "        \"sad\",\n",
    "    ]\n",
    "    AHSO = [\"angry\", \"neutral\", \"sad\", \"others\"]\n",
    "\n",
    "    AHNS = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "    e_config = typical_emo\n",
    "    f_config = [\"mfcc\"]\n",
    "    # f_config=f_config_def\n",
    "    \n",
    "    # model = get_stacking_clf()\n",
    "    # model=rf\n",
    "\n",
    "    # 配置语料库\n",
    "    ## 同库实验\n",
    "    # meta_dict = mp.get_single_db_pair_dict(emodb)\n",
    "    ## 跨库实验\n",
    "    meta_dict = mp.emodb_savee\n",
    "    res_list = []\n",
    "    for i, clf in enumerate(clfs):\n",
    "        res_dict = assess_model(e_config, f_config, clf, meta_dict)\n",
    "        res_list.append(res_dict)\n",
    "    res_list.sort(key=lambda res_dict: res_dict[\"test_score\"], reverse=True)\n",
    "\n",
    "    for i, res_dict in enumerate(res_list):\n",
    "        print(i + 1, \"--\" * 30, \"\\n\")\n",
    "        for key, value in res_dict.items():\n",
    "            if key == \"er\":\n",
    "                print(key, \":\", value.model)\n",
    "            elif key in [\"report\", \"confusion_matrix\"]:\n",
    "                print(key, \":\\n\", value)\n",
    "            else:\n",
    "                print(key, \":\", value)\n",
    "        # print(model, \"@{model}\")\n",
    "        # print(f\"{train_score=}\")\n",
    "        # print(f\"{test_score=}\")\n",
    "        # # 查看混淆矩阵\n",
    "        # print(confusion_matrix)\n",
    "        # # 查看辅助性能指标报告\n",
    "        # print(report)\n",
    "\n",
    "\n",
    "def assess_model(e_config, f_config, model, meta_dict):\n",
    "    er = EmotionRecognizer(\n",
    "        model=model,\n",
    "        **meta_dict,\n",
    "        e_config=e_config,\n",
    "        f_config=f_config,\n",
    "        balance=True,\n",
    "        cross=True,  # 执行跨库任务,调整测试数据集读入\n",
    "        verbose=0,\n",
    "        # std_scaler=False,\n",
    "        # pca_params=dict(n_components=39)\n",
    "        # std_scaler=False,\n",
    "        # pca={\"n_components\":\"mle\"}\n",
    "        # pca={'n_components': 60}\n",
    "    )\n",
    "    # 显示调用训练方法(相当于调用sklearn.estimator.fit)\n",
    "    er.train()\n",
    "    # 评估模型的各项性能指标\n",
    "    train_score = er.train_score()\n",
    "    test_score = er.test_score()\n",
    "    confusion_matrix = er.confusion_matrix()\n",
    "    report = er.check_report()\n",
    "    # cv_score = er.model_cv_score()\n",
    "\n",
    "    # print(model, \"@{model}\")\n",
    "    # print(f\"{train_score=}\")\n",
    "    # print(f\"{test_score=}\")\n",
    "    # # 查看混淆矩阵\n",
    "    # print(confusion_matrix)\n",
    "    # # 查看辅助性能指标报告\n",
    "    # print(report)\n",
    "    # 交叉验证得分\n",
    "    # print(f\"{cv_score=}\")\n",
    "\n",
    "    return dict(\n",
    "        er=er,\n",
    "        train_score=train_score,\n",
    "        test_score=test_score,\n",
    "        confusion_matrix=confusion_matrix,\n",
    "        report=report,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        er = main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=RandomForestClassifier())\n",
      "train_score : 1.0\n",
      "test_score : 0.6847826086956522\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         36.95652       63.04348\n",
      "true_sad            0.00000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.37      0.54        46\n",
      "         sad       0.61      1.00      0.76        46\n",
      "\n",
      "    accuracy                           0.68        92\n",
      "   macro avg       0.81      0.68      0.65        92\n",
      "weighted avg       0.81      0.68      0.65        92\n",
      "\n",
      "2 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=3,\n",
      "                                                       n_estimators=10,\n",
      "                                                       random_state=42)),\n",
      "                               ('adab',\n",
      "                                AdaBoostClassifier(learning_rate=0.1,\n",
      "                                                   n_estimators=10,\n",
      "                                                   random_state=42)),\n",
      "                               ('gradb', GradientBoostingClassifier())],\n",
      "                   final_estimator=StackingClassifier(estimators=[('knc',\n",
      "                                                                   KNeighborsClassifier(n_neighbors=3,\n",
      "                                                                                        p=1,\n",
      "                                                                                        weights='distance')),\n",
      "                                                                  ('gnb',\n",
      "                                                                   GaussianNB())],\n",
      "                                                      final_estimator=LinearSVC()))\n",
      "train_score : 1.0\n",
      "test_score : 0.6630434782608695\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        32.608696      67.391304\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.33      0.49        46\n",
      "         sad       0.60      1.00      0.75        46\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.80      0.66      0.62        92\n",
      "weighted avg       0.80      0.66      0.62        92\n",
      "\n",
      "3 ------------------------------------------------------------ \n",
      "\n",
      "er : LinearSVC(max_iter=5000, random_state=42)\n",
      "train_score : 1.0\n",
      "test_score : 0.6630434782608695\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        32.608696      67.391304\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.33      0.49        46\n",
      "         sad       0.60      1.00      0.75        46\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.80      0.66      0.62        92\n",
      "weighted avg       0.80      0.66      0.62        92\n",
      "\n",
      "4 ------------------------------------------------------------ \n",
      "\n",
      "er : RidgeClassifierCV()\n",
      "train_score : 1.0\n",
      "test_score : 0.5978260869565217\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        21.739130      78.260872\n",
      "true_sad           2.173913      97.826088\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.91      0.22      0.35        46\n",
      "         sad       0.56      0.98      0.71        46\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.73      0.60      0.53        92\n",
      "weighted avg       0.73      0.60      0.53        92\n",
      "\n",
      "5 ------------------------------------------------------------ \n",
      "\n",
      "er : DecisionTreeClassifier()\n",
      "train_score : 1.0\n",
      "test_score : 0.5978260869565217\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        43.478260      56.521736\n",
      "true_sad          23.913044      76.086960\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.65      0.43      0.52        46\n",
      "         sad       0.57      0.76      0.65        46\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.61      0.60      0.59        92\n",
      "weighted avg       0.61      0.60      0.59        92\n",
      "\n",
      "6 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=SVC())\n",
      "train_score : 1.0\n",
      "test_score : 0.5869565217391305\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        17.391304      82.608696\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.17      0.30        46\n",
      "         sad       0.55      1.00      0.71        46\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.77      0.59      0.50        92\n",
      "weighted avg       0.77      0.59      0.50        92\n",
      "\n",
      "7 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('mpl',\n",
      "                                MLPClassifier(alpha=0.01, batch_size=512,\n",
      "                                              hidden_layer_sizes=(300,),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              max_iter=400, random_state=42))],\n",
      "                   final_estimator=LogisticRegression())\n",
      "train_score : 1.0\n",
      "test_score : 0.5869565217391305\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        17.391304      82.608696\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.17      0.30        46\n",
      "         sad       0.55      1.00      0.71        46\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.77      0.59      0.50        92\n",
      "weighted avg       0.77      0.59      0.50        92\n",
      "\n",
      "8 ------------------------------------------------------------ \n",
      "\n",
      "er : RandomForestClassifier()\n",
      "train_score : 1.0\n",
      "test_score : 0.5869565217391305\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        17.391304      82.608696\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.17      0.30        46\n",
      "         sad       0.55      1.00      0.71        46\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.77      0.59      0.50        92\n",
      "weighted avg       0.77      0.59      0.50        92\n",
      "\n",
      "9 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt'))],\n",
      "                   final_estimator=MLPClassifier())\n",
      "train_score : 1.0\n",
      "test_score : 0.5760869565217391\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        15.217391      84.782608\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.15      0.26        46\n",
      "         sad       0.54      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.58        92\n",
      "   macro avg       0.77      0.58      0.48        92\n",
      "weighted avg       0.77      0.58      0.48        92\n",
      "\n",
      "10 ------------------------------------------------------------ \n",
      "\n",
      "er : GaussianNB()\n",
      "train_score : 0.9895833333333334\n",
      "test_score : 0.5652173913043478\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        13.043478       86.95652\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.13      0.23        46\n",
      "         sad       0.53      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.57        92\n",
      "   macro avg       0.77      0.57      0.46        92\n",
      "weighted avg       0.77      0.57      0.46        92\n",
      "\n",
      "11 ------------------------------------------------------------ \n",
      "\n",
      "er : Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearsvc', LinearSVC(random_state=42))])\n",
      "train_score : 1.0\n",
      "test_score : 0.5652173913043478\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        13.043478       86.95652\n",
      "true_sad           0.000000      100.00000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.13      0.23        46\n",
      "         sad       0.53      1.00      0.70        46\n",
      "\n",
      "    accuracy                           0.57        92\n",
      "   macro avg       0.77      0.57      0.46        92\n",
      "weighted avg       0.77      0.57      0.46        92\n",
      "\n",
      "12 ------------------------------------------------------------ \n",
      "\n",
      "er : MLPClassifier(max_iter=3000)\n",
      "train_score : 1.0\n",
      "test_score : 0.5217391304347826\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy        32.608696      67.391304\n",
      "true_sad          28.260868      71.739128\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.54      0.33      0.41        46\n",
      "         sad       0.52      0.72      0.60        46\n",
      "\n",
      "    accuracy                           0.52        92\n",
      "   macro avg       0.53      0.52      0.50        92\n",
      "weighted avg       0.53      0.52      0.50        92\n",
      "\n",
      "13 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=3,\n",
      "                                                       n_estimators=10,\n",
      "                                                       random_state=42)),\n",
      "                               ('adab',\n",
      "                                AdaBoostClassifier(learning_rate=0.1,\n",
      "                                                   n_estimators=10,\n",
      "                                                   random_state=42)),\n",
      "                               ('gradb', GradientBoostingClassifier())],\n",
      "                   final_estimator=GaussianNB())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "14 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('lsvr',\n",
      "                                LinearSVC(max_iter=5000, random_state=42)),\n",
      "                               ('rdcv', RidgeClassifierCV()),\n",
      "                               ('logistic', LogisticRegression()),\n",
      "                               ('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB()),\n",
      "                               ('dtc',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=7,\n",
      "                                                       max_features='sqrt')),\n",
      "                               ('mpl',\n",
      "                                MLPClassifier(alpha=0.01, batch_size=512,\n",
      "                                              hidden_layer_sizes=(300,),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              max_iter=400, random_state=42))],\n",
      "                   final_estimator=GaussianNB())\n",
      "train_score : 1.0\n",
      "test_score : 0.5108695652173914\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy         2.173913      97.826088\n",
      "true_sad           0.000000     100.000000\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       1.00      0.02      0.04        46\n",
      "         sad       0.51      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.51        92\n",
      "   macro avg       0.75      0.51      0.36        92\n",
      "weighted avg       0.75      0.51      0.36        92\n",
      "\n",
      "15 ------------------------------------------------------------ \n",
      "\n",
      "er : StackingClassifier(estimators=[('knc',\n",
      "                                KNeighborsClassifier(n_neighbors=3, p=1,\n",
      "                                                     weights='distance')),\n",
      "                               ('gnb', GaussianNB())],\n",
      "                   final_estimator=LinearSVC())\n",
      "train_score : 1.0\n",
      "test_score : 0.5\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy              0.0          100.0\n",
      "true_sad                0.0          100.0\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.00      0.00      0.00        46\n",
      "         sad       0.50      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.50        92\n",
      "   macro avg       0.25      0.50      0.33        92\n",
      "weighted avg       0.25      0.50      0.33        92\n",
      "\n",
      "16 ------------------------------------------------------------ \n",
      "\n",
      "er : SVC()\n",
      "train_score : 0.9895833333333334\n",
      "test_score : 0.5\n",
      "confusion_matrix :\n",
      "             predicted_happy  predicted_sad\n",
      "true_happy              0.0          100.0\n",
      "true_sad                0.0          100.0\n",
      "report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.00      0.00      0.00        46\n",
      "         sad       0.50      1.00      0.67        46\n",
      "\n",
      "    accuracy                           0.50        92\n",
      "   macro avg       0.25      0.50      0.33        92\n",
      "weighted avg       0.25      0.50      0.33        92\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# file=r'D:\\repos\\CCSER\\SER\\data\\savee\\AudioData\\DC\\h01.wav'\n",
    "# file=meta.speech_dbs_dir/emodb/r'wav/03a01Fa.wav'\n",
    "# predict_res=er.predict(file)\n",
    "# print(f\"{predict_res=}\")\n",
    "# predict_proba=er.predict_proba(file)\n",
    "# print(f\"{predict_proba=}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# data=rec.load_data()\n",
    "\n",
    "# data = load_data(\n",
    "#     train_meta_files=train_meta_files,\n",
    "#     test_meta_files=test_meta_files,\n",
    "#     balance=False,\n",
    "# )\n",
    "\n",
    "# X_train = data[\"X_train\"]\n",
    "# y_train = data[\"y_train\"]\n",
    "# rec.train(X_train=X_train, y_train=y_train)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}