##
# from typing_extensions import deprecated
import random
from time import time
from config.algoparams import ava_cv_modes
import matplotlib.pyplot as pl
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
# from deprecated import deprecated
from sklearn.metrics import (accuracy_score, classification_report,
                             confusion_matrix, fbeta_score, make_scorer,
                             mean_absolute_error, mean_squared_error)
from sklearn.model_selection import GridSearchCV, KFold, ShuffleSplit, StratifiedShuffleSplit, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from tqdm import tqdm

from audio.extractor import load_data_from_meta
from config.EF import (e_config_def, f_config_def, validate_emotions)
from config.MetaPath import (emodb, meta_paths_of_db, ravdess, savee,validate_partition,project_dir)
import config.MetaPath as meta
from audio.core import best_estimators, extract_feature_of_audio

##
class EmotionRecognizer:
    """A class for training, testing and predicting emotions based on
    speech's features that are extracted and fed into `sklearn` or `keras` model"""

    def __init__(
            self,
            model=None,
            classification_task=True,
            dbs=None,
            e_config=None,
            f_config=None,
            train_dbs=None,
            test_dbs=None,
            balance=False,
            shuffle=True,
            override_csv=True,
            verbose=1,
            **kwargs,
    ):
        """
        Params:
            model (sklearn model): the model used to detect emotions. If `model` is None, then self.determine_best_model()
                will be automatically called
                è¿™ä¸ªå‚æ•°å…¶å®å°±æ˜¯sklearnä¸­çš„Estimatorå¯¹è±¡,ä¾‹å¦‚SVC()ç¤ºä¾‹åŒ–å‡ºæ¥çš„å¯¹è±¡
            emotions (list): list of emotions to be used. Note that these emotions must be available in
                RAVDESS & EMODB Datasets, available nine emotions are the following:
                    'neutral', 'calm', 'happy', 'sad', 'angry', 'fear', 'disgust', 'ps' ( pleasant surprised ), 'boredom'.
                é»˜è®¤è¯†åˆ«ä¸‰ç§æƒ…æ„Ÿ
                Default is ["sad", "neutral", "happy"].
            æƒ…æ„Ÿæ•°æ®åº“çš„ä½¿ç”¨å¼€å…³:
            ravdess (bool): whether to use RAVDESS Speech datasets, default is True
            emodb (bool): whether to use EMO-DB Speech dataset, default is True,
            custom_db (bool): whether to use custom Speech dataset that is located in `data/train-custom`
                and `data/test-custom`, default is True
            è¾“å‡ºæ–‡ä»¶åçš„æŒ‡å®š(åº”è¯¥æŒ‡å®šä¸ºcsvæ–‡ä»¶,å³å‚æ•°å¸¦æœ‰æ‰©å±•åç¼€csv)
            ravdess_name (str): the name of the output CSV file for RAVDESS dataset, default is "ravdess.csv"
            emodb_name (str): the name of the output CSV file for EMO-DB dataset, default is "emodb.csv"
            custom_db_name (str): the name of the output CSV file for the custom dataset, default is "custom.csv"
            æŒ‡å®šéœ€è¦æå–çš„æƒ…æ„Ÿç‰¹å¾,é»˜è®¤ä¸‰ç§:mfcc,chroma,mel
            features (list): list of speech features to use, default is ["mfcc", "chroma", "mel"]
                (i.e MFCC, Chroma and MEL spectrogram )
            æŒ‡å®šè¦ä½¿ç”¨çš„åˆ†ç±»æ¨¡å‹è¿˜æ˜¯å›å½’æ¨¡å‹,é»˜è®¤ä½¿ç”¨åˆ†ç±»æ¨¡å‹
            classification (bool): whether to use classification or regression, default is True
            balance (bool): whether to balance the dataset ( both training and testing ), default is True
            verbose (bool/int): whether to print messages on certain tasks, default is 1
        Note that when `ravdess`, `emodb` and `custom_db` are set to `False`, `ravdess` will be set to True
        automatically.
        """
        # emotions
        self.e_config = e_config if e_config else e_config_def
        # make sure that there are only available emotions
        validate_emotions(self.e_config)
        self.f_config = f_config if f_config else f_config_def

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼(å¾…ä¼˜åŒ–)
        # @deprecated(version='1.0', reason='è¯·ä½¿ç”¨ new_function() ä»£æ›¿')
        # self._f_config_dict: dict[str, bool] = get_f_config_dict(self.f_config)
        self.train_dbs = train_dbs
        self.test_dbs = test_dbs

        self.train_meta_files = meta_paths_of_db(
            db=self.train_dbs,
            e_config=self.e_config,
            change_type="str",
            partition="train",
        )

        self.test_meta_files = meta_paths_of_db(
            db=self.test_dbs,
            e_config=self.e_config,
            change_type="str",
            partition="test",
        )

        # print(self.train_meta_files, self.test_meta_files)

        # if self.train_dbs and self.test_dbs:
        #     if isinstance( self.train_dbs,str):
        #         self.train_dbs = [self.train_dbs]
        #         self.train_meta_files=[
        #             meta_paths_of_db(db,e_config=self.e_config) for db in self.train_dbs
        #         ]
        #     if isinstance( self.test_dbs,str):
        #         self.test_dbs = [self.test_dbs]
        #         self.test_meta_files=[
        #             meta_paths_of_db(db,e_config=self.e_config) for db in self.test_dbs
        #         ]

        # å¯ä»¥ä½¿ç”¨python é»˜è®¤å‚æ•°æ¥æ”¹é€ å†™æ³•
        # é»˜è®¤æ‰§è¡Œåˆ†ç±»ä»»åŠ¡
        self.classification_task = classification_task
        self.balance = balance
        self.shuffle=shuffle
        self.override_csv = override_csv
        self.verbose = verbose
        # boolean attributes
        self.balance = False
        self.data_loaded = False
        self.model_trained = False

        self.dbs = dbs if dbs else [ravdess]
        # é‰´äºæ•°æ®é›†(ç‰¹å¾å’Œæ ‡ç­¾)åœ¨è¯„ä¼°æ–¹æ³•æ—¶å°†åå¤ç”¨åˆ°,å› æ­¤è¿™é‡Œå°†è®¾ç½®ç›¸åº”çš„å±æ€§æ¥ä¿å­˜å®ƒä»¬
        # å¦ä¸€æ–¹é¢,å¦‚æœæ¨¡ä»¿sklearnä¸­çš„ç¼–å†™é£æ ¼,å…¶å®æ˜¯å°†æ•°æ®å’Œæ¨¡å‹è®¡ç®—åˆ†å¸ƒåœ¨ä¸åŒçš„æ¨¡å—(ç±»)ä¸­,æ¯”å¦‚
        # sklearn.datasetsè´Ÿè´£æ•°æ®é›†ç”Ÿæˆ
        # sklearn.model_selectionè´Ÿè´£åˆ’åˆ†æ•°æ®é›†å’Œè®­ç»ƒé›†
        # sklearn.algorithms* è´Ÿè´£åˆ›å»ºæ¨¡å‹
        # sklearn.metrics è´Ÿè´£è¯„ä¼°æ¨¡å‹
        # è®¾ç½®ç›¸åº”çš„å±æ€§çš„æ–¹ä¾¿ä¹‹å¤„åœ¨äºæ–¹æ³•çš„è°ƒç”¨å¯ä»¥å°‘ä¼ å‚
        self.X_train = []
        self.X_test = []
        self.y_train = []
        self.y_test = []
        self.y_pred = []
        self.train_audio_paths = []
        self.test_audio_paths = []
        # å¼€å§‹å¡«å……æ•°æ®(æœ€å…ˆå¼€å§‹çš„æ­¥éª¤,æ”¾åœ¨initä¸­éšç€åˆå§‹åŒ–å®ä¾‹çš„æ—¶å€™æ‰§è¡Œ)
        # self.load_data()
        # å±æ€§çš„å…ˆåä½ç½®ä¼šå½±å“ç¨‹åºçš„è¿è¡Œ
        print("@{model}")
        # print(model,"\ncomparing and choosing the best model...")
        # !RandomForestClassifierå®ä¾‹ä¸èƒ½ç›´æ¥ç”¨bool()æ¥åˆ¤æ–­,ä¼šæç¤ºestimators_ä¸å­˜åœ¨
        self.model = model
        # if self.model is None:
        # ä¾èµ–äºboolean attributes

    # def prepare():

    def load_data(self):
        """
        å¯¼å…¥æŒ‡å®šçš„è¯­æ–™åº“æ•°æ®,å¹¶æå–ç‰¹å¾
        Loads and extracts features from the audio files for the db's specified
        - æ³¨æ„,ç”±äºbalanceæ“ä½œå¯¹äºæ•°æ®é›†åˆ’åˆ†æœ‰ä¸€å®šè¦æ±‚,ä¸æ˜¯ä»»ä½•æ•°æ®é›†éƒ½å¯ä»¥æ‰§è¡Œbalanceæ“ä½œ(ä¾‹å¦‚ä¸‰åˆ†ç±»ä¸­,test setä¸­åªç¼ºå¤±äº†æŸä¸€ä¸ªç±»åˆ«çš„æ ·æœ¬,è¿™ä¸­æƒ…å†µä¸‹æ‰§è¡Œbalance,å°†å¯¼è‡´æµ‹è¯•é›†æ ·æœ¬æ•°é‡ä¸ºç©º)
        """
        # åˆ¤æ–­æ˜¯å¦å·²ç»å¯¼å…¥è¿‡æ•°æ®.å¦‚æœå·²ç»å¯¼å…¥,åˆ™è·³è¿‡,å¦åˆ™æ‰§è¡Œå¯¼å…¥
        if not self.data_loaded:
            # è°ƒç”¨data_extractorä¸­çš„æ•°æ®å¯¼å…¥å‡½æ•°
            data = load_data_from_meta(
                train_meta_files=self.train_meta_files,
                test_meta_files=self.test_meta_files,
                f_config=self.f_config,
                e_config=self.e_config,
                classification_task=self.classification_task,
                balance=self.balance,
                shuffle=self.shuffle
            )
            # è®¾ç½®å®ä¾‹çš„å„ä¸ªå±æ€§
            self.X_train = data["X_train"]
            self.X_test = data["X_test"]
            self.y_train = data["y_train"]
            self.y_test = data["y_test"]
            self.train_audio_paths = data["train_audio_paths"]
            self.test_audio_paths = data["test_audio_paths"]
           
            self.balanced_success(data)
            if self.verbose:
                print("[I] Data loaded\n")
            self.data_loaded = True
            # print(id(self))
            if self.verbose > 1:
                print(vars(self))

    def balanced_success(self, res):
        self.balance = res["balance"]

    def train(self, choosing=False, verbose=1):
        """

        Train the model, if data isn't loaded, it 'll be loaded automatically

        X_train=None, y_train=None
        """
        if not self.data_loaded:
            # if data isn't loaded yet, load it then
            self.load_data()

        print("@{self.model}:")
        print(self.model)
        model = self.model if self.model is not None else self.best_model()
        if not self.model_trained or choosing:
            X_train = self.X_train
            y_train = self.y_train
            model.fit(X=X_train, y=y_train)
            self.model_trained = True
        if verbose:
            if choosing == True:
                print(
                    f"[I] Model trained with{choosing=},choosing the best model,override the trained model.."
                )

    def predict(self, audio_path):
        """
        é¢„æµ‹å•ä¸ªéŸ³é¢‘çš„æƒ…æ„Ÿ
        ç”±äºæ˜¯å•ä¸ªéŸ³é¢‘çš„æƒ…æ„Ÿé¢„æµ‹,å› æ­¤ä¸éœ€è¦è€ƒè™‘shuffleå’Œbalanceè¿™äº›æ“ä½œ,åªéœ€è¦æå–è¯­éŸ³ç‰¹å¾,ç„¶åè¿›è¡Œè°ƒç”¨æ¨¡å‹é¢„æµ‹å³å¯
        given an `audio_path`, this method extracts the features
        and predicts the emotion
        """
        feature1 = extract_feature_of_audio(audio_path, self.f_config)
        # print(feature1.shape)
        # print(feature1,"@{feature1}",feature1.shape)
        # feature2=feature1.T
        # print(feature2,"@{feature2}",feature2.shape)

        feature = feature1.reshape(1, -1)
        # print(feature3,"@{feature3}",feature3.shape)
        model = self.model if self.model else self.best_model()
        res = model.predict(feature)
        # reså¯èƒ½æ˜¯ä¸ªåˆ—è¡¨
        # print(res, "@{res}")
        return res[0]
        # return self.model.predict(feature2)[0]

    def peek_test_set(self, n=5):
        res = [
            self.test_audio_paths[:n],
            self.X_test[:n],
            self.y_test[:n],
            self.y_pred[:n],
        ]
        return res

    def predict_proba(self, audio_path):
        """
        Predicts the probability of each emotion.
        """
        if self.classification_task:
            feature = extract_feature_of_audio(audio_path, self.f_config).reshape(1, -1)
            proba = self.model.predict_proba(feature)[0]
            result = {}
            for emotion, prob in zip(self.model.classes_, proba):
                result[emotion] = prob
            return result
        else:
            raise NotImplementedError(
                "Probability prediction doesn't make sense for regression"
            )

    def show_second(self):
        peeker = er.peek_test_set(2)
        feature = peeker[1][1]
        audio_path = peeker[0][1]
        feature_pred = self.predict(audio_path)
        print(feature[:5], feature_pred[:5])

    def grid_search(self, params, n_jobs=2, verbose=3):
        """
        ä½¿ç”¨ç½‘æ ¼åŒ–æœç´¢çš„æ–¹å¼æœç´¢æœ€ä¼˜è¶…å‚æ•°
        Performs GridSearchCV on `params` passed on the `self.model`
        And returns the tuple: (best_estimator, best_params, best_score).
        """
        score = accuracy_score if self.classification_task else mean_absolute_error
        grid = GridSearchCV(
            estimator=self.model,
            param_grid=params,
            scoring=make_scorer(score),
            n_jobs=n_jobs,
            verbose=verbose,
            cv=3,
        )
        # è°ƒç”¨fitå¼€å§‹ä¼ å…¥æ•°æ®é›†å¹¶æœç´¢
        X_train, y_train = self.X_train, self.y_train
        if X_train is None or y_train is None:
            raise ValueError("X_train and y_train are None")
        # fitè¿‡ç¨‹æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹
        grid_result = grid.fit(X_train, y_train)
        return (
            grid_result.best_estimator_,
            grid_result.best_params_,
            grid_result.best_score_,
        )

    def best_model(self):
        """
        ä»å¸¸è§çš„æ¨¡å‹ä¸­è®¡ç®—å‡ºæœ€å¥½çš„Estimator(model)
        è®¡ç®—æœ€ä¼˜modelæ—¶,ä¹Ÿå¯ä»¥è€ƒè™‘åˆ›å»ºæ–°çš„ERå®ä¾‹æ¥åšè®¡ç®—æœ€ä¼˜modelçš„ç”¨é€”,ä½†ä¼šå¢åŠ å¼€é”€

        Loads best estimators and determine which is best for test data,
        and then set it to `self.model`.
        # ä½¿ç”¨MSEæ¥è¯„ä»·å›å½’æ¨¡å‹,ä½¿ç”¨accuracyæ¥è¯„ä»·åˆ†ç±»æ¨¡å‹
        In case of regression, the metric used is MSE(å‡æ–¹è¯¯å·®) and accuracy for classification.

        Note that the execution of this method may take several minutes due
        to training all estimators (stored in `grid` folder) for determining the best possible one.
        """
        if not self.data_loaded:
            self.load_data()

        # loads estimators
        estimators = best_estimators()

        result = []

        if self.verbose:
            # æ§åˆ¶æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            # é€šè¿‡tqdmå°è£…estimatorè¿™ä¸ªå¯è¿­ä»£å¯¹è±¡,å°±å¯ä»¥åœ¨éå†estimatoræ—¶,æ§åˆ¶è¿›åº¦æ¡çš„æ˜¾ç¤º
            estimators = tqdm(estimators)

        for epc in estimators:
            estimator, params_, cv_score_ = epc
            ecn = estimator.__class__.__name__

            if self.verbose:
                # å¦‚æœå¯ç”¨verboseé€‰é¡¹,é‚£ä¹ˆestimatorsä¼šè¢«tqdmåŒ…è£…
                # æ­¤æ—¶å¯ä»¥é€šè¿‡set_descriptionæ–¹æ³•æ¥ä¿®æ”¹è¿›åº¦æ¡çš„æè¿°ä¿¡æ¯
                # æ¯”å¦‚,estimators.set_description(f"Evaluating {estimator.__class__.__name__}")
                estimators.set_description(f"Evaluating <{ecn}>")

            er = EmotionRecognizer(
                model=estimator,
                emotions=self.e_config,
                classification_task=self.classification_task,
                f_config=self.f_config,
                balance=self.balance,
                override_csv=False,
                verbose=0
            )
            # data already loaded
            er.X_train = self.X_train
            er.X_test = self.X_test
            er.y_train = self.y_train
            er.y_test = self.y_test
            er.data_loaded = True
            # train the model
            er.train(verbose=0)
            # get test accuracy
            accuracy = er.test_score()
            # append to result
            result.append((er.model, accuracy))

            # ä½¿ç”¨æœ¬å¯¹è±¡selfè€Œä¸æ˜¯åœ¨åˆ›å»ºä¸€ä¸ªERå¯¹è±¡
            # self.model = estimator
            # er = self  
            # ä»¥ä¸‹çš„è®¡ç®—æ˜¯ç”¨æ¥é€‰å‡ºmodelçš„,è€Œä¸æ˜¯ç›´æ¥ä½œä¸ºselfå¯¹è±¡çš„å±æ€§,è¿™é‡Œå°†selfèµ‹å€¼ç»™er,ä»¥ç¤ºåŒºåˆ«

            # train(fit) the model
            # å¦‚æœè®¾ç½®verbose=1,åˆ™ä¼šé€ä¸ªæ‰“å°å½“å‰è®¡ç®—çš„æ¨¡å‹(è¿›åº¦ä¸æ˜¯åŒä¸€æ¡)
            er.train(choosing=True, verbose=0)

            # train(fit) the model
            # self.train(verbose=1)

            accuracy = er.test_score(choosing=True)
            print(f"\n[I] {ecn} with {accuracy} test accuracy")

            # append to result
            result.append((estimator, accuracy))

        # sort the result
        # regression: best is the lower, not the higher
        # classification: best is higher, not the lower
        result = sorted(
            result, key=lambda item: item[1], reverse=self.classification_task
        )
        best_estimator = result[0][0]
        accuracy = result[0][1]

        self.model = best_estimator
        self.model_trained = True
        if self.verbose:
            if self.classification_task:
                print(
                    f"[ğŸˆ] Best model : {self.model.__class__.__name__} with {accuracy * 100:.3f}% test accuracy"
                )
            else:
                print(
                    f"[I] Best model : {self.model.__class__.__name__} with {accuracy:.5f} mean absolute error"
                )
        return best_estimator

    def test_score(self, choosing=False, verbose=0):
        """
        Calculates score on testing data
        if `self.classification` is True, the metric used is accuracy,
        Mean-Squared-Error is used otherwise (regression)
        """
        X_test = self.X_test
        y_test = self.y_test
        # è°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        model = self.model if self.model is not None else self.best_model()
        self.validate_empty_array(X_test=X_test,y_test=y_test)
        # if len(X_test) == 0:
        #     raise ValueError("X_test is empty")
        # if len(y_test) == 0:
        #     raise ValueError("y_test is empty")
        # é¢„æµ‹è®¡ç®—
        if verbose:
            print(X_test.shape, y_test.shape,"ğŸˆ")
        y_pred = model.predict(X_test)  # type: ignore
        if choosing == False:
            self.y_pred = np.array(y_pred)

        if self.classification_task:
            res = accuracy_score(y_true=y_test, y_pred=y_pred)
        else:
            res = mean_squared_error(y_true=y_test, y_pred=y_pred)
        if self.verbose >= 2 or verbose >= 1:
            report = classification_report(y_true=y_test, y_pred=y_pred)
            print(report, self.model.__class__.__name__)
        return res
    def model_cv_score(self, choosing=False, verbose=1,mean_only=True,n_splits=5,test_size=0.2,cv_mode="sss"):
        """
        ä½¿ç”¨äº¤å‰éªŒè¯çš„æ–¹å¼æ¥è¯„ä¼°æ¨¡å‹
        Calculates score on testing data
        """
        X_train = self.X_train
        y_train = self.y_train
        # è°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        model = self.model if self.model is not None else self.best_model()
        self.validate_empty_array(X_train, y_train)

        # é¢„æµ‹è®¡ç®—
        if verbose:
            print(X_train.shape, y_train.shape,"ğŸˆ")
            print(f"{n_splits=}")
        n_splits=int(n_splits)

        y_pred = model.predict(X_train)  # type: ignore
        if choosing == False:
            self.y_pred = np.array(y_pred)
        # äº¤å‰éªŒè¯çš„æ–¹å¼è¯„ä¼°æ¨¡å‹çš„å¾—åˆ†
        cv_mode_dict=dict(
            sss=StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=0),
            ss=ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=0),
            kfold=KFold(n_splits=n_splits, shuffle=True, random_state=0),
        )
        cv_mode_selected=cv_mode_dict[cv_mode]
        if verbose:
            print(f"{cv_mode=}ğŸˆ")
        if self.classification_task:
            # res = accuracy_score(y_true=y_test, y_pred=y_pred)
            res=cross_val_score(model, X_train, y_train, cv=cv_mode_selected)
            if mean_only:
                res=res.mean()
            
        else:
            res = mean_squared_error(y_true=y_train, y_pred=y_pred)
        if self.verbose >= 2 or verbose >= 1:
            report = classification_report(y_true=y_train, y_pred=y_pred)
            print(report, self.model.__class__.__name__)
        return res

    def validate_empty_array(self, X_test=[], y_test=[]):
        if len(X_test) == 0:
            raise ValueError("X is empty")
        if len(y_test) == 0:
            raise ValueError("y is empty")
    def meta_paths_of_db(self, db, partition="test"):
        res = meta_paths_of_db(
            db=db,
            e_config=self.e_config,
            change_type="str",
            partition=partition,
        )
        return res

    def update_test_set(self, X_test, y_test):
        self.X_test = X_test
        self.y_test = y_test

    def update_test_set_by_meta(self, test_meta):
        """
        è¿™ä¸ªå‡½æ•°è®¾è®¡ç”¨æ¥åšè·¨åº“è¯†åˆ«è¯•éªŒ
        ä»…ä»…æ›¿æ¢æµ‹è¯•é›†ä¸ºä¸åŒåº“,æœ¬èº«æ²¡æœ‰é’ˆå¯¹è·¨åº“è¿›è¡Œä¼˜åŒ–


        Load test data from given test metadata file paths and update instance's test set attributes.

        Args:
            test_meta (list of str): List of file paths of test metadata files.

        Returns:
            None

        examples:
        >>> rec = EmotionRecognizer(model=my_model,**meta_dict, verbose=1)
        >>> rec.train()
        >>> rec.update_test_set_by_meta(test_emodb_csv)
        >>> rec.test_meta_files
        >>> 'meta_files\\test_emodb_HNS.csv'
        >>> rec.X_test.shape
        >>> (43,180)

        >>> rec.test_score()
        >>> 0.4651

        """
        # rec.update_test_set_by_meta(test_emodb_csv)
        self.test_meta_files = test_meta
        print(test_meta, "@{test_meta}")
        test_data = load_data_from_meta(
            test_meta_files=test_meta, e_config=self.e_config, f_config=self.f_config
        )

        X_test = test_data["X_test"]
        y_test = test_data["y_test"]
        # è®¾ç½®å®ä¾‹çš„å„ä¸ªå±æ€§
        self.test_audio_paths = test_data["test_audio_paths"]
        self.update_test_set(X_test, y_test)

    def train_score(self, X_train=None, y_train=None):
        """
        Calculates accuracy score on training data
        if `self.classification` is True, the metric used is accuracy,
        Mean-Squared-Error is used otherwise (regression)
        """
        if X_train is None or y_train is None:
            X_train = self.X_train
            y_train = self.y_train
        y_pred = self.model.predict(X_train)
        if self.classification_task:
            return accuracy_score(y_true=y_train, y_pred=y_pred)
        else:
            return mean_squared_error(y_true=y_train, y_pred=y_pred)

    def train_fbeta_score(self, beta):
        y_pred = self.model.predict(self.X_train)
        y_train = self.y_train
        if y_train is None:
            raise ValueError("y_train is None")

        return fbeta_score(y_true=y_train, y_pred=y_pred, beta=beta, average="micro")

    def test_fbeta_score(self, beta):
        y_pred = self.model.predict(self.X_test)
        y_test = self.y_test
        if y_test is None:
            raise ValueError("y_test is None")
        return fbeta_score(y_true=y_test, y_pred=y_pred, beta=beta, average="micro")

    def confusion_matrix(self, percentage=True, labeled=True):
        """
        Computes confusion matrix to evaluate the test accuracy of the classification
        and returns it as numpy matrix or pandas dataframe (depends on params).
        params:
            percentage (bool): whether to use percentage instead of number of samples, default is True.
            labeled (bool): whether to label the columns and indexes in the dataframe.
        """
        if not self.classification_task:
            raise NotImplementedError(
                "Confusion matrix works only when it is a classification problem"
            )
        y_pred = self.model.predict(self.X_test)
        y_test = self.y_test
        if y_test is None:
            raise ValueError("y_test is None")
        matrix = confusion_matrix(
            y_true=y_test, y_pred=y_pred, labels=self.e_config
        ).astype(np.float32)
        if percentage:
            for i in range(len(matrix)):
                matrix[i] = matrix[i] / np.sum(matrix[i])
            # make it percentage
            matrix *= 100
        if labeled:
            matrix_df = pd.DataFrame(
                matrix,
                index=[f"true_{e}" for e in self.e_config],
                columns=[f"predicted_{e}" for e in self.e_config],
            )
        return matrix_df

    def draw_confusion_matrix(self):
        """Calculates the confusion matrix and shows it"""
        matrix = self.confusion_matrix(percentage=False, labeled=False)
        # TODO: add labels, title, legends, etc.
        pl.imshow(matrix, cmap="binary")
        pl.show()

    def count_samples_in_partition(self, emotion, partition):
        """
        Get the number of data samples of the `emotion` class in a particular `partition` ('test' or 'train').

        :param emotion: The emotion class to count.
        :param partition: The partition to count samples in ('test' or 'train').
        :return: The number of data samples of the `emotion` class in the `partition`.
        :raises ValueError: If `y_test` or `y_train` is `None`.
        """
        partition = validate_partition(partition, Noneable=False)
        if partition == "test":
            if self.y_test is None:
                raise ValueError("y_test is None")
            count = sum(1 for y in self.y_test if y == emotion)
        else:
            if self.y_train is None:
                raise ValueError("y_train is None")
            count = sum(1 for y in self.y_train if y == emotion)
        return count

    def count_samples_by_class(self):
        """
        Returns a dataframe that contains the number of training
        and testing samples for all emotions.
        Note that if data isn't loaded yet, it'll be loaded
        """
        if not self.data_loaded:
            self.load_data()
        train_samples = []
        test_samples = []
        total = []
        for emotion in self.e_config:
            n_train = self.count_samples_in_partition(emotion, "train")
            n_test = self.count_samples_in_partition(emotion, "test")
            train_samples.append(n_train)
            test_samples.append(n_test)
            total.append(n_train + n_test)

        # get total
        total.append(sum(train_samples) + sum(test_samples))
        train_samples.append(sum(train_samples))
        test_samples.append(sum(test_samples))
        return pd.DataFrame(
            data={"train": train_samples, "test": test_samples, "total": total},
            index=self.e_config + ["total"],
        )

    def get_random_emotion_index(self, emotion, partition="train"):
        """
        Returns a random index of a `partition` sample with the given `emotion`.

        Args:
            emotion (str): The name of the emotion to look for.
            partition (str): The partition to sample from. Only "train" or "test" are accepted.

        Returns:
            int: The index of a random sample with the given `emotion` in the specified `partition`.

        Raises:
            TypeError: If `partition` is not "train" or "test".
        """
        partition = validate_partition(partition, Noneable=False)
        indices = []
        if partition == "train":
            indices = [i for i, y in enumerate(self.y_train) if y == emotion]
        elif partition == "test":
            indices = [i for i, y in enumerate(self.y_test) if y == emotion]

        return random.choice(indices)



from config.MetaPath import (test_emodb_csv)

passive_emo = ["angry", "sad"]
passive_emo_others=passive_emo+["others"]
typical_emo = ['happy', 'neutral', 'sad']
e_config = typical_emo
def main(EmotionRecognizer, e_config):
    # my_model = RandomForestClassifier(max_depth=3, max_features=0.2)
    my_model = SVC(C=0.001, gamma=0.001, kernel="poly",probability=True)
    my_model=KNeighborsClassifier(n_neighbors=3, p=1, weights='distance')
    # my_model = None

    # rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=[ravdess],train_dbs=[ravdess], verbose=1)
    # rec = EmotionRecognizer(model=my_model,e_config=AHNPS,f_config=f_config_def,test_dbs=emodb,train_dbs=emodb, verbose=1)

    single_db=emodb
    meta_dict = {"train_dbs": single_db, "test_dbs": single_db}
    er = EmotionRecognizer(
        model=my_model,
        e_config=e_config,
        f_config=f_config_def,
        **meta_dict,
        verbose=1,
    )


    er.train()
    
    train_score=er.train_score()
    print(f"{train_score=}")
    test_score = er.test_score()
    print(f"{test_score=}")
    cv_score=er.model_cv_score()
    print(f"{cv_score=}")

    return er

if __name__ == "__main__":

    er=main(EmotionRecognizer, e_config)
##

    # file=r'D:\repos\CCSER\SER\data\savee\AudioData\DC\h01.wav'
    # file=meta.speech_dbs_dir/emodb/r'wav/03a01Fa.wav'
    # predict_res=er.predict(file)
    # print(f"{predict_res=}")
    # predict_proba=er.predict_proba(file)
    # print(f"{predict_proba=}")

##


# rec.update_test_set_by_meta(r'D:\repos\CCSER\SER\meta_files\test_ravdess_AHNPS.csv')

# rec.update_test_set_by_meta(r'D:\repos\CCSER\SER\meta_files\test_emodb_AHNPS.csv')


# data=rec.load_data()

# data = load_data(
#     train_meta_files=train_meta_files,
#     test_meta_files=test_meta_files,
#     balance=False,
# )

# X_train = data["X_train"]
# y_train = data["y_train"]
# rec.train(X_train=X_train, y_train=y_train)
